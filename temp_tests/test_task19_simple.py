# -*- coding: utf-8 -*-
"""
ä»»åŠ¡19: ç®€åŒ–ç‰ˆç«¯åˆ°ç«¯ç³»ç»Ÿé›†æˆæµ‹è¯•
é¿å…ç¼–ç é—®é¢˜ï¼Œä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½éªŒè¯
"""

import os
import sys
import time
import numpy as np
import torch
from typing import Dict, List, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from entities import UAV, Target
from environment import UAVTaskEnv, DirectedGraph
from config import Config
from curriculum_stages import CurriculumStages
from transformer_gnn import TransformerGNN

class SimpleEndToEndTester:
    """ç®€åŒ–ç‰ˆç«¯åˆ°ç«¯ç³»ç»Ÿé›†æˆæµ‹è¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.config = Config()
        self.curriculum_stages = CurriculumStages()
        self.test_results = {}
        
        print("ç®€åŒ–ç‰ˆç«¯åˆ°ç«¯é›†æˆæµ‹è¯•åˆå§‹åŒ–å®Œæˆ")
    
    def test_curriculum_learning_basic(self) -> Dict:
        """æµ‹è¯•1: åŸºç¡€è¯¾ç¨‹å­¦ä¹ åŠŸèƒ½"""
        print("\n=== æµ‹è¯•1: åŸºç¡€è¯¾ç¨‹å­¦ä¹ åŠŸèƒ½ ===")
        results = {
            "test_name": "curriculum_learning_basic",
            "status": "running",
            "stages_verified": 0,
            "errors": []
        }
        
        try:
            # éªŒè¯è¯¾ç¨‹é˜¶æ®µé…ç½®
            stages = self.curriculum_stages.stages
            print(f"è¯¾ç¨‹é˜¶æ®µæ•°é‡: {len(stages)}")
            
            for i, stage in enumerate(stages):
                print(f"é˜¶æ®µ{i}: {stage.stage_name}")
                print(f"  UAVèŒƒå›´: {stage.n_uavs_range}")
                print(f"  ç›®æ ‡èŒƒå›´: {stage.n_targets_range}")
                print(f"  æœ€å¤§å›åˆ: {stage.max_episodes}")
                
                # éªŒè¯é˜¶æ®µé…ç½®åˆç†æ€§
                assert stage.n_uavs_range[0] <= stage.n_uavs_range[1]
                assert stage.n_targets_range[0] <= stage.n_targets_range[1]
                assert stage.max_episodes > 0
                
                results["stages_verified"] += 1
            
            # æµ‹è¯•é˜¶æ®µåˆ‡æ¢
            original_stage = self.curriculum_stages.current_stage_id
            
            # æµ‹è¯•æ¨è¿›
            if self.curriculum_stages.advance_to_next_stage():
                print(f"é˜¶æ®µæ¨è¿›æµ‹è¯•: {original_stage} -> {self.curriculum_stages.current_stage_id}")
            
            # æµ‹è¯•å›é€€
            if self.curriculum_stages.fallback_to_previous_stage():
                print(f"é˜¶æ®µå›é€€æµ‹è¯•: {self.curriculum_stages.current_stage_id + 1} -> {self.curriculum_stages.current_stage_id}")
            
            # æ¢å¤åŸå§‹é˜¶æ®µ
            self.curriculum_stages.current_stage_id = original_stage
            
            results["status"] = "passed"
            print("è¯¾ç¨‹å­¦ä¹ åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            results["status"] = "failed"
            results["errors"].append(str(e))
            print(f"è¯¾ç¨‹å­¦ä¹ åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def test_zero_shot_transfer_basic(self) -> Dict:
        """æµ‹è¯•2: åŸºç¡€é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›"""
        print("\n=== æµ‹è¯•2: åŸºç¡€é›¶æ ·æœ¬è¿ç§»èƒ½åŠ› ===")
        results = {
            "test_name": "zero_shot_transfer_basic",
            "status": "running",
            "scenarios_tested": 0,
            "model_created": False,
            "errors": []
        }
        
        try:
            # åˆ›å»ºå°è§„æ¨¡è®­ç»ƒåœºæ™¯
            print("åˆ›å»ºå°è§„æ¨¡è®­ç»ƒåœºæ™¯...")
            small_uavs, small_targets, obstacles = self._create_test_scenario(3, 2)
            small_graph = DirectedGraph(small_uavs, small_targets, self.config.GRAPH_N_PHI, obstacles, self.config)
            small_env = UAVTaskEnv(small_uavs, small_targets, small_graph, obstacles, self.config, obs_mode="graph")
            
            print(f"å°è§„æ¨¡åœºæ™¯: {len(small_uavs)} UAVs, {len(small_targets)} ç›®æ ‡")
            results["scenarios_tested"] += 1
            
            # åˆ›å»ºTransformerGNNæ¨¡å‹
            print("åˆ›å»ºTransformerGNNæ¨¡å‹...")
            model_config = {
                "embed_dim": 32,
                "num_heads": 2,
                "num_layers": 1,
                "dropout": 0.1,
                "use_position_encoding": True,
                "use_noisy_linear": False,
                "use_local_attention": True,
                "k_adaptive": True,
                "k_min": 2,
                "k_max": 4
            }
            
            model = TransformerGNN(
                obs_space=small_env.observation_space,
                action_space=small_env.action_space,
                num_outputs=small_env.action_space.n,
                model_config=model_config,
                name="test_transformer_gnn"
            )
            
            param_count = sum(p.numel() for p in model.parameters())
            print(f"æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {param_count}")
            results["model_created"] = True
            
            # æµ‹è¯•ä¸åŒè§„æ¨¡åœºæ™¯
            test_scenarios = [(5, 3), (8, 5)]
            
            for n_uavs, n_targets in test_scenarios:
                print(f"æµ‹è¯•åœºæ™¯: {n_uavs} UAVs, {n_targets} ç›®æ ‡")
                
                # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
                test_uavs, test_targets, test_obstacles = self._create_test_scenario(n_uavs, n_targets)
                test_graph = DirectedGraph(test_uavs, test_targets, self.config.GRAPH_N_PHI, test_obstacles, self.config)
                test_env = UAVTaskEnv(test_uavs, test_targets, test_graph, test_obstacles, self.config, obs_mode="graph")
                
                # ä¸ºæ¯ä¸ªåœºæ™¯åˆ›å»ºå¯¹åº”çš„æ¨¡å‹ï¼ˆé›¶æ ·æœ¬è¿ç§»çš„æ ¸å¿ƒæµ‹è¯•ï¼‰
                test_model = TransformerGNN(
                    obs_space=test_env.observation_space,
                    action_space=test_env.action_space,
                    num_outputs=test_env.action_space.n,
                    model_config=model_config,
                    name=f"test_transformer_gnn_{n_uavs}_{n_targets}"
                )
                
                # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
                obs = test_env.reset()
                obs_tensor = self._convert_obs_to_tensor(obs)
                
                with torch.no_grad():
                    logits, _ = test_model({"obs": obs_tensor}, [], [])
                    value = test_model.value_function()
                
                # éªŒè¯è¾“å‡ºæ ¼å¼
                assert logits.shape[0] == 1, f"æ‰¹æ¬¡ç»´åº¦é”™è¯¯: {logits.shape[0]}"
                assert logits.shape[1] == test_env.action_space.n, f"åŠ¨ä½œç»´åº¦é”™è¯¯: {logits.shape[1]} != {test_env.action_space.n}"
                assert value.shape[0] == 1, f"å€¼å‡½æ•°ç»´åº¦é”™è¯¯: {value.shape[0]}"
                
                print(f"  æ¨¡å‹è¾“å‡ºéªŒè¯é€šè¿‡: logits={logits.shape}, value={value.shape}")
                print(f"  åŠ¨ä½œç©ºé—´å¤§å°: {test_env.action_space.n}")
                results["scenarios_tested"] += 1
            
            results["status"] = "passed"
            print("é›¶æ ·æœ¬è¿ç§»åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            results["status"] = "failed"
            results["errors"].append(str(e))
            print(f"é›¶æ ·æœ¬è¿ç§»åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def test_scale_invariant_metrics_basic(self) -> Dict:
        """æµ‹è¯•3: åŸºç¡€å°ºåº¦ä¸å˜æŒ‡æ ‡"""
        print("\n=== æµ‹è¯•3: åŸºç¡€å°ºåº¦ä¸å˜æŒ‡æ ‡ ===")
        results = {
            "test_name": "scale_invariant_metrics_basic",
            "status": "running",
            "metrics_tested": 0,
            "errors": []
        }
        
        try:
            # æµ‹è¯•ä¸åŒè§„æ¨¡åœºæ™¯çš„æŒ‡æ ‡è®¡ç®—
            scenarios = [(3, 2), (6, 4)]
            
            for n_uavs, n_targets in scenarios:
                print(f"æµ‹è¯•åœºæ™¯: {n_uavs} UAVs, {n_targets} ç›®æ ‡")
                
                uavs, targets, obstacles = self._create_test_scenario(n_uavs, n_targets)
                graph = DirectedGraph(uavs, targets, self.config.GRAPH_N_PHI, obstacles, self.config)
                env = UAVTaskEnv(uavs, targets, graph, obstacles, self.config, obs_mode="graph")
                
                # æ‰§è¡Œå‡ æ­¥è·å–å¥–åŠ±
                obs = env.reset()
                total_reward = 0
                
                for _ in range(5):
                    action = env.action_space.sample()
                    next_obs, reward, done, truncated, info = env.step(action)
                    total_reward += reward
                    obs = next_obs
                    
                    if done or truncated:
                        break
                
                # è®¡ç®—Per-Agent Reward
                n_active_uavs = len([u for u in env.uavs if np.any(u.resources > 0)])
                per_agent_reward = total_reward / n_active_uavs if n_active_uavs > 0 else 0
                
                print(f"  Per-Agent Reward: {per_agent_reward:.3f}")
                
                # è®¡ç®—Normalized Completion Score
                satisfied_targets = sum(1 for t in env.targets if np.all(t.remaining_resources <= 0))
                completion_rate = satisfied_targets / len(env.targets)
                
                print(f"  Completion Rate: {completion_rate:.3f}")
                
                results["metrics_tested"] += 1
            
            results["status"] = "passed"
            print("å°ºåº¦ä¸å˜æŒ‡æ ‡åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            results["status"] = "failed"
            results["errors"].append(str(e))
            print(f"å°ºåº¦ä¸å˜æŒ‡æ ‡åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def test_dual_mode_compatibility(self) -> Dict:
        """æµ‹è¯•4: åŒæ¨¡å¼å…¼å®¹æ€§"""
        print("\n=== æµ‹è¯•4: åŒæ¨¡å¼å…¼å®¹æ€§ ===")
        results = {
            "test_name": "dual_mode_compatibility",
            "status": "running",
            "modes_tested": 0,
            "errors": []
        }
        
        try:
            # åˆ›å»ºæµ‹è¯•åœºæ™¯
            uavs, targets, obstacles = self._create_test_scenario(4, 3)
            graph = DirectedGraph(uavs, targets, self.config.GRAPH_N_PHI, obstacles, self.config)
            
            # æµ‹è¯•æ‰å¹³æ¨¡å¼
            print("æµ‹è¯•æ‰å¹³æ¨¡å¼...")
            flat_env = UAVTaskEnv(uavs, targets, graph, obstacles, self.config, obs_mode="flat")
            flat_obs = flat_env.reset()
            
            assert isinstance(flat_obs, np.ndarray)
            assert len(flat_obs.shape) == 1
            print(f"  æ‰å¹³è§‚æµ‹å½¢çŠ¶: {flat_obs.shape}")
            
            # æµ‹è¯•ç¯å¢ƒæ­¥è¿›
            action = flat_env.action_space.sample()
            next_obs, reward, done, truncated, info = flat_env.step(action)
            print(f"  æ‰å¹³æ¨¡å¼æ­¥è¿›æˆåŠŸï¼Œå¥–åŠ±: {reward:.3f}")
            
            results["modes_tested"] += 1
            
            # æµ‹è¯•å›¾æ¨¡å¼
            print("æµ‹è¯•å›¾æ¨¡å¼...")
            graph_env = UAVTaskEnv(uavs, targets, graph, obstacles, self.config, obs_mode="graph")
            graph_obs = graph_env.reset()
            
            assert isinstance(graph_obs, dict)
            required_keys = ["uav_features", "target_features", "relative_positions", "distances", "masks"]
            for key in required_keys:
                assert key in graph_obs, f"ç¼ºå°‘é”®: {key}"
            
            print(f"  å›¾è§‚æµ‹é”®: {list(graph_obs.keys())}")
            
            # æµ‹è¯•ç¯å¢ƒæ­¥è¿›
            action = graph_env.action_space.sample()
            next_obs, reward, done, truncated, info = graph_env.step(action)
            print(f"  å›¾æ¨¡å¼æ­¥è¿›æˆåŠŸï¼Œå¥–åŠ±: {reward:.3f}")
            
            results["modes_tested"] += 1
            
            results["status"] = "passed"
            print("åŒæ¨¡å¼å…¼å®¹æ€§æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            results["status"] = "failed"
            results["errors"].append(str(e))
            print(f"åŒæ¨¡å¼å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def run_simple_tests(self) -> Dict:
        """è¿è¡Œç®€åŒ–ç‰ˆæµ‹è¯•å¥—ä»¶"""
        print("=" * 60)
        print("å¼€å§‹ç®€åŒ–ç‰ˆç«¯åˆ°ç«¯ç³»ç»Ÿé›†æˆæµ‹è¯•")
        print("=" * 60)
        
        start_time = time.time()
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_results = {
            "test_suite": "simple_end_to_end_integration",
            "start_time": start_time,
            "test_results": {},
            "summary": {}
        }
        
        # æ‰§è¡Œæµ‹è¯•
        test_results["test_results"]["curriculum_learning"] = self.test_curriculum_learning_basic()
        test_results["test_results"]["zero_shot_transfer"] = self.test_zero_shot_transfer_basic()
        test_results["test_results"]["scale_invariant_metrics"] = self.test_scale_invariant_metrics_basic()
        test_results["test_results"]["dual_mode_compatibility"] = self.test_dual_mode_compatibility()
        
        # ç”Ÿæˆæ‘˜è¦
        end_time = time.time()
        test_results["end_time"] = end_time
        test_results["total_duration"] = end_time - start_time
        
        # ç»Ÿè®¡ç»“æœ
        passed_tests = sum(1 for result in test_results["test_results"].values() 
                          if result["status"] == "passed")
        failed_tests = sum(1 for result in test_results["test_results"].values() 
                          if result["status"] == "failed")
        total_tests = len(test_results["test_results"])
        
        test_results["summary"] = {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "success_rate": passed_tests / total_tests if total_tests > 0 else 0,
            "overall_status": "PASSED" if failed_tests == 0 else "FAILED"
        }
        
        # æ‰“å°æ‘˜è¦
        self._print_summary(test_results)
        
        return test_results
    
    def _create_test_scenario(self, n_uavs: int, n_targets: int) -> Tuple[List[UAV], List[Target], List]:
        """åˆ›å»ºæµ‹è¯•åœºæ™¯"""
        # åˆ›å»ºUAVs
        uavs = []
        for i in range(n_uavs):
            uav = UAV(
                id=i,
                position=(np.random.uniform(0, 1000), np.random.uniform(0, 1000)),
                heading=np.random.uniform(0, 2*np.pi),
                resources=np.array([50.0, 30.0]),
                max_distance=200.0,
                velocity_range=(10.0, 50.0),
                economic_speed=25.0
            )
            uavs.append(uav)
        
        # åˆ›å»ºç›®æ ‡
        targets = []
        for i in range(n_targets):
            target = Target(
                id=i,
                position=(np.random.uniform(0, 1000), np.random.uniform(0, 1000)),
                resources=np.array([30.0, 20.0]),
                value=100.0
            )
            targets.append(target)
        
        # ç®€å•éšœç¢ç‰©ï¼ˆç©ºåˆ—è¡¨ï¼‰
        obstacles = []
        
        return uavs, targets, obstacles
    
    def _convert_obs_to_tensor(self, obs) -> torch.Tensor:
        """å°†è§‚æµ‹è½¬æ¢ä¸ºå¼ é‡"""
        if isinstance(obs, dict):
            # å›¾æ¨¡å¼è§‚æµ‹
            obs_tensor = {}
            for key, value in obs.items():
                if isinstance(value, dict):
                    obs_tensor[key] = {k: torch.FloatTensor(v).unsqueeze(0) for k, v in value.items()}
                else:
                    obs_tensor[key] = torch.FloatTensor(value).unsqueeze(0)
            return obs_tensor
        else:
            # æ‰å¹³æ¨¡å¼è§‚æµ‹
            return torch.FloatTensor(obs).unsqueeze(0)
    
    def _print_summary(self, test_results: Dict):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ç®€åŒ–ç‰ˆç«¯åˆ°ç«¯ç³»ç»Ÿé›†æˆæµ‹è¯•æ‘˜è¦")
        print("=" * 60)
        
        summary = test_results["summary"]
        print(f"æ€»æµ‹è¯•æ•°é‡: {summary['total_tests']}")
        print(f"é€šè¿‡æµ‹è¯•: {summary['passed_tests']}")
        print(f"å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
        print(f"æˆåŠŸç‡: {summary['success_rate']:.1%}")
        print(f"æ€»è€—æ—¶: {test_results['total_duration']:.2f}ç§’")
        print(f"æ•´ä½“çŠ¶æ€: {summary['overall_status']}")
        
        print("\nè¯¦ç»†æµ‹è¯•ç»“æœ:")
        for test_name, result in test_results["test_results"].items():
            status_symbol = "âœ“" if result["status"] == "passed" else "âœ—"
            print(f"  {status_symbol} {test_name}: {result['status'].upper()}")
            
            if result["status"] == "failed" and result.get("errors"):
                for error in result["errors"][:1]:  # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ªé”™è¯¯
                    print(f"    é”™è¯¯: {error}")
        
        print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    print("ç®€åŒ–ç‰ˆç«¯åˆ°ç«¯ç³»ç»Ÿé›†æˆæµ‹è¯•å¯åŠ¨")
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = SimpleEndToEndTester()
    
    try:
        # è¿è¡Œæµ‹è¯•
        results = tester.run_simple_tests()
        
        # æ ¹æ®æµ‹è¯•ç»“æœè®¾ç½®é€€å‡ºç 
        if results["summary"]["overall_status"] == "PASSED":
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿé›†æˆéªŒè¯æˆåŠŸï¼")
            return 0
        else:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼{results['summary']['failed_tests']}ä¸ªæµ‹è¯•æœªé€šè¿‡ã€‚")
            return 1
        
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 2


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)