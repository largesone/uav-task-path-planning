# -*- coding: utf-8 -*-
"""
ä»»åŠ¡19: ç«¯åˆ°ç«¯ç³»ç»Ÿé›†æˆæµ‹è¯•
å®ç°å®Œæ•´çš„è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæµç¨‹æµ‹è¯•ï¼ŒéªŒè¯é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›å’Œç³»ç»Ÿå®Œæ•´æ€§
"""

import os
import sys
import time
import json
import numpy as np
import torch
import tempfile
import shutil
from typing import Dict, List, Tuple, Optional
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from entities import UAV, Target
from environment import UAVTaskEnv, DirectedGraph
from config import Config
from scenarios import get_balanced_scenario, get_small_scenario, get_complex_scenario
from curriculum_stages import CurriculumStages, StageConfig
from transformer_gnn import TransformerGNN
from mixed_experience_replay import MixedExperienceReplay
from model_state_manager import ModelStateManager
from rollback_threshold_manager import RollbackThresholdManager

# TensorBoardæ”¯æŒ
try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False
    print("è­¦å‘Š: TensorBoardæœªå®‰è£…ï¼Œå°†è·³è¿‡TensorBoardæµ‹è¯•")

class EndToEndIntegrationTester:
    """ç«¯åˆ°ç«¯ç³»ç»Ÿé›†æˆæµ‹è¯•å™¨"""
    
    def __init__(self, test_output_dir: str = None):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨
        
        Args:
            test_output_dir: æµ‹è¯•è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºä¸´æ—¶ç›®å½•
        """
        self.test_output_dir = test_output_dir or tempfile.mkdtemp(prefix="e2e_test_")
        self.config = Config()
        self.curriculum_stages = CurriculumStages()
        self.test_results = {}
        self.tensorboard_dir = os.path.join(self.test_output_dir, "tensorboard")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.test_output_dir, exist_ok=True)
        os.makedirs(self.tensorboard_dir, exist_ok=True)
        
        print(f"ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•åˆå§‹åŒ–å®Œæˆ")
        print(f"æµ‹è¯•è¾“å‡ºç›®å½•: {self.test_output_dir}")
        print(f"TensorBoardç›®å½•: {self.tensorboard_dir}")
    
    def test_curriculum_learning_pipeline(self) -> Dict:
        """
        æµ‹è¯•1: å®Œæ•´çš„è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæµç¨‹æµ‹è¯•
        
        éªŒè¯ï¼š
        - è¯¾ç¨‹é˜¶æ®µé…ç½®æ­£ç¡®æ€§
        - é˜¶æ®µé—´åˆ‡æ¢æœºåˆ¶
        - å›é€€é—¨é™æœºåˆ¶
        - æ··åˆç»éªŒå›æ”¾
        """
        print("\n=== æµ‹è¯•1: è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæµç¨‹ ===")
        test_start_time = time.time()
        results = {
            "test_name": "curriculum_learning_pipeline",
            "status": "running",
            "stages_tested": [],
            "stage_transitions": [],
            "fallback_events": [],
            "errors": []
        }
        
        try:
            # 1.1 æµ‹è¯•è¯¾ç¨‹é˜¶æ®µé…ç½®
            print("1.1 éªŒè¯è¯¾ç¨‹é˜¶æ®µé…ç½®...")
            stages = self.curriculum_stages.stages
            assert len(stages) >= 4, f"è¯¾ç¨‹é˜¶æ®µæ•°é‡ä¸è¶³: {len(stages)}"
            
            for i, stage in enumerate(stages):
                assert stage.stage_id == i, f"é˜¶æ®µIDä¸åŒ¹é…: {stage.stage_id} != {i}"
                assert stage.n_uavs_range[0] <= stage.n_uavs_range[1], f"UAVæ•°é‡èŒƒå›´æ— æ•ˆ: {stage.n_uavs_range}"
                assert stage.n_targets_range[0] <= stage.n_targets_range[1], f"ç›®æ ‡æ•°é‡èŒƒå›´æ— æ•ˆ: {stage.n_targets_range}"
                
                results["stages_tested"].append({
                    "stage_id": stage.stage_id,
                    "stage_name": stage.stage_name,
                    "uav_range": stage.n_uavs_range,
                    "target_range": stage.n_targets_range,
                    "max_episodes": stage.max_episodes
                })
            
            print(f"âœ“ è¯¾ç¨‹é˜¶æ®µé…ç½®éªŒè¯é€šè¿‡ï¼Œå…±{len(stages)}ä¸ªé˜¶æ®µ")
            
            # 1.2 æµ‹è¯•æ¯ä¸ªé˜¶æ®µçš„åœºæ™¯ç”Ÿæˆ
            print("1.2 æµ‹è¯•å„é˜¶æ®µåœºæ™¯ç”Ÿæˆ...")
            for stage in stages[:2]:  # æµ‹è¯•å‰ä¸¤ä¸ªé˜¶æ®µä»¥èŠ‚çœæ—¶é—´
                print(f"  æµ‹è¯•é˜¶æ®µ{stage.stage_id}: {stage.stage_name}")
                
                # ç”Ÿæˆå¤šä¸ªéšæœºåœºæ™¯
                for _ in range(5):
                    n_uavs, n_targets = stage.get_random_scenario_size()
                    assert stage.n_uavs_range[0] <= n_uavs <= stage.n_uavs_range[1]
                    assert stage.n_targets_range[0] <= n_targets <= stage.n_targets_range[1]
                
                # åˆ›å»ºå®é™…åœºæ™¯å¹¶æµ‹è¯•ç¯å¢ƒ
                n_uavs, n_targets = stage.get_random_scenario_size()
                uavs, targets, obstacles = self._create_test_scenario(n_uavs, n_targets)
                
                # æµ‹è¯•åŒæ¨¡å¼ç¯å¢ƒ
                for obs_mode in ["flat", "graph"]:
                    graph = DirectedGraph(uavs, targets, self.config.GRAPH_N_PHI, obstacles, self.config)
                    env = UAVTaskEnv(uavs, targets, graph, obstacles, self.config, obs_mode=obs_mode)
                    
                    # æµ‹è¯•ç¯å¢ƒé‡ç½®å’Œæ­¥è¿›
                    obs = env.reset()
                    action = env.action_space.sample()
                    next_obs, reward, done, truncated, info = env.step(action)
                    
                    print(f"    âœ“ {obs_mode}æ¨¡å¼ç¯å¢ƒæµ‹è¯•é€šè¿‡")
            
            print("âœ“ å„é˜¶æ®µåœºæ™¯ç”Ÿæˆæµ‹è¯•é€šè¿‡")
            
            # 1.3 æµ‹è¯•é˜¶æ®µåˆ‡æ¢æœºåˆ¶
            print("1.3 æµ‹è¯•é˜¶æ®µåˆ‡æ¢æœºåˆ¶...")
            original_stage = self.curriculum_stages.current_stage_id
            
            # æµ‹è¯•æ¨è¿›
            if self.curriculum_stages.advance_to_next_stage():
                results["stage_transitions"].append({
                    "type": "advance",
                    "from_stage": original_stage,
                    "to_stage": self.curriculum_stages.current_stage_id
                })
                print(f"  âœ“ é˜¶æ®µæ¨è¿›æµ‹è¯•é€šè¿‡: {original_stage} -> {self.curriculum_stages.current_stage_id}")
            
            # æµ‹è¯•å›é€€
            if self.curriculum_stages.fallback_to_previous_stage():
                results["stage_transitions"].append({
                    "type": "fallback",
                    "from_stage": self.curriculum_stages.current_stage_id + 1,
                    "to_stage": self.curriculum_stages.current_stage_id
                })
                print(f"  âœ“ é˜¶æ®µå›é€€æµ‹è¯•é€šè¿‡")
            
            # æ¢å¤åŸå§‹é˜¶æ®µ
            self.curriculum_stages.current_stage_id = original_stage
            
            # 1.4 æµ‹è¯•æ··åˆç»éªŒå›æ”¾æœºåˆ¶
            print("1.4 æµ‹è¯•æ··åˆç»éªŒå›æ”¾æœºåˆ¶...")
            mixed_replay = MixedExperienceReplay(
                capacity=1000,
                current_stage_ratio=0.7,
                historical_ratio=0.3
            )
            
            # æ·»åŠ ä¸åŒé˜¶æ®µçš„ç»éªŒ
            for stage_id in range(3):
                for _ in range(50):
                    experience = {
                        'state': np.random.randn(10),
                        'action': np.random.randint(0, 5),
                        'reward': np.random.randn(),
                        'next_state': np.random.randn(10),
                        'done': False,
                        'stage_id': stage_id
                    }
                    mixed_replay.add_experience(experience, stage_id)
            
            # æµ‹è¯•é‡‡æ ·
            batch = mixed_replay.sample_batch(32, current_stage_id=1)
            assert len(batch) == 32, f"æ‰¹æ¬¡å¤§å°ä¸æ­£ç¡®: {len(batch)}"
            
            # éªŒè¯æ··åˆæ¯”ä¾‹
            stage_counts = {}
            for exp in batch:
                stage_id = exp['stage_id']
                stage_counts[stage_id] = stage_counts.get(stage_id, 0) + 1
            
            print(f"  âœ“ æ··åˆç»éªŒå›æ”¾æµ‹è¯•é€šè¿‡ï¼Œé˜¶æ®µåˆ†å¸ƒ: {stage_counts}")
            
            results["status"] = "passed"
            results["duration"] = time.time() - test_start_time
            print(f"âœ“ è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæµç¨‹æµ‹è¯•é€šè¿‡ (è€—æ—¶: {results['duration']:.2f}ç§’)")
            
        except Exception as e:
            results["status"] = "failed"
            results["errors"].append(str(e))
            print(f"âœ— è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def test_zero_shot_transfer_capability(self) -> Dict:
        """
        æµ‹è¯•2: éªŒè¯é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›
        
        éªŒè¯ï¼š
        - ä»å°è§„æ¨¡è®­ç»ƒåœºæ™¯åˆ°å¤§è§„æ¨¡æµ‹è¯•åœºæ™¯çš„è¿ç§»
        - å°ºåº¦ä¸å˜æ€§
        - æ€§èƒ½ä¿æŒåº¦
        """
        print("\n=== æµ‹è¯•2: é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›éªŒè¯ ===")
        test_start_time = time.time()
        results = {
            "test_name": "zero_shot_transfer",
            "status": "running",
            "training_scenarios": [],
            "test_scenarios": [],
            "transfer_results": [],
            "errors": []
        }
        
        try:
            # 2.1 åˆ›å»ºå°è§„æ¨¡è®­ç»ƒåœºæ™¯
            print("2.1 åˆ›å»ºå°è§„æ¨¡è®­ç»ƒåœºæ™¯...")
            small_uavs, small_targets, obstacles = self._create_test_scenario(3, 2)
            small_graph = DirectedGraph(small_uavs, small_targets, self.config.GRAPH_N_PHI, obstacles, self.config)
            small_env = UAVTaskEnv(small_uavs, small_targets, small_graph, obstacles, self.config, obs_mode="graph")
            
            results["training_scenarios"].append({
                "n_uavs": len(small_uavs),
                "n_targets": len(small_targets),
                "obs_space_shape": str(small_env.observation_space)
            })
            
            # 2.2 åˆ›å»ºTransformerGNNæ¨¡å‹
            print("2.2 åˆ›å»ºTransformerGNNæ¨¡å‹...")
            model_config = {
                "embed_dim": 64,
                "num_heads": 4,
                "num_layers": 2,
                "dropout": 0.1,
                "use_position_encoding": True,
                "use_noisy_linear": False,  # ç®€åŒ–æµ‹è¯•
                "use_local_attention": True,
                "k_adaptive": True,
                "k_min": 2,
                "k_max": 8
            }
            
            model = TransformerGNN(
                obs_space=small_env.observation_space,
                action_space=small_env.action_space,
                num_outputs=small_env.action_space.n,
                model_config=model_config,
                name="test_transformer_gnn"
            )
            
            print(f"  âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
            
            # 2.3 ç®€åŒ–è®­ç»ƒï¼ˆå¿«é€Ÿæ”¶æ•›æµ‹è¯•ï¼‰
            print("2.3 æ‰§è¡Œç®€åŒ–è®­ç»ƒ...")
            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
            
            training_losses = []
            for episode in range(50):  # ç®€åŒ–è®­ç»ƒè½®æ•°
                obs = small_env.reset()
                episode_loss = 0
                
                for step in range(10):  # ç®€åŒ–æ­¥æ•°
                    # è½¬æ¢è§‚æµ‹ä¸ºå¼ é‡
                    obs_tensor = self._convert_obs_to_tensor(obs)
                    
                    # å‰å‘ä¼ æ’­
                    logits, _ = model({"obs": obs_tensor}, [], [])
                    
                    # é€‰æ‹©åŠ¨ä½œ
                    action_probs = torch.softmax(logits, dim=-1)
                    action = torch.multinomial(action_probs, 1).item()
                    
                    # ç¯å¢ƒæ­¥è¿›
                    next_obs, reward, done, truncated, info = small_env.step(action)
                    
                    # ç®€åŒ–æŸå¤±è®¡ç®—ï¼ˆç­–ç•¥æ¢¯åº¦çš„ç®€åŒ–ç‰ˆæœ¬ï¼‰
                    loss = -torch.log(action_probs[0, action]) * reward
                    
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    
                    episode_loss += loss.item()
                    obs = next_obs
                    
                    if done or truncated:
                        break
                
                training_losses.append(episode_loss)
                
                if episode % 10 == 0:
                    print(f"  è®­ç»ƒè¿›åº¦: Episode {episode}, Loss: {episode_loss:.4f}")
            
            print(f"  âœ“ ç®€åŒ–è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {training_losses[-1]:.4f}")
            
            # 2.4 æµ‹è¯•ä¸åŒè§„æ¨¡åœºæ™¯çš„é›¶æ ·æœ¬è¿ç§»
            print("2.4 æµ‹è¯•é›¶æ ·æœ¬è¿ç§»...")
            test_scenarios = [
                (5, 3),   # ä¸­ç­‰è§„æ¨¡
                (8, 5),   # å¤§è§„æ¨¡
                (12, 8)   # è¶…å¤§è§„æ¨¡
            ]
            
            for n_uavs, n_targets in test_scenarios:
                print(f"  æµ‹è¯•åœºæ™¯: {n_uavs} UAVs, {n_targets} ç›®æ ‡")
                
                # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
                test_uavs, test_targets, test_obstacles = self._create_test_scenario(n_uavs, n_targets)
                test_graph = DirectedGraph(test_uavs, test_targets, self.config.GRAPH_N_PHI, test_obstacles, self.config)
                test_env = UAVTaskEnv(test_uavs, test_targets, test_graph, test_obstacles, self.config, obs_mode="graph")
                
                # æ‰§è¡Œæµ‹è¯•å›åˆ
                test_rewards = []
                test_completion_rates = []
                
                for test_episode in range(5):  # æ¯ä¸ªè§„æ¨¡æµ‹è¯•5å›åˆ
                    obs = test_env.reset()
                    episode_reward = 0
                    
                    for step in range(20):  # é™åˆ¶æ­¥æ•°
                        obs_tensor = self._convert_obs_to_tensor(obs)
                        
                        with torch.no_grad():
                            logits, _ = model({"obs": obs_tensor}, [], [])
                            action_probs = torch.softmax(logits, dim=-1)
                            action = torch.argmax(action_probs, dim=-1).item()
                        
                        next_obs, reward, done, truncated, info = test_env.step(action)
                        episode_reward += reward
                        obs = next_obs
                        
                        if done or truncated:
                            break
                    
                    # è®¡ç®—å®Œæˆç‡
                    completed_targets = sum(1 for t in test_env.targets if np.all(t.remaining_resources <= 0))
                    completion_rate = completed_targets / len(test_env.targets)
                    
                    test_rewards.append(episode_reward)
                    test_completion_rates.append(completion_rate)
                
                avg_reward = np.mean(test_rewards)
                avg_completion = np.mean(test_completion_rates)
                
                transfer_result = {
                    "scenario_size": (n_uavs, n_targets),
                    "avg_reward": float(avg_reward),
                    "avg_completion_rate": float(avg_completion),
                    "reward_std": float(np.std(test_rewards)),
                    "completion_std": float(np.std(test_completion_rates))
                }
                
                results["transfer_results"].append(transfer_result)
                results["test_scenarios"].append({
                    "n_uavs": n_uavs,
                    "n_targets": n_targets,
                    "obs_space_shape": str(test_env.observation_space)
                })
                
                print(f"    âœ“ å¹³å‡å¥–åŠ±: {avg_reward:.2f}, å¹³å‡å®Œæˆç‡: {avg_completion:.3f}")
            
            # 2.5 åˆ†æè¿ç§»æ€§èƒ½
            print("2.5 åˆ†æè¿ç§»æ€§èƒ½...")
            if len(results["transfer_results"]) >= 2:
                # è®¡ç®—æ€§èƒ½è¡°å‡
                baseline_completion = results["transfer_results"][0]["avg_completion_rate"]
                performance_retention = []
                
                for result in results["transfer_results"][1:]:
                    retention = result["avg_completion_rate"] / baseline_completion if baseline_completion > 0 else 0
                    performance_retention.append(retention)
                
                avg_retention = np.mean(performance_retention)
                results["performance_retention"] = float(avg_retention)
                
                print(f"  âœ“ å¹³å‡æ€§èƒ½ä¿æŒåº¦: {avg_retention:.3f}")
                
                # åˆ¤æ–­è¿ç§»æˆåŠŸæ ‡å‡†
                if avg_retention >= 0.7:  # ä¿æŒ70%ä»¥ä¸Šæ€§èƒ½è®¤ä¸ºè¿ç§»æˆåŠŸ
                    print("  âœ“ é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›éªŒè¯é€šè¿‡")
                else:
                    print(f"  âš  é›¶æ ·æœ¬è¿ç§»æ€§èƒ½è¾ƒä½ï¼Œä¿æŒåº¦: {avg_retention:.3f}")
            
            results["status"] = "passed"
            results["duration"] = time.time() - test_start_time
            print(f"âœ“ é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›éªŒè¯å®Œæˆ (è€—æ—¶: {results['duration']:.2f}ç§’)")
            
        except Exception as e:
            results["status"] = "failed"
            results["errors"].append(str(e))
            print(f"âœ— é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›éªŒè¯å¤±è´¥: {e}")
        
        return results
    
    def test_scale_invariant_metrics(self) -> Dict:
        """
        æµ‹è¯•3: éªŒè¯å°ºåº¦ä¸å˜æŒ‡æ ‡çš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§
        
        éªŒè¯ï¼š
        - Per-Agent Rewardè®¡ç®—
        - Normalized Completion Scoreè®¡ç®—
        - Efficiency Metricè®¡ç®—
        - ä¸åŒè§„æ¨¡åœºæ™¯ä¸‹çš„æŒ‡æ ‡ä¸€è‡´æ€§
        """
        print("\n=== æµ‹è¯•3: å°ºåº¦ä¸å˜æŒ‡æ ‡éªŒè¯ ===")
        test_start_time = time.time()
        results = {
            "test_name": "scale_invariant_metrics",
            "status": "running",
            "metric_tests": [],
            "consistency_tests": [],
            "errors": []
        }
        
        try:
            # 3.1 æµ‹è¯•Per-Agent Rewardè®¡ç®—
            print("3.1 æµ‹è¯•Per-Agent Rewardè®¡ç®—...")
            
            scenarios = [(3, 2), (6, 4), (12, 8)]  # ä¸åŒè§„æ¨¡åœºæ™¯
            per_agent_rewards = []
            
            for n_uavs, n_targets in scenarios:
                uavs, targets, obstacles = self._create_test_scenario(n_uavs, n_targets)
                graph = DirectedGraph(uavs, targets, self.config.GRAPH_N_PHI, obstacles, self.config)
                env = UAVTaskEnv(uavs, targets, graph, obstacles, self.config, obs_mode="graph")
                
                # æ‰§è¡Œå‡ æ­¥è·å–å¥–åŠ±
                obs = env.reset()
                total_reward = 0
                steps = 0
                
                for _ in range(10):
                    action = env.action_space.sample()
                    next_obs, reward, done, truncated, info = env.step(action)
                    total_reward += reward
                    steps += 1
                    obs = next_obs
                    
                    if done or truncated:
                        break
                
                # è®¡ç®—Per-Agent Reward
                n_active_uavs = len([u for u in env.uavs if np.any(u.resources > 0)])
                per_agent_reward = total_reward / n_active_uavs if n_active_uavs > 0 else 0
                
                per_agent_rewards.append({
                    "scenario": (n_uavs, n_targets),
                    "total_reward": float(total_reward),
                    "n_active_uavs": n_active_uavs,
                    "per_agent_reward": float(per_agent_reward)
                })
                
                print(f"  åœºæ™¯({n_uavs}, {n_targets}): Per-Agent Reward = {per_agent_reward:.3f}")
            
            results["metric_tests"].append({
                "metric_name": "per_agent_reward",
                "test_results": per_agent_rewards
            })
            
            # 3.2 æµ‹è¯•Normalized Completion Scoreè®¡ç®—
            print("3.2 æµ‹è¯•Normalized Completion Scoreè®¡ç®—...")
            
            completion_scores = []
            for n_uavs, n_targets in scenarios:
                uavs, targets, obstacles = self._create_test_scenario(n_uavs, n_targets)
                graph = DirectedGraph(uavs, targets, self.config.GRAPH_N_PHI, obstacles, self.config)
                env = UAVTaskEnv(uavs, targets, graph, obstacles, self.config, obs_mode="graph")
                
                # æ¨¡æ‹Ÿéƒ¨åˆ†å®ŒæˆçŠ¶æ€
                obs = env.reset()
                for _ in range(15):  # æ‰§è¡Œæ›´å¤šæ­¥éª¤
                    action = env.action_space.sample()
                    next_obs, reward, done, truncated, info = env.step(action)
                    obs = next_obs
                    if done or truncated:
                        break
                
                # è®¡ç®—Normalized Completion Score
                satisfied_targets = sum(1 for t in env.targets if np.all(t.remaining_resources <= 0))
                satisfied_targets_rate = satisfied_targets / len(env.targets)
                
                # è®¡ç®—æ‹¥å µæŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                total_allocations = sum(len(t.allocated_uavs) for t in env.targets)
                avg_congestion = total_allocations / len(env.targets) if len(env.targets) > 0 else 0
                congestion_metric = min(avg_congestion / n_uavs, 1.0) if n_uavs > 0 else 0
                
                normalized_completion_score = satisfied_targets_rate * (1 - congestion_metric)
                
                completion_scores.append({
                    "scenario": (n_uavs, n_targets),
                    "satisfied_targets_rate": float(satisfied_targets_rate),
                    "congestion_metric": float(congestion_metric),
                    "normalized_completion_score": float(normalized_completion_score)
                })
                
                print(f"  åœºæ™¯({n_uavs}, {n_targets}): Completion Score = {normalized_completion_score:.3f}")
            
            results["metric_tests"].append({
                "metric_name": "normalized_completion_score",
                "test_results": completion_scores
            })
            
            # 3.3 æµ‹è¯•Efficiency Metricè®¡ç®—
            print("3.3 æµ‹è¯•Efficiency Metricè®¡ç®—...")
            
            efficiency_metrics = []
            for n_uavs, n_targets in scenarios:
                uavs, targets, obstacles = self._create_test_scenario(n_uavs, n_targets)
                graph = DirectedGraph(uavs, targets, self.config.GRAPH_N_PHI, obstacles, self.config)
                env = UAVTaskEnv(uavs, targets, graph, obstacles, self.config, obs_mode="graph")
                
                obs = env.reset()
                total_flight_distance = 0
                
                for _ in range(10):
                    action = env.action_space.sample()
                    
                    # è®°å½•é£è¡Œè·ç¦»
                    target_idx, uav_idx, phi_idx = env._action_to_assignment(action)
                    if target_idx < len(env.targets) and uav_idx < len(env.uavs):
                        target = env.targets[target_idx]
                        uav = env.uavs[uav_idx]
                        distance = np.linalg.norm(np.array(target.position) - np.array(uav.current_position))
                        total_flight_distance += distance
                    
                    next_obs, reward, done, truncated, info = env.step(action)
                    obs = next_obs
                    if done or truncated:
                        break
                
                # è®¡ç®—Efficiency Metric
                completed_targets = sum(1 for t in env.targets if np.all(t.remaining_resources <= 0))
                efficiency_metric = completed_targets / total_flight_distance if total_flight_distance > 0 else 0
                
                efficiency_metrics.append({
                    "scenario": (n_uavs, n_targets),
                    "completed_targets": completed_targets,
                    "total_flight_distance": float(total_flight_distance),
                    "efficiency_metric": float(efficiency_metric)
                })
                
                print(f"  åœºæ™¯({n_uavs}, {n_targets}): Efficiency = {efficiency_metric:.6f}")
            
            results["metric_tests"].append({
                "metric_name": "efficiency_metric",
                "test_results": efficiency_metrics
            })
            
            # 3.4 æµ‹è¯•æŒ‡æ ‡ä¸€è‡´æ€§
            print("3.4 æµ‹è¯•æŒ‡æ ‡ä¸€è‡´æ€§...")
            
            # æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
            for metric_test in results["metric_tests"]:
                metric_name = metric_test["metric_name"]
                test_results = metric_test["test_results"]
                
                if metric_name == "per_agent_reward":
                    # Per-Agent Rewardåº”è¯¥ç›¸å¯¹ç¨³å®š
                    rewards = [r["per_agent_reward"] for r in test_results]
                    reward_std = np.std(rewards)
                    consistency_score = 1.0 / (1.0 + reward_std)  # æ ‡å‡†å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜
                    
                elif metric_name == "normalized_completion_score":
                    # Completion Scoreåº”è¯¥åœ¨[0, 1]èŒƒå›´å†…
                    scores = [r["normalized_completion_score"] for r in test_results]
                    valid_range = all(0 <= s <= 1 for s in scores)
                    consistency_score = 1.0 if valid_range else 0.0
                    
                elif metric_name == "efficiency_metric":
                    # Efficiency Metricåº”è¯¥ä¸ºæ­£æ•°
                    efficiencies = [r["efficiency_metric"] for r in test_results]
                    all_positive = all(e >= 0 for e in efficiencies)
                    consistency_score = 1.0 if all_positive else 0.0
                
                results["consistency_tests"].append({
                    "metric_name": metric_name,
                    "consistency_score": float(consistency_score),
                    "details": f"æµ‹è¯•é€šè¿‡" if consistency_score > 0.8 else f"ä¸€è‡´æ€§è¾ƒä½"
                })
                
                print(f"  {metric_name}ä¸€è‡´æ€§å¾—åˆ†: {consistency_score:.3f}")
            
            results["status"] = "passed"
            results["duration"] = time.time() - test_start_time
            print(f"âœ“ å°ºåº¦ä¸å˜æŒ‡æ ‡éªŒè¯å®Œæˆ (è€—æ—¶: {results['duration']:.2f}ç§’)")
            
        except Exception as e:
            results["status"] = "failed"
            results["errors"].append(str(e))
            print(f"âœ— å°ºåº¦ä¸å˜æŒ‡æ ‡éªŒè¯å¤±è´¥: {e}")
        
        return results
    
    def test_tensorboard_logging(self) -> Dict:
        """
        æµ‹è¯•4: TensorBoardæ—¥å¿—è®°å½•å’Œå¯è§†åŒ–åŠŸèƒ½å®Œæ•´æ€§
        
        éªŒè¯ï¼š
        - TensorBoard Writeråˆ›å»ºå’Œä½¿ç”¨
        - å„ç±»æŒ‡æ ‡è®°å½•
        - æ—¥å¿—æ–‡ä»¶ç”Ÿæˆ
        - å¯è§†åŒ–æ•°æ®å®Œæ•´æ€§
        """
        print("\n=== æµ‹è¯•4: TensorBoardæ—¥å¿—è®°å½•éªŒè¯ ===")
        test_start_time = time.time()
        results = {
            "test_name": "tensorboard_logging",
            "status": "running",
            "tensorboard_available": TENSORBOARD_AVAILABLE,
            "log_files_created": [],
            "metrics_logged": [],
            "errors": []
        }
        
        try:
            if not TENSORBOARD_AVAILABLE:
                print("âš  TensorBoardä¸å¯ç”¨ï¼Œè·³è¿‡ç›¸å…³æµ‹è¯•")
                results["status"] = "skipped"
                results["skip_reason"] = "TensorBoard not available"
                return results
            
            # 4.1 åˆ›å»ºTensorBoard Writer
            print("4.1 åˆ›å»ºTensorBoard Writer...")
            tb_test_dir = os.path.join(self.tensorboard_dir, "test_run")
            writer = SummaryWriter(tb_test_dir)
            
            print(f"  âœ“ TensorBoard Writeråˆ›å»ºæˆåŠŸ: {tb_test_dir}")
            
            # 4.2 æµ‹è¯•å„ç±»æŒ‡æ ‡è®°å½•
            print("4.2 æµ‹è¯•æŒ‡æ ‡è®°å½•...")
            
            # è®°å½•æ ‡é‡æŒ‡æ ‡
            scalar_metrics = [
                ("Training/Loss", [1.5, 1.2, 0.9, 0.7, 0.5]),
                ("Training/Reward", [10.0, 15.0, 20.0, 25.0, 30.0]),
                ("Training/Completion_Rate", [0.2, 0.4, 0.6, 0.7, 0.8]),
                ("Training/Per_Agent_Reward", [3.3, 3.7, 4.1, 4.5, 4.8]),
                ("Training/Efficiency_Metric", [0.001, 0.002, 0.003, 0.004, 0.005])
            ]
            
            for metric_name, values in scalar_metrics:
                for step, value in enumerate(values):
                    writer.add_scalar(metric_name, value, step)
                
                results["metrics_logged"].append({
                    "metric_name": metric_name,
                    "num_points": len(values),
                    "value_range": (min(values), max(values))
                })
                
                print(f"  âœ“ è®°å½•æ ‡é‡æŒ‡æ ‡: {metric_name} ({len(values)}ä¸ªæ•°æ®ç‚¹)")
            
            # è®°å½•ç›´æ–¹å›¾æ•°æ®
            print("4.3 æµ‹è¯•ç›´æ–¹å›¾è®°å½•...")
            for step in range(5):
                # æ¨¡æ‹Ÿç½‘ç»œæƒé‡åˆ†å¸ƒ
                weights = torch.randn(100) * (0.5 + step * 0.1)
                writer.add_histogram("Weights/layer1", weights, step)
                
                # æ¨¡æ‹Ÿæ¢¯åº¦åˆ†å¸ƒ
                gradients = torch.randn(100) * (0.1 + step * 0.02)
                writer.add_histogram("Gradients/layer1", gradients, step)
            
            results["metrics_logged"].extend([
                {"metric_name": "Weights/layer1", "type": "histogram", "num_points": 5},
                {"metric_name": "Gradients/layer1", "type": "histogram", "num_points": 5}
            ])
            
            print("  âœ“ ç›´æ–¹å›¾è®°å½•å®Œæˆ")
            
            # 4.4 è®°å½•è¯¾ç¨‹å­¦ä¹ ç›¸å…³æŒ‡æ ‡
            print("4.4 æµ‹è¯•è¯¾ç¨‹å­¦ä¹ æŒ‡æ ‡è®°å½•...")
            
            curriculum_metrics = [
                ("Curriculum/Current_Stage", [0, 0, 1, 1, 2]),
                ("Curriculum/Stage_Progress", [0.2, 0.5, 0.1, 0.8, 0.3]),
                ("Curriculum/Fallback_Count", [0, 0, 0, 1, 1]),
                ("Curriculum/Mixed_Replay_Ratio", [0.7, 0.7, 0.7, 0.7, 0.7])
            ]
            
            for metric_name, values in curriculum_metrics:
                for step, value in enumerate(values):
                    writer.add_scalar(metric_name, step, value)
                
                results["metrics_logged"].append({
                    "metric_name": metric_name,
                    "type": "curriculum",
                    "num_points": len(values)
                })
            
            print("  âœ“ è¯¾ç¨‹å­¦ä¹ æŒ‡æ ‡è®°å½•å®Œæˆ")
            
            # 4.5 å…³é—­Writerå¹¶æ£€æŸ¥æ–‡ä»¶
            print("4.5 æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ç”Ÿæˆ...")
            writer.close()
            
            # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
            if os.path.exists(tb_test_dir):
                log_files = list(Path(tb_test_dir).rglob("*"))
                results["log_files_created"] = [str(f) for f in log_files]
                
                print(f"  âœ“ ç”Ÿæˆæ—¥å¿—æ–‡ä»¶æ•°é‡: {len(log_files)}")
                for log_file in log_files[:5]:  # æ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶
                    print(f"    - {log_file.name}")
                
                if len(log_files) > 5:
                    print(f"    ... è¿˜æœ‰{len(log_files) - 5}ä¸ªæ–‡ä»¶")
            
            # 4.6 éªŒè¯æ—¥å¿—æ•°æ®å®Œæ•´æ€§
            print("4.6 éªŒè¯æ—¥å¿—æ•°æ®å®Œæ•´æ€§...")
            
            total_metrics = len(results["metrics_logged"])
            scalar_metrics_count = len([m for m in results["metrics_logged"] if m.get("type") != "histogram"])
            histogram_metrics_count = len([m for m in results["metrics_logged"] if m.get("type") == "histogram"])
            
            print(f"  âœ“ æ€»æŒ‡æ ‡æ•°é‡: {total_metrics}")
            print(f"  âœ“ æ ‡é‡æŒ‡æ ‡: {scalar_metrics_count}")
            print(f"  âœ“ ç›´æ–¹å›¾æŒ‡æ ‡: {histogram_metrics_count}")
            
            # éªŒè¯å…³é”®æŒ‡æ ‡æ˜¯å¦éƒ½å·²è®°å½•
            required_metrics = ["Training/Loss", "Training/Reward", "Training/Completion_Rate"]
            logged_metric_names = [m["metric_name"] for m in results["metrics_logged"]]
            
            missing_metrics = [m for m in required_metrics if m not in logged_metric_names]
            if missing_metrics:
                results["errors"].append(f"ç¼ºå°‘å…³é”®æŒ‡æ ‡: {missing_metrics}")
                print(f"  âš  ç¼ºå°‘å…³é”®æŒ‡æ ‡: {missing_metrics}")
            else:
                print("  âœ“ æ‰€æœ‰å…³é”®æŒ‡æ ‡å·²è®°å½•")
            
            results["status"] = "passed"
            results["duration"] = time.time() - test_start_time
            print(f"âœ“ TensorBoardæ—¥å¿—è®°å½•éªŒè¯å®Œæˆ (è€—æ—¶: {results['duration']:.2f}ç§’)")
            
        except Exception as e:
            results["status"] = "failed"
            results["errors"].append(str(e))
            print(f"âœ— TensorBoardæ—¥å¿—è®°å½•éªŒè¯å¤±è´¥: {e}")
        
        return results
    
    def test_solution_output_compatibility(self) -> Dict:
        """
        æµ‹è¯•5: éªŒè¯æ–¹æ¡ˆè¾“å‡ºå’Œè¯„ä¼°æµç¨‹çš„å®Œæ•´æ€§
        
        éªŒè¯ï¼š
        - TransformerGNNè¾“å‡ºæ ¼å¼å…¼å®¹æ€§
        - æ–¹æ¡ˆè½¬æ¢æ¥å£
        - è¯„ä¼°æµç¨‹å®Œæ•´æ€§
        - ä¸ç°æœ‰ç³»ç»Ÿçš„å…¼å®¹æ€§
        """
        print("\n=== æµ‹è¯•5: æ–¹æ¡ˆè¾“å‡ºå…¼å®¹æ€§éªŒè¯ ===")
        test_start_time = time.time()
        results = {
            "test_name": "solution_output_compatibility",
            "status": "running",
            "output_format_tests": [],
            "compatibility_tests": [],
            "evaluation_tests": [],
            "errors": []
        }
        
        try:
            # 5.1 æµ‹è¯•TransformerGNNè¾“å‡ºæ ¼å¼
            print("5.1 æµ‹è¯•TransformerGNNè¾“å‡ºæ ¼å¼...")
            
            # åˆ›å»ºæµ‹è¯•ç¯å¢ƒå’Œæ¨¡å‹
            uavs, targets, obstacles = self._create_test_scenario(4, 3)
            graph = DirectedGraph(uavs, targets, self.config.GRAPH_N_PHI, obstacles, self.config)
            env = UAVTaskEnv(uavs, targets, graph, obstacles, self.config, obs_mode="graph")
            
            model_config = {
                "embed_dim": 32,
                "num_heads": 2,
                "num_layers": 1,
                "use_noisy_linear": False
            }
            
            model = TransformerGNN(
                obs_space=env.observation_space,
                action_space=env.action_space,
                num_outputs=env.action_space.n,
                model_config=model_config,
                name="compatibility_test"
            )
            
            # æµ‹è¯•æ¨¡å‹è¾“å‡º
            obs = env.reset()
            obs_tensor = self._convert_obs_to_tensor(obs)
            
            with torch.no_grad():
                logits, state = model({"obs": obs_tensor}, [], [])
                value = model.value_function()
            
            # éªŒè¯è¾“å‡ºæ ¼å¼
            assert logits.shape[0] == 1, f"æ‰¹æ¬¡ç»´åº¦é”™è¯¯: {logits.shape[0]}"
            assert logits.shape[1] == env.action_space.n, f"åŠ¨ä½œç»´åº¦é”™è¯¯: {logits.shape[1]} != {env.action_space.n}"
            assert value.shape[0] == 1, f"å€¼å‡½æ•°ç»´åº¦é”™è¯¯: {value.shape[0]}"
            
            results["output_format_tests"].append({
                "test_name": "transformer_gnn_output",
                "logits_shape": list(logits.shape),
                "value_shape": list(value.shape),
                "action_space_size": env.action_space.n,
                "status": "passed"
            })
            
            print(f"  âœ“ TransformerGNNè¾“å‡ºæ ¼å¼éªŒè¯é€šè¿‡")
            print(f"    - Logitså½¢çŠ¶: {logits.shape}")
            print(f"    - Valueå½¢çŠ¶: {value.shape}")
            
            # 5.2 æµ‹è¯•åŠ¨ä½œé€‰æ‹©å’Œè½¬æ¢
            print("5.2 æµ‹è¯•åŠ¨ä½œé€‰æ‹©å’Œè½¬æ¢...")
            
            # æµ‹è¯•ä¸åŒçš„åŠ¨ä½œé€‰æ‹©ç­–ç•¥
            action_selection_methods = [
                ("greedy", lambda x: torch.argmax(x, dim=-1)),
                ("sampling", lambda x: torch.multinomial(torch.softmax(x, dim=-1), 1)),
                ("epsilon_greedy", lambda x: torch.randint(0, x.shape[-1], (1,)) if np.random.random() < 0.1 
                                           else torch.argmax(x, dim=-1))
            ]
            
            for method_name, action_fn in action_selection_methods:
                action = action_fn(logits).item()
                
                # éªŒè¯åŠ¨ä½œæœ‰æ•ˆæ€§
                assert 0 <= action < env.action_space.n, f"åŠ¨ä½œè¶…å‡ºèŒƒå›´: {action}"
                
                # æµ‹è¯•åŠ¨ä½œè½¬æ¢
                target_idx, uav_idx, phi_idx = env._action_to_assignment(action)
                assert 0 <= target_idx < len(env.targets), f"ç›®æ ‡ç´¢å¼•æ— æ•ˆ: {target_idx}"
                assert 0 <= uav_idx < len(env.uavs), f"UAVç´¢å¼•æ— æ•ˆ: {uav_idx}"
                assert 0 <= phi_idx < graph.n_phi, f"æ–¹å‘ç´¢å¼•æ— æ•ˆ: {phi_idx}"
                
                results["output_format_tests"].append({
                    "test_name": f"action_selection_{method_name}",
                    "action": action,
                    "target_idx": target_idx,
                    "uav_idx": uav_idx,
                    "phi_idx": phi_idx,
                    "status": "passed"
                })
                
                print(f"  âœ“ {method_name}åŠ¨ä½œé€‰æ‹©æµ‹è¯•é€šè¿‡: action={action} -> ({target_idx}, {uav_idx}, {phi_idx})")
            
            # 5.3 æµ‹è¯•æ–¹æ¡ˆç”Ÿæˆå’Œè¯„ä¼°
            print("5.3 æµ‹è¯•æ–¹æ¡ˆç”Ÿæˆå’Œè¯„ä¼°...")
            
            # ç”Ÿæˆå®Œæ•´æ–¹æ¡ˆ
            obs = env.reset()
            solution_steps = []
            
            for step in range(10):  # é™åˆ¶æ­¥æ•°
                obs_tensor = self._convert_obs_to_tensor(obs)
                
                with torch.no_grad():
                    logits, _ = model({"obs": obs_tensor}, [], [])
                    action = torch.argmax(logits, dim=-1).item()
                
                # è®°å½•æ–¹æ¡ˆæ­¥éª¤
                target_idx, uav_idx, phi_idx = env._action_to_assignment(action)
                solution_steps.append({
                    "step": step,
                    "action": action,
                    "target_id": env.targets[target_idx].id,
                    "uav_id": env.uavs[uav_idx].id,
                    "phi_idx": phi_idx
                })
                
                next_obs, reward, done, truncated, info = env.step(action)
                obs = next_obs
                
                if done or truncated:
                    break
            
            # è¯„ä¼°æ–¹æ¡ˆè´¨é‡
            completed_targets = sum(1 for t in env.targets if np.all(t.remaining_resources <= 0))
            completion_rate = completed_targets / len(env.targets)
            total_reward = sum(step.get("reward", 0) for step in solution_steps)
            
            evaluation_result = {
                "solution_steps": len(solution_steps),
                "completed_targets": completed_targets,
                "total_targets": len(env.targets),
                "completion_rate": float(completion_rate),
                "total_reward": float(total_reward)
            }
            
            results["evaluation_tests"].append({
                "test_name": "solution_generation",
                "result": evaluation_result,
                "status": "passed"
            })
            
            print(f"  âœ“ æ–¹æ¡ˆç”Ÿæˆæµ‹è¯•é€šè¿‡:")
            print(f"    - æ–¹æ¡ˆæ­¥æ•°: {len(solution_steps)}")
            print(f"    - å®Œæˆç‡: {completion_rate:.3f}")
            print(f"    - æ€»å¥–åŠ±: {total_reward:.2f}")
            
            # 5.4 æµ‹è¯•ä¸ç°æœ‰ç³»ç»Ÿçš„å…¼å®¹æ€§
            print("5.4 æµ‹è¯•ç³»ç»Ÿå…¼å®¹æ€§...")
            
            # æµ‹è¯•æ‰å¹³æ¨¡å¼å…¼å®¹æ€§
            flat_env = UAVTaskEnv(uavs, targets, graph, obstacles, self.config, obs_mode="flat")
            flat_obs = flat_env.reset()
            
            # éªŒè¯æ‰å¹³è§‚æµ‹æ ¼å¼
            assert isinstance(flat_obs, np.ndarray), f"æ‰å¹³è§‚æµ‹åº”ä¸ºnumpyæ•°ç»„: {type(flat_obs)}"
            assert len(flat_obs.shape) == 1, f"æ‰å¹³è§‚æµ‹åº”ä¸º1ç»´: {flat_obs.shape}"
            
            results["compatibility_tests"].append({
                "test_name": "flat_mode_compatibility",
                "obs_shape": list(flat_obs.shape),
                "obs_type": str(type(flat_obs)),
                "status": "passed"
            })
            
            print(f"  âœ“ æ‰å¹³æ¨¡å¼å…¼å®¹æ€§éªŒè¯é€šè¿‡")
            
            # æµ‹è¯•å›¾æ¨¡å¼å…¼å®¹æ€§
            graph_obs = env.reset()
            assert isinstance(graph_obs, dict), f"å›¾è§‚æµ‹åº”ä¸ºå­—å…¸: {type(graph_obs)}"
            
            required_keys = ["uav_features", "target_features", "relative_positions", "distances", "masks"]
            missing_keys = [key for key in required_keys if key not in graph_obs]
            
            if missing_keys:
                results["errors"].append(f"å›¾è§‚æµ‹ç¼ºå°‘é”®: {missing_keys}")
            else:
                results["compatibility_tests"].append({
                    "test_name": "graph_mode_compatibility",
                    "obs_keys": list(graph_obs.keys()),
                    "status": "passed"
                })
                print(f"  âœ“ å›¾æ¨¡å¼å…¼å®¹æ€§éªŒè¯é€šè¿‡")
            
            results["status"] = "passed"
            results["duration"] = time.time() - test_start_time
            print(f"âœ“ æ–¹æ¡ˆè¾“å‡ºå…¼å®¹æ€§éªŒè¯å®Œæˆ (è€—æ—¶: {results['duration']:.2f}ç§’)")
            
        except Exception as e:
            results["status"] = "failed"
            results["errors"].append(str(e))
            print(f"âœ— æ–¹æ¡ˆè¾“å‡ºå…¼å®¹æ€§éªŒè¯å¤±è´¥: {e}")
        
        return results
    
    def run_comprehensive_test(self) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•
        
        Returns:
            å®Œæ•´çš„æµ‹è¯•ç»“æœå­—å…¸
        """
        print("=" * 80)
        print("å¼€å§‹ç«¯åˆ°ç«¯ç³»ç»Ÿé›†æˆæµ‹è¯•")
        print("=" * 80)
        
        start_time = time.time()
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_results = {
            "test_suite": "end_to_end_integration",
            "start_time": start_time,
            "test_results": {},
            "summary": {}
        }
        
        # æµ‹è¯•1: è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæµç¨‹
        test_results["test_results"]["curriculum_learning"] = self.test_curriculum_learning_pipeline()
        
        # æµ‹è¯•2: é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›
        test_results["test_results"]["zero_shot_transfer"] = self.test_zero_shot_transfer_capability()
        
        # æµ‹è¯•3: å°ºåº¦ä¸å˜æŒ‡æ ‡
        test_results["test_results"]["scale_invariant_metrics"] = self.test_scale_invariant_metrics()
        
        # æµ‹è¯•4: TensorBoardæ—¥å¿—è®°å½•
        test_results["test_results"]["tensorboard_logging"] = self.test_tensorboard_logging()
        
        # æµ‹è¯•5: æ–¹æ¡ˆè¾“å‡ºå…¼å®¹æ€§
        test_results["test_results"]["solution_compatibility"] = self.test_solution_output_compatibility()
        
        # ç”Ÿæˆæµ‹è¯•æ‘˜è¦
        end_time = time.time()
        test_results["end_time"] = end_time
        test_results["total_duration"] = end_time - start_time
        
        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        passed_tests = sum(1 for result in test_results["test_results"].values() 
                          if result["status"] == "passed")
        failed_tests = sum(1 for result in test_results["test_results"].values() 
                          if result["status"] == "failed")
        skipped_tests = sum(1 for result in test_results["test_results"].values() 
                           if result["status"] == "skipped")
        total_tests = len(test_results["test_results"])
        
        test_results["summary"] = {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "skipped_tests": skipped_tests,
            "success_rate": passed_tests / total_tests if total_tests > 0 else 0,
            "overall_status": "PASSED" if failed_tests == 0 else "FAILED"
        }
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        self._save_test_results(test_results)
        
        # æ‰“å°æµ‹è¯•æ‘˜è¦
        self._print_test_summary(test_results)
        
        return test_results
    
    def _create_test_scenario(self, n_uavs: int, n_targets: int) -> Tuple[List[UAV], List[Target], List]:
        """åˆ›å»ºæµ‹è¯•åœºæ™¯"""
        # åˆ›å»ºUAVs
        uavs = []
        for i in range(n_uavs):
            uav = UAV(
                id=i,
                position=(np.random.uniform(0, 1000), np.random.uniform(0, 1000)),
                resources=np.array([50.0, 30.0]),
                max_distance=200.0,
                velocity_range=(10.0, 50.0)
            )
            uavs.append(uav)
        
        # åˆ›å»ºç›®æ ‡
        targets = []
        for i in range(n_targets):
            target = Target(
                id=i,
                position=(np.random.uniform(0, 1000), np.random.uniform(0, 1000)),
                resources=np.array([30.0, 20.0]),
                value=100.0
            )
            targets.append(target)
        
        # ç®€å•éšœç¢ç‰©ï¼ˆç©ºåˆ—è¡¨ï¼‰
        obstacles = []
        
        return uavs, targets, obstacles
    
    def _convert_obs_to_tensor(self, obs) -> torch.Tensor:
        """å°†è§‚æµ‹è½¬æ¢ä¸ºå¼ é‡"""
        if isinstance(obs, dict):
            # å›¾æ¨¡å¼è§‚æµ‹
            obs_tensor = {}
            for key, value in obs.items():
                if isinstance(value, dict):
                    obs_tensor[key] = {k: torch.FloatTensor(v).unsqueeze(0) for k, v in value.items()}
                else:
                    obs_tensor[key] = torch.FloatTensor(value).unsqueeze(0)
            return obs_tensor
        else:
            # æ‰å¹³æ¨¡å¼è§‚æµ‹
            return torch.FloatTensor(obs).unsqueeze(0)
    
    def _save_test_results(self, test_results: Dict):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        results_file = os.path.join(self.test_output_dir, "end_to_end_test_results.json")
        
        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        def convert_for_json(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, dict):
                return {k: convert_for_json(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_for_json(item) for item in obj]
            else:
                return obj
        
        json_results = convert_for_json(test_results)
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        
        print(f"\næµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    def _print_test_summary(self, test_results: Dict):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ç«¯åˆ°ç«¯ç³»ç»Ÿé›†æˆæµ‹è¯•æ‘˜è¦")
        print("=" * 80)
        
        summary = test_results["summary"]
        print(f"æ€»æµ‹è¯•æ•°é‡: {summary['total_tests']}")
        print(f"é€šè¿‡æµ‹è¯•: {summary['passed_tests']}")
        print(f"å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
        print(f"è·³è¿‡æµ‹è¯•: {summary['skipped_tests']}")
        print(f"æˆåŠŸç‡: {summary['success_rate']:.1%}")
        print(f"æ€»è€—æ—¶: {test_results['total_duration']:.2f}ç§’")
        print(f"æ•´ä½“çŠ¶æ€: {summary['overall_status']}")
        
        print("\nè¯¦ç»†æµ‹è¯•ç»“æœ:")
        for test_name, result in test_results["test_results"].items():
            status_symbol = "âœ“" if result["status"] == "passed" else "âœ—" if result["status"] == "failed" else "âš "
            duration = result.get("duration", 0)
            print(f"  {status_symbol} {test_name}: {result['status'].upper()} ({duration:.2f}s)")
            
            if result["status"] == "failed" and result.get("errors"):
                for error in result["errors"][:2]:  # æ˜¾ç¤ºå‰ä¸¤ä¸ªé”™è¯¯
                    print(f"    é”™è¯¯: {error}")
        
        print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    print("ç«¯åˆ°ç«¯ç³»ç»Ÿé›†æˆæµ‹è¯•å¯åŠ¨")
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = EndToEndIntegrationTester()
    
    try:
        # è¿è¡Œå®Œæ•´æµ‹è¯•
        results = tester.run_comprehensive_test()
        
        # æ ¹æ®æµ‹è¯•ç»“æœè®¾ç½®é€€å‡ºç 
        if results["summary"]["overall_status"] == "PASSED":
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿé›†æˆéªŒè¯æˆåŠŸï¼")
            exit_code = 0
        else:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼{results['summary']['failed_tests']}ä¸ªæµ‹è¯•æœªé€šè¿‡ã€‚")
            exit_code = 1
        
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        exit_code = 2
    
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        if hasattr(tester, 'test_output_dir') and tester.test_output_dir.startswith('/tmp'):
            print(f"\næ¸…ç†ä¸´æ—¶ç›®å½•: {tester.test_output_dir}")
            # shutil.rmtree(tester.test_output_dir, ignore_errors=True)
    
    return exit_code


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
