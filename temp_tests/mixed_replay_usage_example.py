#!/usr/bin/env python3
"""
æ··åˆç»éªŒå›æ”¾æœºåˆ¶ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•åœ¨å®é™…é¡¹ç›®ä¸­ä½¿ç”¨æ··åˆç»éªŒå›æ”¾æœºåˆ¶
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from mixed_experience_replay import MixedExperienceReplay, ExperiencePoolManager
from rllib_mixed_replay_integration import create_mixed_replay_config, CurriculumLearningCallback

def example_basic_usage():
    """åŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""
    print("ğŸ“š åŸºç¡€ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # 1. åˆ›å»ºæ··åˆç»éªŒå›æ”¾å®ä¾‹
    replay = MixedExperienceReplay(
        capacity_per_stage=10000,      # æ¯é˜¶æ®µå®¹é‡
        current_stage_ratio=0.7,       # å½“å‰é˜¶æ®µé‡‡æ ·æ¯”ä¾‹
        historical_stage_ratio=0.3,    # å†å²é˜¶æ®µé‡‡æ ·æ¯”ä¾‹
        max_stages_to_keep=3,          # æœ€å¤šä¿ç•™çš„å†å²é˜¶æ®µæ•°
        min_historical_samples=500     # å¯ç”¨æ··åˆé‡‡æ ·çš„æœ€å°å†å²æ ·æœ¬æ•°
    )
    
    print(f"âœ… åˆ›å»ºæ··åˆç»éªŒå›æ”¾å®ä¾‹ï¼Œå½“å‰é˜¶æ®µ: {replay.current_stage}")
    
    # 2. ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ - ç®€å•åœºæ™¯
    print("\nğŸ¯ ç¬¬ä¸€é˜¶æ®µè®­ç»ƒï¼ˆç®€å•åœºæ™¯ï¼‰")
    replay.set_current_stage(0)
    
    # æ¨¡æ‹Ÿæ·»åŠ ç¬¬ä¸€é˜¶æ®µçš„ç»éªŒ
    for episode in range(5):
        for step in range(20):
            experience = {
                'obs': f'stage0_obs_{episode}_{step}',
                'action': step % 3,
                'reward': 0.1 * step,
                'done': step == 19,
                'info': {'episode': episode, 'step': step}
            }
            replay.add_experience(experience)
    
    print(f"æ·»åŠ äº† {len(replay)} ä¸ªç¬¬ä¸€é˜¶æ®µç»éªŒ")
    
    # ç¬¬ä¸€é˜¶æ®µé‡‡æ ·ï¼ˆåªä»å½“å‰é˜¶æ®µï¼‰
    batch = replay.sample_mixed_batch(10)
    print(f"ç¬¬ä¸€é˜¶æ®µé‡‡æ ·äº† {len(batch)} ä¸ªç»éªŒ")
    
    # 3. ç¬¬äºŒé˜¶æ®µè®­ç»ƒ - ä¸­ç­‰å¤æ‚åº¦
    print("\nğŸ¯ ç¬¬äºŒé˜¶æ®µè®­ç»ƒï¼ˆä¸­ç­‰å¤æ‚åº¦ï¼‰")
    replay.set_current_stage(1)
    
    # æ¨¡æ‹Ÿæ·»åŠ ç¬¬äºŒé˜¶æ®µçš„ç»éªŒ
    for episode in range(3):
        for step in range(30):
            experience = {
                'obs': f'stage1_obs_{episode}_{step}',
                'action': (step + episode) % 4,
                'reward': 0.15 * step + 0.1 * episode,
                'done': step == 29,
                'info': {'episode': episode, 'step': step, 'complexity': 'medium'}
            }
            replay.add_experience(experience)
    
    print(f"å½“å‰é˜¶æ®µæœ‰ {len(replay)} ä¸ªç»éªŒ")
    
    # ç¬¬äºŒé˜¶æ®µæ··åˆé‡‡æ ·ï¼ˆ70%å½“å‰ + 30%å†å²ï¼‰
    batch = replay.sample_mixed_batch(20)
    print(f"ç¬¬äºŒé˜¶æ®µæ··åˆé‡‡æ ·äº† {len(batch)} ä¸ªç»éªŒ")
    
    # ç»Ÿè®¡ä¸åŒé˜¶æ®µçš„ç»éªŒæ¯”ä¾‹
    stage_counts = {}
    for exp in batch:
        stage_id = exp.get('stage_id', 'unknown')
        stage_counts[stage_id] = stage_counts.get(stage_id, 0) + 1
    
    print("æ··åˆé‡‡æ ·ç»“æœ:")
    for stage_id, count in stage_counts.items():
        percentage = (count / len(batch)) * 100
        print(f"  é˜¶æ®µ {stage_id}: {count} ä¸ªç»éªŒ ({percentage:.1f}%)")
    
    # 4. è·å–ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
    stats = replay.get_statistics()
    print(f"å½“å‰é˜¶æ®µ: {stats['current_stage']}")
    print(f"é˜¶æ®µç¼“å†²åŒºå¤§å°: {stats['stage_buffer_sizes']}")
    print(f"æ€»æ·»åŠ æ ·æœ¬æ•°: {stats['total_samples_added']}")
    print(f"æ··åˆæ‰¹æ¬¡ç”Ÿæˆæ•°: {stats['mixed_batches_generated']}")

def example_experience_pool_manager():
    """ç»éªŒæ± ç®¡ç†å™¨ä½¿ç”¨ç¤ºä¾‹"""
    print("\n\nğŸ“š ç»éªŒæ± ç®¡ç†å™¨ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # åˆ›å»ºç»éªŒæ± ç®¡ç†å™¨
    manager = ExperiencePoolManager(
        default_capacity=5000,
        max_stages=4,
        auto_cleanup=True
    )
    
    print("âœ… åˆ›å»ºç»éªŒæ± ç®¡ç†å™¨")
    
    # ä¸ºä¸åŒé˜¶æ®µåˆ›å»ºç»éªŒæ± 
    stages = [0, 1, 2]
    for stage_id in stages:
        pool = manager.create_pool(stage_id, capacity=3000 + stage_id * 1000)
        print(f"åˆ›å»ºé˜¶æ®µ {stage_id} ç»éªŒæ± ï¼Œå®¹é‡: {pool.capacity_per_stage}")
        
        # æ·»åŠ ä¸€äº›ç¤ºä¾‹ç»éªŒ
        for i in range(50 + stage_id * 20):
            experience = {
                'obs': f'stage{stage_id}_sample_{i}',
                'action': i % (stage_id + 2),
                'reward': 0.1 * i * (stage_id + 1),
                'stage_info': f'complexity_level_{stage_id}'
            }
            manager.add_experience_to_stage(stage_id, experience)
    
    # é˜¶æ®µé—´ç»éªŒè½¬ç§»
    print("\nğŸ”„ é˜¶æ®µé—´ç»éªŒè½¬ç§»")
    transferred = manager.transfer_experiences(0, 1, ratio=0.1)
    print(f"ä»é˜¶æ®µ0å‘é˜¶æ®µ1è½¬ç§»äº† {transferred} ä¸ªç»éªŒ")
    
    # å…¨å±€ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š å…¨å±€ç»Ÿè®¡ä¿¡æ¯")
    global_stats = manager.get_global_statistics()
    print(f"æ´»è·ƒç»éªŒæ± æ•°é‡: {global_stats['total_active_pools']}")
    print(f"æ´»è·ƒé˜¶æ®µ: {global_stats['active_stages']}")
    
    # å†…å­˜ä½¿ç”¨ä¼°ç®—
    memory_info = manager.get_memory_usage_estimate()
    print(f"æ€»æ ·æœ¬æ•°: {memory_info['total_samples']}")
    print(f"ä¼°ç®—å†…å­˜ä½¿ç”¨: {memory_info['estimated_memory_mb']:.2f} MB")
    print(f"å¹³å‡æ¯æ± æ ·æœ¬æ•°: {memory_info['average_samples_per_pool']:.1f}")

def example_curriculum_learning():
    """è¯¾ç¨‹å­¦ä¹ ä½¿ç”¨ç¤ºä¾‹"""
    print("\n\nğŸ“š è¯¾ç¨‹å­¦ä¹ ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # å®šä¹‰è¯¾ç¨‹å­¦ä¹ é…ç½®
    curriculum_config = {
        'stages': {
            0: {  # ç®€å•åœºæ™¯ï¼š2-3ä¸ªUAVï¼Œ1-2ä¸ªç›®æ ‡
                'max_episodes': 1000,
                'success_threshold': 0.8,
                'rollback_threshold': 0.6,
                'description': 'ç®€å•åœºæ™¯ - åŸºç¡€åè°ƒ'
            },
            1: {  # ä¸­ç­‰åœºæ™¯ï¼š4-6ä¸ªUAVï¼Œ3-4ä¸ªç›®æ ‡
                'max_episodes': 1500,
                'success_threshold': 0.85,
                'rollback_threshold': 0.65,
                'description': 'ä¸­ç­‰åœºæ™¯ - å¤æ‚åè°ƒ'
            },
            2: {  # å¤æ‚åœºæ™¯ï¼š8-12ä¸ªUAVï¼Œ5-8ä¸ªç›®æ ‡
                'max_episodes': 2000,
                'success_threshold': 0.9,
                'rollback_threshold': 0.7,
                'description': 'å¤æ‚åœºæ™¯ - å¤§è§„æ¨¡åè°ƒ'
            }
        }
    }
    
    # åˆ›å»ºè¯¾ç¨‹å­¦ä¹ å›è°ƒ
    callback = CurriculumLearningCallback(curriculum_config)
    print("âœ… åˆ›å»ºè¯¾ç¨‹å­¦ä¹ å›è°ƒ")
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„é˜¶æ®µåˆ¤æ–­
    print("\nğŸ¯ æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹")
    
    # æ¨¡æ‹Ÿä¸åŒæ€§èƒ½æ°´å¹³çš„episodeæ•°æ®
    test_scenarios = [
        {'episode_reward_mean': 0.85, 'description': 'é«˜æ€§èƒ½ - åº”è¯¥æ¨è¿›'},
        {'episode_reward_mean': 0.75, 'description': 'ä¸­ç­‰æ€§èƒ½ - ç»§ç»­è®­ç»ƒ'},
        {'episode_reward_mean': 0.55, 'description': 'ä½æ€§èƒ½ - å¯èƒ½å›é€€'},
    ]
    
    for scenario in test_scenarios:
        episode_data = {'episode_reward_mean': scenario['episode_reward_mean']}
        
        should_advance = callback._should_advance_stage(episode_data)
        should_rollback = callback._should_rollback_stage(episode_data)
        
        print(f"\nåœºæ™¯: {scenario['description']}")
        print(f"  å¹³å‡å¥–åŠ±: {scenario['episode_reward_mean']}")
        print(f"  æ˜¯å¦æ¨è¿›: {should_advance}")
        print(f"  æ˜¯å¦å›é€€: {should_rollback}")

def example_rllib_integration():
    """Ray RLlibé›†æˆä½¿ç”¨ç¤ºä¾‹"""
    print("\n\nğŸ“š Ray RLlibé›†æˆä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    try:
        # åˆ›å»ºDQNé…ç½®
        dqn_config = create_mixed_replay_config(
            algorithm_type="DQN",
            mixed_replay_config={
                'current_stage_ratio': 0.7,
                'historical_stage_ratio': 0.3,
                'max_stages_to_keep': 3,
                'buffer_capacity': 50000
            }
        )
        print("âœ… åˆ›å»ºDQNæ··åˆå›æ”¾é…ç½®")
        
        # åˆ›å»ºPPOé…ç½®
        ppo_config = create_mixed_replay_config(
            algorithm_type="PPO",
            mixed_replay_config={
                'current_stage_ratio': 0.8,
                'historical_stage_ratio': 0.2,
                'max_stages_to_keep': 2,
                'buffer_capacity': 25000
            }
        )
        print("âœ… åˆ›å»ºPPOæ··åˆå›æ”¾é…ç½®")
        
        print("\nğŸ”§ é…ç½®è¯¦æƒ…:")
        print(f"DQNæ··åˆå›æ”¾é…ç½®: {dqn_config.mixed_replay_config}")
        print(f"PPOæ··åˆå›æ”¾é…ç½®: {ppo_config.mixed_replay_config}")
        
    except Exception as e:
        print(f"âš ï¸  RLlibé›†æˆç¤ºä¾‹è·³è¿‡: {e}")

if __name__ == "__main__":
    print("ğŸš€ æ··åˆç»éªŒå›æ”¾æœºåˆ¶ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    try:
        example_basic_usage()
        example_experience_pool_manager()
        example_curriculum_learning()
        example_rllib_integration()
        
        print("\n\nğŸŠ æ‰€æœ‰ä½¿ç”¨ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("\nğŸ’¡ æç¤º:")
        print("- åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œæ ¹æ®å…·ä½“åœºæ™¯è°ƒæ•´å‚æ•°")
        print("- ç›‘æ§ç»Ÿè®¡ä¿¡æ¯ä»¥ä¼˜åŒ–æ€§èƒ½")
        print("- ç»“åˆè¯¾ç¨‹å­¦ä¹ ç­–ç•¥è·å¾—æœ€ä½³æ•ˆæœ")
        
    except Exception as e:
        print(f"\nâŒ ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)