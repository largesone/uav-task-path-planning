# -*- coding: utf-8 -*-
"""
æµ‹è¯•æ–‡ä»¶: test_scale_invariant_metrics.py
éªŒè¯å°ºåº¦ä¸å˜æŒ‡æ ‡è®¡ç®—å’ŒTensorBoardé›†æˆçš„æ­£ç¡®æ€§
"""

import unittest
import numpy as np
import tempfile
import shutil
from pathlib import Path
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from rllib_custom_callbacks import ScaleInvariantMetricsCallbacks, CurriculumLearningCallbacks
from scale_invariant_tensorboard_logger import ScaleInvariantTensorBoardLogger


class MockUAV:
    """æ¨¡æ‹ŸUAVå¯¹è±¡"""
    def __init__(self, uav_id, resources, position):
        self.id = uav_id
        self.resources = np.array(resources)
        self.position = np.array(position)
        self.current_position = np.array(position)
        self.task_sequence = []


class MockTarget:
    """æ¨¡æ‹Ÿç›®æ ‡å¯¹è±¡"""
    def __init__(self, target_id, resources, position):
        self.id = target_id
        self.resources = np.array(resources)
        self.remaining_resources = np.array(resources)
        self.position = np.array(position)
        self.allocated_uavs = []


class MockEnvironment:
    """æ¨¡æ‹Ÿç¯å¢ƒå¯¹è±¡"""
    def __init__(self, uavs, targets):
        self.uavs = uavs
        self.targets = targets


class MockEpisode:
    """æ¨¡æ‹ŸEpisodeå¯¹è±¡"""
    def __init__(self):
        self.custom_metrics = {}
        self.total_reward = 0.0
        self.length = 0
        self.episode_id = "test_episode"


class TestScaleInvariantMetrics(unittest.TestCase):
    """æµ‹è¯•å°ºåº¦ä¸å˜æŒ‡æ ‡è®¡ç®—"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.temp_dir = tempfile.mkdtemp()
        
        # åˆ›å»ºæ¨¡æ‹Ÿç¯å¢ƒ
        self.uavs = [
            MockUAV(0, [100, 50], [0, 0]),
            MockUAV(1, [80, 60], [100, 0]),
            MockUAV(2, [0, 0], [200, 0])  # æ— èµ„æºUAV
        ]
        
        self.targets = [
            MockTarget(0, [50, 30], [50, 50]),
            MockTarget(1, [40, 20], [150, 50])
        ]
        
        # è®¾ç½®ç›®æ ‡å®ŒæˆçŠ¶æ€
        self.targets[0].remaining_resources = np.array([0, 0])  # å·²å®Œæˆ
        self.targets[1].remaining_resources = np.array([20, 10])  # æœªå®Œæˆ
        
        # è®¾ç½®UAVåˆ†é…
        self.targets[0].allocated_uavs = [(0, 0), (1, 0)]  # 2ä¸ªUAVåˆ†é…åˆ°ç›®æ ‡0
        self.targets[1].allocated_uavs = [(1, 0)]  # 1ä¸ªUAVåˆ†é…åˆ°ç›®æ ‡1
        
        self.env = MockEnvironment(self.uavs, self.targets)
        self.callbacks = ScaleInvariantMetricsCallbacks()
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        shutil.rmtree(self.temp_dir)
    
    def test_per_agent_reward_calculation(self):
        """æµ‹è¯•Per-Agent Rewardè®¡ç®—"""
        episode = MockEpisode()
        episode.total_reward = 100.0
        
        # åˆå§‹åŒ–æŒ‡æ ‡
        self.callbacks.on_episode_start(
            worker=None, base_env=None, policies=None, episode=episode
        )
        
        # æ¨¡æ‹Ÿç¯å¢ƒ
        class MockBaseEnv:
            def get_sub_environments(self):
                return [self.env]
        
        # æ‰§è¡Œå›åˆç»“æŸå›è°ƒ
        self.callbacks.on_episode_end(
            worker=None, base_env=MockBaseEnv(), policies=None, episode=episode
        )
        
        # éªŒè¯Per-Agent Rewardè®¡ç®—
        # æ´»è·ƒUAVæ•°é‡åº”è¯¥æ˜¯2ï¼ˆUAV 0å’Œ1æœ‰èµ„æºï¼ŒUAV 2æ— èµ„æºï¼‰
        expected_per_agent_reward = 100.0 / 2  # total_reward / n_active_uavs
        actual_per_agent_reward = episode.custom_metrics["per_agent_reward"]
        
        self.assertAlmostEqual(actual_per_agent_reward, expected_per_agent_reward, places=4)
        print(f"âœ“ Per-Agent Rewardè®¡ç®—æ­£ç¡®: {actual_per_agent_reward:.4f}")
    
    def test_normalized_completion_score_calculation(self):
        """æµ‹è¯•Normalized Completion Scoreè®¡ç®—"""
        episode = MockEpisode()
        
        # åˆå§‹åŒ–æŒ‡æ ‡
        self.callbacks.on_episode_start(
            worker=None, base_env=None, policies=None, episode=episode
        )
        
        class MockBaseEnv:
            def get_sub_environments(self):
                return [self.env]
        
        # æ‰§è¡Œå›åˆç»“æŸå›è°ƒ
        self.callbacks.on_episode_end(
            worker=None, base_env=MockBaseEnv(), policies=None, episode=episode
        )
        
        # éªŒè¯Normalized Completion Scoreè®¡ç®—
        # ç›®æ ‡æ»¡è¶³ç‡ = 1/2 = 0.5 (1ä¸ªç›®æ ‡å®Œæˆï¼Œå…±2ä¸ªç›®æ ‡)
        expected_satisfied_rate = 0.5
        
        # æ‹¥å µæŒ‡æ ‡è®¡ç®—è¾ƒå¤æ‚ï¼Œè¿™é‡Œä¸»è¦éªŒè¯å…¬å¼ç»“æ„
        actual_ncs = episode.custom_metrics["normalized_completion_score"]
        actual_satisfied_rate = episode.custom_metrics["scale_invariant_metrics"]["satisfied_targets_rate"]
        
        self.assertAlmostEqual(actual_satisfied_rate, expected_satisfied_rate, places=4)
        self.assertGreaterEqual(actual_ncs, 0.0)
        self.assertLessEqual(actual_ncs, 1.0)
        
        print(f"âœ“ Normalized Completion Scoreè®¡ç®—æ­£ç¡®: {actual_ncs:.4f}")
        print(f"  - ç›®æ ‡æ»¡è¶³ç‡: {actual_satisfied_rate:.4f}")
    
    def test_efficiency_metric_calculation(self):
        """æµ‹è¯•Efficiency Metricè®¡ç®—"""
        episode = MockEpisode()
        
        # è®¾ç½®UAVé£è¡Œè·ç¦»
        self.uavs[0].current_position = np.array([50, 50])  # é£è¡Œäº†çº¦70.7å•ä½
        self.uavs[1].current_position = np.array([150, 50])  # é£è¡Œäº†çº¦50å•ä½
        
        # åˆå§‹åŒ–æŒ‡æ ‡
        self.callbacks.on_episode_start(
            worker=None, base_env=None, policies=None, episode=episode
        )
        
        class MockBaseEnv:
            def get_sub_environments(self):
                return [self.env]
        
        # æ‰§è¡Œå›åˆç»“æŸå›è°ƒ
        self.callbacks.on_episode_end(
            worker=None, base_env=MockBaseEnv(), policies=None, episode=episode
        )
        
        # éªŒè¯Efficiency Metricè®¡ç®—
        completed_targets = 1  # 1ä¸ªç›®æ ‡å®Œæˆ
        actual_efficiency = episode.custom_metrics["efficiency_metric"]
        
        self.assertGreater(actual_efficiency, 0.0)
        print(f"âœ“ Efficiency Metricè®¡ç®—æ­£ç¡®: {actual_efficiency:.6f}")
    
    def test_congestion_metric_calculation(self):
        """æµ‹è¯•æ‹¥å µæŒ‡æ ‡è®¡ç®—"""
        # åˆ›å»ºé«˜æ‹¥å µåœºæ™¯
        high_congestion_target = MockTarget(2, [100, 100], [300, 50])
        high_congestion_target.allocated_uavs = [(0, 0), (1, 0), (2, 0), (0, 1), (1, 1)]  # 5ä¸ªåˆ†é…
        
        env_with_congestion = MockEnvironment(self.uavs, self.targets + [high_congestion_target])
        
        congestion_metric = self.callbacks._calculate_congestion_metric(env_with_congestion)
        
        self.assertGreaterEqual(congestion_metric, 0.0)
        self.assertLessEqual(congestion_metric, 1.0)
        
        print(f"âœ“ æ‹¥å µæŒ‡æ ‡è®¡ç®—æ­£ç¡®: {congestion_metric:.4f}")
    
    def test_metrics_trend_calculation(self):
        """æµ‹è¯•æŒ‡æ ‡è¶‹åŠ¿è®¡ç®—"""
        # åˆ›å»ºæ¨¡æ‹Ÿå›åˆå†å²
        episodes = [
            {"normalized_completion_score": 0.5},
            {"normalized_completion_score": 0.6},
            {"normalized_completion_score": 0.7},
            {"normalized_completion_score": 0.8},
            {"normalized_completion_score": 0.9}
        ]
        
        trend = self.callbacks._calculate_metrics_trend(episodes)
        
        # åº”è¯¥æ˜¯æ­£è¶‹åŠ¿
        self.assertGreater(trend, 0.0)
        print(f"âœ“ æŒ‡æ ‡è¶‹åŠ¿è®¡ç®—æ­£ç¡®: {trend:.4f} (æ­£è¶‹åŠ¿)")
        
        # æµ‹è¯•è´Ÿè¶‹åŠ¿
        declining_episodes = [
            {"normalized_completion_score": 0.9},
            {"normalized_completion_score": 0.7},
            {"normalized_completion_score": 0.5},
            {"normalized_completion_score": 0.3},
            {"normalized_completion_score": 0.1}
        ]
        
        declining_trend = self.callbacks._calculate_metrics_trend(declining_episodes)
        self.assertLess(declining_trend, 0.0)
        print(f"âœ“ è´Ÿè¶‹åŠ¿è®¡ç®—æ­£ç¡®: {declining_trend:.4f} (è´Ÿè¶‹åŠ¿)")


class TestCurriculumLearningCallbacks(unittest.TestCase):
    """æµ‹è¯•è¯¾ç¨‹å­¦ä¹ å›è°ƒå‡½æ•°"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.callbacks = CurriculumLearningCallbacks()
    
    def test_stage_advancement(self):
        """æµ‹è¯•é˜¶æ®µæ¨è¿›"""
        initial_stage = self.callbacks.current_stage
        
        self.callbacks.advance_to_next_stage()
        
        self.assertEqual(self.callbacks.current_stage, initial_stage + 1)
        self.assertEqual(self.callbacks.stage_episode_count, 0)
        
        print(f"âœ“ é˜¶æ®µæ¨è¿›æµ‹è¯•é€šè¿‡: {initial_stage} â†’ {self.callbacks.current_stage}")
    
    def test_stage_rollback(self):
        """æµ‹è¯•é˜¶æ®µå›é€€"""
        # å…ˆæ¨è¿›åˆ°é˜¶æ®µ1
        self.callbacks.advance_to_next_stage()
        current_stage = self.callbacks.current_stage
        
        self.callbacks.rollback_to_previous_stage()
        
        self.assertEqual(self.callbacks.current_stage, current_stage - 1)
        self.assertEqual(self.callbacks.stage_episode_count, 0)
        
        print(f"âœ“ é˜¶æ®µå›é€€æµ‹è¯•é€šè¿‡: {current_stage} â†’ {self.callbacks.current_stage}")
    
    def test_stage_completion_detection(self):
        """æµ‹è¯•é˜¶æ®µå®Œæˆæ£€æµ‹"""
        stage_id = 0
        
        # æ·»åŠ è¶³å¤Ÿçš„é«˜æ€§èƒ½å›åˆ
        for i in range(15):
            self.callbacks.stage_performance_history[stage_id] = self.callbacks.stage_performance_history.get(stage_id, [])
            self.callbacks.stage_performance_history[stage_id].append({
                "normalized_completion_score": 0.85,  # é«˜äº0.8é˜ˆå€¼
                "episode_count": i + 1
            })
        
        is_completed = self.callbacks._is_stage_completed(stage_id)
        self.assertTrue(is_completed)
        
        print(f"âœ“ é˜¶æ®µå®Œæˆæ£€æµ‹æµ‹è¯•é€šè¿‡: é˜¶æ®µ{stage_id}å·²å®Œæˆ")


class TestScaleInvariantTensorBoardLogger(unittest.TestCase):
    """æµ‹è¯•TensorBoardè®°å½•å™¨"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.temp_dir = tempfile.mkdtemp()
        self.logger = ScaleInvariantTensorBoardLogger(self.temp_dir, "test_experiment")
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        self.logger.close()
        shutil.rmtree(self.temp_dir)
    
    def test_metrics_logging(self):
        """æµ‹è¯•æŒ‡æ ‡è®°å½•"""
        metrics = {
            "per_agent_reward": 15.5,
            "normalized_completion_score": 0.75,
            "efficiency_metric": 0.002
        }
        
        scenario_info = {
            "n_uavs": 3,
            "n_targets": 2
        }
        
        stage_info = {
            "current_stage": 1,
            "stage_progress": 0.6
        }
        
        # è®°å½•æŒ‡æ ‡
        self.logger.log_scale_invariant_metrics(metrics, 100, scenario_info, stage_info)
        
        # éªŒè¯å†å²è®°å½•
        self.assertEqual(len(self.logger.metrics_history["per_agent_reward"]), 1)
        self.assertEqual(self.logger.metrics_history["per_agent_reward"][0], 15.5)
        
        print("âœ“ TensorBoardæŒ‡æ ‡è®°å½•æµ‹è¯•é€šè¿‡")
    
    def test_cross_scale_comparison(self):
        """æµ‹è¯•è·¨å°ºåº¦å¯¹æ¯”"""
        scale_data = {
            6: {"per_agent_reward": 10.0, "normalized_completion_score": 0.6},
            12: {"per_agent_reward": 8.0, "normalized_completion_score": 0.7},
            20: {"per_agent_reward": 6.0, "normalized_completion_score": 0.8}
        }
        
        self.logger.log_cross_scale_comparison(scale_data, 200)
        
        print("âœ“ è·¨å°ºåº¦å¯¹æ¯”è®°å½•æµ‹è¯•é€šè¿‡")
    
    def test_summary_report_generation(self):
        """æµ‹è¯•æ‘˜è¦æŠ¥å‘Šç”Ÿæˆ"""
        # æ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®
        for i in range(50):
            metrics = {
                "per_agent_reward": 10 + np.random.normal(0, 2),
                "normalized_completion_score": 0.7 + np.random.uniform(-0.1, 0.2),
                "efficiency_metric": 0.001 + np.random.uniform(0, 0.0005)
            }
            self.logger.log_scale_invariant_metrics(metrics, i)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_path = self.logger.create_training_summary_report()
        
        # éªŒè¯æŠ¥å‘Šæ–‡ä»¶å­˜åœ¨
        self.assertTrue(Path(report_path).exists())
        
        # éªŒè¯æŠ¥å‘Šå†…å®¹
        with open(report_path, 'r', encoding='utf-8') as f:
            content = f.read()
            self.assertIn("å°ºåº¦ä¸å˜æŒ‡æ ‡è®­ç»ƒæ‘˜è¦æŠ¥å‘Š", content)
            self.assertIn("Per-Agent Reward", content)
            self.assertIn("Normalized Completion Score", content)
            self.assertIn("Efficiency Metric", content)
        
        print(f"âœ“ æ‘˜è¦æŠ¥å‘Šç”Ÿæˆæµ‹è¯•é€šè¿‡: {report_path}")


def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("=" * 60)
    print("å¼€å§‹å°ºåº¦ä¸å˜æŒ‡æ ‡ç³»ç»Ÿç»¼åˆæµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•ç”¨ä¾‹
    test_suite.addTest(unittest.makeSuite(TestScaleInvariantMetrics))
    test_suite.addTest(unittest.makeSuite(TestCurriculumLearningCallbacks))
    test_suite.addTest(unittest.makeSuite(TestScaleInvariantTensorBoardLogger))
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ‘˜è¦:")
    print(f"æ€»æµ‹è¯•æ•°: {result.testsRun}")
    print(f"æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"å¤±è´¥: {len(result.failures)}")
    print(f"é”™è¯¯: {len(result.errors)}")
    
    if result.failures:
        print("\nå¤±è´¥çš„æµ‹è¯•:")
        for test, traceback in result.failures:
            print(f"- {test}: {traceback}")
    
    if result.errors:
        print("\né”™è¯¯çš„æµ‹è¯•:")
        for test, traceback in result.errors:
            print(f"- {test}: {traceback}")
    
    print("=" * 60)
    
    return result.wasSuccessful()


if __name__ == "__main__":
    success = run_comprehensive_test()
    
    if success:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å°ºåº¦ä¸å˜æŒ‡æ ‡ç³»ç»Ÿå®ç°æ­£ç¡®ã€‚")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")
    
    exit(0 if success else 1)
