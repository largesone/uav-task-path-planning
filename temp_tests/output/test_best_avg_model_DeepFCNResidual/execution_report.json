{
  "scenario_name": "test_best_avg_model",
  "network_type": "DeepFCNResidual",
  "training_time": 6.323599815368652,
  "task_assignments_count": 4,
  "evaluation_metrics": {
    "completion_rate": 0.6666666666666667,
    "satisfied_targets_rate": 1.0,
    "target_satisfaction_rate": 0.5,
    "resource_satisfaction_rate": 0.8333333333333334,
    "resource_utilization_rate": 0.8333333333333334,
    "total_reward_score": 2900.0,
    "marginal_utility_score": 66.66666666666666,
    "resource_efficiency_score": 2500.0,
    "distance_cost_score": -0.0,
    "target_completion_score": 333.33333333333337,
    "completion_bonus": 0,
    "completed_targets_count": 1,
    "total_targets_count": 2,
    "total_contribution": 10.0,
    "total_demand": 12,
    "total_initial_supply": 12
  },
  "training_statistics": {
    "reward_mean": -254.8315090323908,
    "reward_std": 392.87803555206926,
    "reward_max": 412.73059746891823,
    "reward_min": -785.8395057324126,
    "reward_final": -785.8395057324126,
    "reward_improvement": -738.8632024742691,
    "loss_mean": 2485.8505181728265,
    "loss_std": 1682.0881484629524,
    "loss_final": 3969.245864868164,
    "loss_min": 0.0
  },
  "training_history_summary": {
    "total_episodes": 13,
    "final_reward": -785.8395057324126,
    "final_loss": 3969.245864868164,
    "final_epsilon": 0.8432232235918236,
    "final_completion_rate": 0.5
  },
  "config_summary": {
    "episodes": 30,
    "learning_rate": 0.0001,
    "batch_size": 64,
    "gamma": 0.99,
    "epsilon_start": 0.9,
    "epsilon_end": 0.1,
    "epsilon_decay": 0.9995
  }
}