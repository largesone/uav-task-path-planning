# -*- coding: utf-8 -*-
"""
æµ‹è¯•æ–‡ä»¶: test_per_agent_reward_normalization.py
æè¿°: éªŒè¯Per-Agentå¥–åŠ±å½’ä¸€åŒ–åŠŸèƒ½çš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§

æµ‹è¯•ç›®æ ‡:
1. éªŒè¯ä¸åŒæ— äººæœºæ•°é‡ä¸‹çš„å¥–åŠ±ä¸€è‡´æ€§
2. éªŒè¯æ‹¥å µæƒ©ç½šç­‰æ•°å€¼ä¼šéšæ— äººæœºæ•°é‡Nå¢é•¿çš„å¥–åŠ±é¡¹è¢«æ­£ç¡®å½’ä¸€åŒ–
3. éªŒè¯å¥–åŠ±ç»„ä»¶è·Ÿè¸ªåŠŸèƒ½
4. éªŒè¯å½’ä¸€åŒ–å‰åçš„å¥–åŠ±å€¼è®°å½•
"""

import numpy as np
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from entities import UAV, Target
from environment import DirectedGraph, UAVTaskEnv

class MockConfig:
    """æ¨¡æ‹Ÿé…ç½®ç±»ï¼Œç”¨äºæµ‹è¯•"""
    def __init__(self):
        self.GRAPH_N_PHI = 8
        self.MAP_SIZE = 1000.0
        self.USE_PHRRT_DURING_TRAINING = False
        self.ENABLE_REWARD_LOGGING = True
        self.UAV_COMM_FAILURE_RATE = 0.0
        self.UAV_SENSING_FAILURE_RATE = 0.0

def create_test_scenario(n_uavs: int, n_targets: int = 3):
    """
    åˆ›å»ºæµ‹è¯•åœºæ™¯ï¼Œæ”¯æŒå¯å˜æ— äººæœºæ•°é‡
    
    Args:
        n_uavs: æ— äººæœºæ•°é‡
        n_targets: ç›®æ ‡æ•°é‡
        
    Returns:
        tuple: (uavs, targets, graph, obstacles, config)
    """
    config = MockConfig()
    
    # åˆ›å»ºUAVåˆ—è¡¨
    uavs = []
    for i in range(n_uavs):
        uav = UAV(
            id=i,
            position=(100 + i * 50, 100 + i * 30),
            heading=0.0,  # æ·»åŠ æœå‘å‚æ•°
            resources=np.array([50.0, 40.0]),
            max_distance=1000,
            velocity_range=(30, 100),
            economic_speed=60.0  # æ·»åŠ ç»æµé€Ÿåº¦å‚æ•°
        )
        uavs.append(uav)
    
    # åˆ›å»ºç›®æ ‡åˆ—è¡¨
    targets = []
    target_positions = [(800, 200), (600, 600), (200, 800)]
    for i in range(n_targets):
        target = Target(
            id=i,
            position=target_positions[i % len(target_positions)],
            resources=np.array([80.0, 60.0]),
            value=100.0
        )
        targets.append(target)
    
    # åˆ›å»ºå›¾
    obstacles = []  # ç®€åŒ–æµ‹è¯•ï¼Œä¸ä½¿ç”¨éšœç¢ç‰©
    graph = DirectedGraph(uavs, targets, config.GRAPH_N_PHI, obstacles, config)
    
    return uavs, targets, graph, obstacles, config

def test_active_uav_count_calculation():
    """æµ‹è¯•æœ‰æ•ˆæ— äººæœºæ•°é‡è®¡ç®—"""
    print("=== æµ‹è¯•1: æœ‰æ•ˆæ— äººæœºæ•°é‡è®¡ç®— ===")
    
    # æµ‹è¯•ä¸åŒæ•°é‡çš„UAV
    for n_uavs in [2, 5, 10, 15]:
        uavs, targets, graph, obstacles, config = create_test_scenario(n_uavs)
        env = UAVTaskEnv(uavs, targets, graph, obstacles, config)
        
        active_count = env._calculate_active_uav_count()
        print(f"æ€»UAVæ•°é‡: {n_uavs}, æœ‰æ•ˆUAVæ•°é‡: {active_count}")
        
        # éªŒè¯æœ‰æ•ˆæ•°é‡ä¸è¶…è¿‡æ€»æ•°é‡
        assert active_count <= n_uavs, f"æœ‰æ•ˆUAVæ•°é‡({active_count})ä¸åº”è¶…è¿‡æ€»æ•°é‡({n_uavs})"
        assert active_count >= 1, f"æœ‰æ•ˆUAVæ•°é‡({active_count})åº”è‡³å°‘ä¸º1"
    
    print("âœ“ æœ‰æ•ˆæ— äººæœºæ•°é‡è®¡ç®—æµ‹è¯•é€šè¿‡\n")

def test_congestion_penalty_normalization():
    """æµ‹è¯•æ‹¥å µæƒ©ç½šçš„å½’ä¸€åŒ–"""
    print("=== æµ‹è¯•2: æ‹¥å µæƒ©ç½šå½’ä¸€åŒ– ===")
    
    # åˆ›å»ºæ‹¥å µåœºæ™¯ï¼šå¤šä¸ªUAVåˆ†é…åˆ°åŒä¸€ç›®æ ‡
    uavs, targets, graph, obstacles, config = create_test_scenario(6, 2)
    env = UAVTaskEnv(uavs, targets, graph, obstacles, config)
    
    # æ¨¡æ‹Ÿå¤šä¸ªUAVåˆ†é…åˆ°ç¬¬ä¸€ä¸ªç›®æ ‡
    target = targets[0]
    target.allocated_uavs = [(0, 0), (1, 1), (2, 2), (3, 3)]  # 4ä¸ªUAVåˆ†é…åˆ°åŒä¸€ç›®æ ‡
    
    # æµ‹è¯•ä¸åŒæœ‰æ•ˆUAVæ•°é‡ä¸‹çš„æ‹¥å µæƒ©ç½š
    test_cases = [
        {"n_active": 4, "expected_normalization": True},
        {"n_active": 6, "expected_normalization": True},
        {"n_active": 2, "expected_normalization": True}
    ]
    
    for case in test_cases:
        # æ¨¡æ‹Ÿä¸åŒçš„æœ‰æ•ˆUAVæ•°é‡
        original_method = env._calculate_active_uav_count
        env._calculate_active_uav_count = lambda: case["n_active"]
        
        # è®¡ç®—æ‹¥å µæƒ©ç½š
        congestion_raw = env._calculate_congestion_penalty(target, uavs[0], case["n_active"])
        congestion_normalized = congestion_raw / case["n_active"]
        
        print(f"æœ‰æ•ˆUAVæ•°é‡: {case['n_active']}")
        print(f"  åŸå§‹æ‹¥å µæƒ©ç½š: {congestion_raw:.4f}")
        print(f"  å½’ä¸€åŒ–æ‹¥å µæƒ©ç½š: {congestion_normalized:.4f}")
        print(f"  å½’ä¸€åŒ–å› å­: {1.0/case['n_active']:.4f}")
        
        # éªŒè¯å½’ä¸€åŒ–æ•ˆæœ
        if congestion_raw > 0:
            assert congestion_normalized < congestion_raw, "å½’ä¸€åŒ–åçš„æƒ©ç½šåº”å°äºåŸå§‹æƒ©ç½š"
            assert abs(congestion_normalized - congestion_raw / case["n_active"]) < 1e-6, "å½’ä¸€åŒ–è®¡ç®—é”™è¯¯"
        
        # æ¢å¤åŸå§‹æ–¹æ³•
        env._calculate_active_uav_count = original_method
    
    print("âœ“ æ‹¥å µæƒ©ç½šå½’ä¸€åŒ–æµ‹è¯•é€šè¿‡\n")

def test_reward_consistency_across_uav_counts():
    """æµ‹è¯•ä¸åŒæ— äººæœºæ•°é‡ä¸‹çš„å¥–åŠ±ä¸€è‡´æ€§"""
    print("=== æµ‹è¯•3: ä¸åŒæ— äººæœºæ•°é‡ä¸‹çš„å¥–åŠ±ä¸€è‡´æ€§ ===")
    
    # æµ‹è¯•åœºæ™¯ï¼šç›¸åŒçš„åŠ¨ä½œåœ¨ä¸åŒUAVæ•°é‡ä¸‹åº”äº§ç”Ÿç›¸ä¼¼çš„å½’ä¸€åŒ–å¥–åŠ±
    uav_counts = [3, 6, 9, 12]
    rewards_by_count = {}
    
    for n_uavs in uav_counts:
        uavs, targets, graph, obstacles, config = create_test_scenario(n_uavs, 3)
        env = UAVTaskEnv(uavs, targets, graph, obstacles, config)
        
        # æ‰§è¡Œç›¸åŒçš„åŠ¨ä½œï¼šç¬¬ä¸€ä¸ªUAVåˆ†é…åˆ°ç¬¬ä¸€ä¸ªç›®æ ‡
        target = targets[0]
        uav = uavs[0]
        actual_contribution = np.array([20.0, 15.0])  # å›ºå®šè´¡çŒ®
        path_len = 500.0  # å›ºå®šè·¯å¾„é•¿åº¦
        travel_time = 10.0  # å›ºå®šæ—…è¡Œæ—¶é—´
        was_satisfied = False
        done = False
        
        # è®¡ç®—å¥–åŠ±
        reward = env._calculate_reward(target, uav, actual_contribution, path_len, 
                                     was_satisfied, travel_time, done)
        
        rewards_by_count[n_uavs] = {
            'reward': reward,
            'components': env.last_reward_components.copy()
        }
        
        print(f"UAVæ•°é‡: {n_uavs}, å¥–åŠ±: {reward:.4f}")
        
        # æ‰“å°å½’ä¸€åŒ–ä¿¡æ¯
        norm_info = env.last_reward_components['per_agent_normalization']
        print(f"  æœ‰æ•ˆUAVæ•°é‡: {norm_info['n_active_uavs']}")
        print(f"  å½’ä¸€åŒ–ç»„ä»¶: {norm_info['components_normalized']}")
        print(f"  å½’ä¸€åŒ–èŠ‚çœ: {norm_info['normalization_impact']['normalization_savings']:.4f}")
    
    # éªŒè¯å¥–åŠ±ä¸€è‡´æ€§ï¼šå½’ä¸€åŒ–åçš„å¥–åŠ±åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
    reward_values = [data['reward'] for data in rewards_by_count.values()]
    reward_std = np.std(reward_values)
    reward_mean = np.mean(reward_values)
    
    print(f"\nå¥–åŠ±ç»Ÿè®¡:")
    print(f"  å¹³å‡å¥–åŠ±: {reward_mean:.4f}")
    print(f"  æ ‡å‡†å·®: {reward_std:.4f}")
    print(f"  å˜å¼‚ç³»æ•°: {reward_std/abs(reward_mean):.4f}")
    
    # éªŒè¯å˜å¼‚ç³»æ•°åœ¨åˆç†èŒƒå›´å†…ï¼ˆå½’ä¸€åŒ–åº”è¯¥å‡å°‘å˜å¼‚æ€§ï¼‰
    cv_threshold = 0.3  # å˜å¼‚ç³»æ•°é˜ˆå€¼
    assert reward_std / abs(reward_mean) < cv_threshold, \
        f"å¥–åŠ±å˜å¼‚ç³»æ•°({reward_std/abs(reward_mean):.4f})è¶…è¿‡é˜ˆå€¼({cv_threshold})"
    
    print("âœ“ å¥–åŠ±ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡\n")

def test_reward_component_tracking():
    """æµ‹è¯•å¥–åŠ±ç»„ä»¶è·Ÿè¸ªåŠŸèƒ½"""
    print("=== æµ‹è¯•4: å¥–åŠ±ç»„ä»¶è·Ÿè¸ªåŠŸèƒ½ ===")
    
    uavs, targets, graph, obstacles, config = create_test_scenario(5, 2)
    env = UAVTaskEnv(uavs, targets, graph, obstacles, config)
    
    # æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œ
    target = targets[0]
    uav = uavs[0]
    actual_contribution = np.array([30.0, 25.0])
    path_len = 300.0
    travel_time = 8.0
    was_satisfied = False
    done = False
    
    reward = env._calculate_reward(target, uav, actual_contribution, path_len, 
                                 was_satisfied, travel_time, done)
    
    components = env.last_reward_components
    
    # éªŒè¯å¿…è¦çš„ç»„ä»¶å­˜åœ¨
    required_components = [
        'n_active_uavs', 'normalization_applied', 'total_positive', 'total_costs',
        'final_reward', 'per_agent_normalization', 'debug_info'
    ]
    
    for component in required_components:
        assert component in components, f"ç¼ºå°‘å¿…è¦çš„å¥–åŠ±ç»„ä»¶: {component}"
    
    # éªŒè¯å½’ä¸€åŒ–ä¿¡æ¯
    norm_info = components['per_agent_normalization']
    assert 'n_active_uavs' in norm_info
    assert 'normalization_factor' in norm_info
    assert 'components_normalized' in norm_info
    assert 'normalization_impact' in norm_info
    
    # éªŒè¯å½’ä¸€åŒ–å½±å“åˆ†æ
    impact = norm_info['normalization_impact']
    assert 'total_raw_normalized_rewards' in impact
    assert 'total_normalized_rewards' in impact
    assert 'normalization_savings' in impact
    assert 'components_impact' in impact
    
    print("å¥–åŠ±ç»„ä»¶è·Ÿè¸ªä¿¡æ¯:")
    print(f"  æœ‰æ•ˆUAVæ•°é‡: {norm_info['n_active_uavs']}")
    print(f"  å½’ä¸€åŒ–å› å­: {norm_info['normalization_factor']:.4f}")
    print(f"  åº”ç”¨å½’ä¸€åŒ–çš„ç»„ä»¶: {norm_info['components_normalized']}")
    print(f"  å½’ä¸€åŒ–èŠ‚çœ: {impact['normalization_savings']:.4f}")
    print(f"  æœ€ç»ˆå¥–åŠ±: {components['final_reward']:.4f}")
    
    print("âœ“ å¥–åŠ±ç»„ä»¶è·Ÿè¸ªæµ‹è¯•é€šè¿‡\n")

def test_normalization_impact_analysis():
    """æµ‹è¯•å½’ä¸€åŒ–å½±å“åˆ†æ"""
    print("=== æµ‹è¯•5: å½’ä¸€åŒ–å½±å“åˆ†æ ===")
    
    # åˆ›å»ºé«˜æ‹¥å µåœºæ™¯
    uavs, targets, graph, obstacles, config = create_test_scenario(8, 2)
    env = UAVTaskEnv(uavs, targets, graph, obstacles, config)
    
    # æ¨¡æ‹Ÿé«˜æ‹¥å µï¼šå¤šä¸ªUAVåˆ†é…åˆ°åŒä¸€ç›®æ ‡
    target = targets[0]
    target.allocated_uavs = [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]  # 5ä¸ªUAVåˆ†é…åˆ°åŒä¸€ç›®æ ‡
    
    # æ‰§è¡ŒåŠ¨ä½œ
    uav = uavs[0]
    actual_contribution = np.array([25.0, 20.0])
    path_len = 400.0
    travel_time = 12.0
    was_satisfied = False
    done = False
    
    reward = env._calculate_reward(target, uav, actual_contribution, path_len, 
                                 was_satisfied, travel_time, done)
    
    components = env.last_reward_components
    impact = components['per_agent_normalization']['normalization_impact']
    
    print("å½’ä¸€åŒ–å½±å“åˆ†æ:")
    print(f"  åŸå§‹å½’ä¸€åŒ–å¥–åŠ±æ€»å’Œ: {impact['total_raw_normalized_rewards']:.4f}")
    print(f"  å½’ä¸€åŒ–åå¥–åŠ±æ€»å’Œ: {impact['total_normalized_rewards']:.4f}")
    print(f"  å½’ä¸€åŒ–èŠ‚çœ: {impact['normalization_savings']:.4f}")
    
    # éªŒè¯å½’ä¸€åŒ–ç¡®å®äº§ç”Ÿäº†å½±å“
    if impact['total_raw_normalized_rewards'] > 0:
        assert impact['normalization_savings'] > 0, "åœ¨é«˜æ‹¥å µåœºæ™¯ä¸‹ï¼Œå½’ä¸€åŒ–åº”è¯¥äº§ç”ŸèŠ‚çœæ•ˆæœ"
        assert impact['total_normalized_rewards'] < impact['total_raw_normalized_rewards'], \
            "å½’ä¸€åŒ–åçš„å¥–åŠ±åº”å°äºåŸå§‹å¥–åŠ±"
    
    # éªŒè¯ç»„ä»¶çº§åˆ«çš„å½±å“åˆ†æ
    for component, details in impact['components_impact'].items():
        print(f"  {component}ç»„ä»¶:")
        print(f"    åŸå§‹å€¼: {details['raw']:.4f}")
        print(f"    å½’ä¸€åŒ–å€¼: {details['normalized']:.4f}")
        print(f"    å‡å°‘é‡: {details['reduction']:.4f}")
        
        if details['raw'] > 0:
            assert details['reduction'] >= 0, f"{component}ç»„ä»¶çš„å½’ä¸€åŒ–å‡å°‘é‡åº”ä¸ºéè´Ÿ"
    
    print("âœ“ å½’ä¸€åŒ–å½±å“åˆ†ææµ‹è¯•é€šè¿‡\n")

def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("å¼€å§‹Per-Agentå¥–åŠ±å½’ä¸€åŒ–ç»¼åˆæµ‹è¯•...\n")
    
    try:
        test_active_uav_count_calculation()
        test_congestion_penalty_normalization()
        test_reward_consistency_across_uav_counts()
        test_reward_component_tracking()
        test_normalization_impact_analysis()
        
        print("ğŸ‰ æ‰€æœ‰Per-Agentå¥–åŠ±å½’ä¸€åŒ–æµ‹è¯•é€šè¿‡ï¼")
        print("\næµ‹è¯•æ€»ç»“:")
        print("âœ“ æœ‰æ•ˆæ— äººæœºæ•°é‡è®¡ç®—æ­£ç¡®")
        print("âœ“ æ‹¥å µæƒ©ç½šå½’ä¸€åŒ–åŠŸèƒ½æ­£å¸¸")
        print("âœ“ ä¸åŒUAVæ•°é‡ä¸‹å¥–åŠ±ä¿æŒä¸€è‡´æ€§")
        print("âœ“ å¥–åŠ±ç»„ä»¶è·Ÿè¸ªåŠŸèƒ½å®Œæ•´")
        print("âœ“ å½’ä¸€åŒ–å½±å“åˆ†æå‡†ç¡®")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = run_comprehensive_test()
    sys.exit(0 if success else 1)