# -*- coding: utf-8 -*-
# æ–‡ä»¶å: temp_tests/test_transformer_gnn_basic.py
# æè¿°: TransformerGNNåŸºç¡€åŠŸèƒ½æµ‹è¯•

import torch
import numpy as np
from gymnasium import spaces
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from transformer_gnn import TransformerGNN, create_transformer_gnn_model

def test_transformer_gnn_dict_obs():
    """æµ‹è¯•TransformerGNNåœ¨å›¾æ¨¡å¼è§‚æµ‹ä¸‹çš„åŸºæœ¬åŠŸèƒ½"""
    print("=== æµ‹è¯•TransformerGNNå›¾æ¨¡å¼è§‚æµ‹ ===")
    
    # å®šä¹‰å›¾æ¨¡å¼è§‚æµ‹ç©ºé—´
    obs_space = spaces.Dict({
        'uav_features': spaces.Box(low=0.0, high=1.0, shape=(3, 9), dtype=np.float32),  # 3ä¸ªUAVï¼Œæ¯ä¸ª9ä¸ªç‰¹å¾
        'target_features': spaces.Box(low=0.0, high=1.0, shape=(2, 8), dtype=np.float32),  # 2ä¸ªç›®æ ‡ï¼Œæ¯ä¸ª8ä¸ªç‰¹å¾
        'relative_positions': spaces.Box(low=-1.0, high=1.0, shape=(3, 2, 2), dtype=np.float32),  # 3x2çš„ç›¸å¯¹ä½ç½®
        'distances': spaces.Box(low=0.0, high=1.0, shape=(3, 2), dtype=np.float32),  # 3x2çš„è·ç¦»çŸ©é˜µ
        'masks': spaces.Dict({
            'uav_mask': spaces.Box(low=0, high=1, shape=(3,), dtype=np.int32),
            'target_mask': spaces.Box(low=0, high=1, shape=(2,), dtype=np.int32)
        })
    })
    
    action_space = spaces.Discrete(36)  # 3 UAV * 2 ç›®æ ‡ * 6 æ–¹å‘
    num_outputs = 36
    
    # æ¨¡å‹é…ç½®
    model_config = {
        "embed_dim": 64,
        "num_heads": 4,
        "num_layers": 2,
        "dropout": 0.1,
        "use_position_encoding": True,
        "use_noisy_linear": False  # æš‚æ—¶å…³é—­å™ªå£°çº¿æ€§å±‚ä»¥ç®€åŒ–æµ‹è¯•
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = TransformerGNN(obs_space, action_space, num_outputs, model_config, "test_model")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    test_obs = {
        'uav_features': torch.randn(batch_size, 3, 9),
        'target_features': torch.randn(batch_size, 2, 8),
        'relative_positions': torch.randn(batch_size, 3, 2, 2),
        'distances': torch.rand(batch_size, 3, 2),
        'masks': {
            'uav_mask': torch.ones(batch_size, 3, dtype=torch.int32),
            'target_mask': torch.ones(batch_size, 2, dtype=torch.int32)
        }
    }
    
    # å‰å‘ä¼ æ’­æµ‹è¯•
    input_dict = {"obs": test_obs}
    state = []
    seq_lens = torch.tensor([1] * batch_size)
    
    print(f"è¾“å…¥è§‚æµ‹å½¢çŠ¶:")
    for key, value in test_obs.items():
        if isinstance(value, dict):
            print(f"  {key}:")
            for sub_key, sub_value in value.items():
                print(f"    {sub_key}: {sub_value.shape}")
        else:
            print(f"  {key}: {value.shape}")
    
    # æ‰§è¡Œå‰å‘ä¼ æ’­
    logits, new_state = model.forward(input_dict, state, seq_lens)
    value = model.value_function()
    
    print(f"è¾“å‡ºlogitså½¢çŠ¶: {logits.shape}")
    print(f"è¾“å‡ºå€¼å‡½æ•°å½¢çŠ¶: {value.shape}")
    print(f"logitsèŒƒå›´: [{logits.min().item():.3f}, {logits.max().item():.3f}]")
    print(f"å€¼å‡½æ•°èŒƒå›´: [{value.min().item():.3f}, {value.max().item():.3f}]")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert logits.shape == (batch_size, num_outputs), f"æœŸæœ›logitså½¢çŠ¶: ({batch_size}, {num_outputs}), å®é™…: {logits.shape}"
    assert value.shape == (batch_size,), f"æœŸæœ›å€¼å‡½æ•°å½¢çŠ¶: ({batch_size},), å®é™…: {value.shape}"
    
    print("âœ“ å›¾æ¨¡å¼è§‚æµ‹æµ‹è¯•é€šè¿‡")
    return True

def test_transformer_gnn_flat_obs():
    """æµ‹è¯•TransformerGNNåœ¨æ‰å¹³æ¨¡å¼è§‚æµ‹ä¸‹çš„åŸºæœ¬åŠŸèƒ½"""
    print("\n=== æµ‹è¯•TransformerGNNæ‰å¹³æ¨¡å¼è§‚æµ‹ ===")
    
    # å®šä¹‰æ‰å¹³æ¨¡å¼è§‚æµ‹ç©ºé—´
    input_dim = 100
    obs_space = spaces.Box(low=-np.inf, high=np.inf, shape=(input_dim,), dtype=np.float32)
    action_space = spaces.Discrete(36)
    num_outputs = 36
    
    # æ¨¡å‹é…ç½®
    model_config = {
        "embed_dim": 64,
        "num_heads": 4,
        "num_layers": 2,
        "dropout": 0.1,
        "use_position_encoding": True,
        "use_noisy_linear": False
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = TransformerGNN(obs_space, action_space, num_outputs, model_config, "test_model_flat")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    test_obs = torch.randn(batch_size, input_dim)
    
    # å‰å‘ä¼ æ’­æµ‹è¯•
    input_dict = {"obs": test_obs}
    state = []
    seq_lens = torch.tensor([1] * batch_size)
    
    print(f"è¾“å…¥è§‚æµ‹å½¢çŠ¶: {test_obs.shape}")
    
    # æ‰§è¡Œå‰å‘ä¼ æ’­
    logits, new_state = model.forward(input_dict, state, seq_lens)
    value = model.value_function()
    
    print(f"è¾“å‡ºlogitså½¢çŠ¶: {logits.shape}")
    print(f"è¾“å‡ºå€¼å‡½æ•°å½¢çŠ¶: {value.shape}")
    print(f"logitsèŒƒå›´: [{logits.min().item():.3f}, {logits.max().item():.3f}]")
    print(f"å€¼å‡½æ•°èŒƒå›´: [{value.min().item():.3f}, {value.max().item():.3f}]")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert logits.shape == (batch_size, num_outputs), f"æœŸæœ›logitså½¢çŠ¶: ({batch_size}, {num_outputs}), å®é™…: {logits.shape}"
    assert value.shape == (batch_size,), f"æœŸæœ›å€¼å‡½æ•°å½¢çŠ¶: ({batch_size},), å®é™…: {value.shape}"
    
    print("âœ“ æ‰å¹³æ¨¡å¼è§‚æµ‹æµ‹è¯•é€šè¿‡")
    return True

def test_model_parameters():
    """æµ‹è¯•æ¨¡å‹å‚æ•°æ•°é‡å’Œç»“æ„"""
    print("\n=== æµ‹è¯•æ¨¡å‹å‚æ•° ===")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹
    obs_space = spaces.Box(low=-np.inf, high=np.inf, shape=(50,), dtype=np.float32)
    action_space = spaces.Discrete(10)
    num_outputs = 10
    
    model_config = {
        "embed_dim": 32,
        "num_heads": 2,
        "num_layers": 1,
        "dropout": 0.1,
        "use_position_encoding": True,
        "use_noisy_linear": False
    }
    
    model = TransformerGNN(obs_space, action_space, num_outputs, model_config, "test_params")
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
    
    # æ£€æŸ¥ä¸»è¦ç»„ä»¶
    print(f"UAVç¼–ç å™¨å‚æ•°: {sum(p.numel() for p in model.uav_encoder.parameters()):,}")
    print(f"ç›®æ ‡ç¼–ç å™¨å‚æ•°: {sum(p.numel() for p in model.target_encoder.parameters()):,}")
    print(f"Transformerç¼–ç å™¨å‚æ•°: {sum(p.numel() for p in model.transformer_encoder.parameters()):,}")
    print(f"è¾“å‡ºå±‚å‚æ•°: {sum(p.numel() for p in model.output_layer.parameters()):,}")
    print(f"å€¼å‡½æ•°å¤´å‚æ•°: {sum(p.numel() for p in model.value_head.parameters()):,}")
    
    if model.use_position_encoding:
        print(f"ä½ç½®ç¼–ç å™¨å‚æ•°: {sum(p.numel() for p in model.position_encoder.parameters()):,}")
    
    print("âœ“ å‚æ•°ç»Ÿè®¡å®Œæˆ")
    return True

def test_factory_function():
    """æµ‹è¯•å·¥å‚å‡½æ•°"""
    print("\n=== æµ‹è¯•å·¥å‚å‡½æ•° ===")
    
    obs_space = spaces.Box(low=-np.inf, high=np.inf, shape=(20,), dtype=np.float32)
    action_space = spaces.Discrete(5)
    num_outputs = 5
    model_config = {"embed_dim": 16}
    
    model = create_transformer_gnn_model(obs_space, action_space, num_outputs, model_config)
    
    assert isinstance(model, TransformerGNN), "å·¥å‚å‡½æ•°åº”è¯¥è¿”å›TransformerGNNå®ä¾‹"
    print("âœ“ å·¥å‚å‡½æ•°æµ‹è¯•é€šè¿‡")
    return True

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹TransformerGNNåŸºç¡€åŠŸèƒ½æµ‹è¯•...")
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_transformer_gnn_dict_obs()
        test_transformer_gnn_flat_obs()
        test_model_parameters()
        test_factory_function()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼TransformerGNNåŸºç¡€æ¶æ„å®ç°æ­£ç¡®ã€‚")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    main()