# -*- coding: utf-8 -*-
# æ–‡ä»¶å: test_transformer_gnn_integration.py
# æè¿°: TransformerGNNä¸å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶é›†æˆæµ‹è¯•

import torch
import torch.nn as nn
import numpy as np
import sys
import os
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from transformer_gnn import TransformerGNN, create_transformer_gnn_model


class MockSpace:
    """æ¨¡æ‹Ÿè§‚æµ‹ç©ºé—´"""
    def __init__(self, shape):
        self.shape = shape


def create_test_obs_space():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„è§‚æµ‹ç©ºé—´"""
    return {
        'uav_features': MockSpace((5, 8)),
        'target_features': MockSpace((8, 6)),
        'relative_positions': MockSpace((5, 8, 2)),
        'distances': MockSpace((5, 8)),
        'masks': {
            'uav_mask': MockSpace((5,)),
            'target_mask': MockSpace((8,)),
            'interaction_mask': MockSpace((5, 8))
        }
    }


def create_test_observation(batch_size=2, num_uavs=5, num_targets=8):
    """åˆ›å»ºæµ‹è¯•ç”¨çš„è§‚æµ‹æ•°æ®"""
    return {
        'uav_features': torch.randn(batch_size, num_uavs, 8),
        'target_features': torch.randn(batch_size, num_targets, 6),
        'relative_positions': torch.randn(batch_size, num_uavs, num_targets, 2),
        'distances': torch.rand(batch_size, num_uavs, num_targets) * 10 + 0.1,
        'masks': {
            'uav_mask': torch.ones(batch_size, num_uavs, dtype=torch.bool),
            'target_mask': torch.ones(batch_size, num_targets, dtype=torch.bool),
            'interaction_mask': torch.ones(batch_size, num_uavs, num_targets, dtype=torch.bool)
        }
    }


def test_transformer_gnn_with_local_attention():
    """æµ‹è¯•TransformerGNNä¸å±€éƒ¨æ³¨æ„åŠ›çš„é›†æˆ"""
    print("\n=== æµ‹è¯•TransformerGNNä¸å±€éƒ¨æ³¨æ„åŠ›é›†æˆ ===")
    
    # åˆ›å»ºè§‚æµ‹ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´
    obs_space = create_test_obs_space()
    action_space = MockSpace((10,))
    
    # æ¨¡å‹é…ç½®
    model_config = {
        "embed_dim": 64,
        "num_heads": 4,
        "num_layers": 2,
        "dropout": 0.1,
        "use_position_encoding": True,
        "use_local_attention": True,
        "k_adaptive": True,
        "k_min": 2,
        "k_max": 8,
        "use_flash_attention": False,  # ä½¿ç”¨æ ‡å‡†å®ç°ç¡®ä¿ç¨³å®šæ€§
        "use_noisy_linear": False  # æš‚æ—¶ç¦ç”¨ä»¥ç®€åŒ–æµ‹è¯•
    }
    
    # åˆ›å»ºæ¨¡å‹
    model = TransformerGNN(
        obs_space=obs_space,
        action_space=action_space,
        num_outputs=10,
        model_config=model_config,
        name="test_transformer_gnn"
    )
    
    print(f"âœ“ TransformerGNNæ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"  - åµŒå…¥ç»´åº¦: {model.embed_dim}")
    print(f"  - æ³¨æ„åŠ›å¤´æ•°: {model.num_heads}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å±€éƒ¨æ³¨æ„åŠ›å±æ€§
    if hasattr(model, 'use_local_attention'):
        print(f"  - å±€éƒ¨æ³¨æ„åŠ›: {'å¯ç”¨' if model.use_local_attention else 'ç¦ç”¨'}")
    else:
        print(f"  - å±€éƒ¨æ³¨æ„åŠ›: å±æ€§ä¸å­˜åœ¨")
        
    # æ£€æŸ¥æ˜¯å¦æœ‰å±€éƒ¨æ³¨æ„åŠ›æ¨¡å—
    if hasattr(model, 'local_attention'):
        print(f"  - å±€éƒ¨æ³¨æ„åŠ›æ¨¡å—: {'å­˜åœ¨' if model.local_attention is not None else 'ä¸å­˜åœ¨'}")
    else:
        print(f"  - å±€éƒ¨æ³¨æ„åŠ›æ¨¡å—: å±æ€§ä¸å­˜åœ¨")
    
    return model


def test_forward_pass():
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\n=== æµ‹è¯•å‰å‘ä¼ æ’­ ===")
    
    model = test_transformer_gnn_with_local_attention()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    obs = create_test_observation(batch_size)
    
    # è®¾ç½®éƒ¨åˆ†æ©ç ä»¥æµ‹è¯•é²æ£’æ€§
    obs['masks']['uav_mask'][0, -1] = False  # ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„æœ€åä¸€ä¸ªUAVæ— æ•ˆ
    obs['masks']['target_mask'][1, -2:] = False  # ç¬¬äºŒä¸ªæ‰¹æ¬¡çš„æœ€åä¸¤ä¸ªç›®æ ‡æ— æ•ˆ
    
    # å‰å‘ä¼ æ’­
    input_dict = {"obs": obs}
    state = []
    seq_lens = None
    
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    with torch.no_grad():
        logits, new_state = model.forward(input_dict, state, seq_lens)
        value = model.value_function()
    
    # éªŒè¯è¾“å‡º
    assert logits.shape == (batch_size, 10), f"æœŸæœ›logitså½¢çŠ¶ä¸º({batch_size}, 10)ï¼Œå®é™…ä¸º{logits.shape}"
    assert value.shape == (batch_size,), f"æœŸæœ›valueå½¢çŠ¶ä¸º({batch_size},)ï¼Œå®é™…ä¸º{value.shape}"
    
    # éªŒè¯æ•°å€¼ç¨³å®šæ€§
    assert not torch.isnan(logits).any(), "logitsåŒ…å«NaNå€¼"
    assert not torch.isinf(logits).any(), "logitsåŒ…å«Infå€¼"
    assert not torch.isnan(value).any(), "valueåŒ…å«NaNå€¼"
    assert not torch.isinf(value).any(), "valueåŒ…å«Infå€¼"
    
    print(f"âœ“ å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")
    print(f"  - Logitså½¢çŠ¶: {logits.shape}")
    print(f"  - Valueå½¢çŠ¶: {value.shape}")
    print(f"  - LogitsèŒƒå›´: [{logits.min().item():.3f}, {logits.max().item():.3f}]")
    print(f"  - ValueèŒƒå›´: [{value.min().item():.3f}, {value.max().item():.3f}]")
    
    return model, logits, value


def test_gradient_computation():
    """æµ‹è¯•æ¢¯åº¦è®¡ç®—"""
    print("\n=== æµ‹è¯•æ¢¯åº¦è®¡ç®— ===")
    
    model = test_transformer_gnn_with_local_attention()
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    obs = create_test_observation(batch_size=1)
    input_dict = {"obs": obs}
    
    # å‰å‘ä¼ æ’­
    logits, _ = model.forward(input_dict, [], None)
    value = model.value_function()
    
    # è®¡ç®—æŸå¤±
    target_logits = torch.randn_like(logits)
    target_value = torch.randn_like(value)
    
    loss = nn.MSELoss()(logits, target_logits) + nn.MSELoss()(value, target_value)
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # éªŒè¯æ¢¯åº¦
    grad_count = 0
    zero_grad_count = 0
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            assert param.grad is not None, f"å‚æ•° {name} æ²¡æœ‰æ¢¯åº¦"
            
            if torch.allclose(param.grad, torch.zeros_like(param.grad)):
                zero_grad_count += 1
            else:
                grad_count += 1
    
    print(f"âœ“ æ¢¯åº¦è®¡ç®—æµ‹è¯•é€šè¿‡")
    print(f"  - æœ‰éé›¶æ¢¯åº¦çš„å‚æ•°: {grad_count}")
    print(f"  - é›¶æ¢¯åº¦å‚æ•°: {zero_grad_count}")
    print(f"  - æŸå¤±å€¼: {loss.item():.6f}")
    
    return model


def test_different_scales():
    """æµ‹è¯•ä¸åŒè§„æ¨¡çš„è¾“å…¥"""
    print("\n=== æµ‹è¯•ä¸åŒè§„æ¨¡è¾“å…¥ ===")
    
    # æµ‹è¯•ä¸åŒçš„è§„æ¨¡
    test_scales = [
        (1, 2, 3),    # å°è§„æ¨¡
        (2, 5, 8),    # ä¸­ç­‰è§„æ¨¡
        (1, 10, 15),  # å¤§è§„æ¨¡
    ]
    
    for batch_size, num_uavs, num_targets in test_scales:
        print(f"æµ‹è¯•è§„æ¨¡: æ‰¹æ¬¡={batch_size}, UAV={num_uavs}, ç›®æ ‡={num_targets}")
        
        # åˆ›å»ºå¯¹åº”è§„æ¨¡çš„è§‚æµ‹ç©ºé—´
        obs_space = {
            'uav_features': MockSpace((num_uavs, 8)),
            'target_features': MockSpace((num_targets, 6)),
            'relative_positions': MockSpace((num_uavs, num_targets, 2)),
            'distances': MockSpace((num_uavs, num_targets)),
            'masks': {
                'uav_mask': MockSpace((num_uavs,)),
                'target_mask': MockSpace((num_targets,)),
                'interaction_mask': MockSpace((num_uavs, num_targets))
            }
        }
        
        action_space = MockSpace((10,))
        
        model_config = {
            "embed_dim": 64,
            "num_heads": 4,
            "use_local_attention": True,
            "k_adaptive": True,
            "use_noisy_linear": False
        }
        
        # åˆ›å»ºæ¨¡å‹
        model = TransformerGNN(
            obs_space=obs_space,
            action_space=action_space,
            num_outputs=10,
            model_config=model_config,
            name=f"test_model_{num_uavs}_{num_targets}"
        )
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        obs = create_test_observation(batch_size, num_uavs, num_targets)
        
        # å‰å‘ä¼ æ’­
        model.eval()
        with torch.no_grad():
            logits, _ = model.forward({"obs": obs}, [], None)
            value = model.value_function()
        
        # éªŒè¯è¾“å‡º
        assert logits.shape == (batch_size, 10)
        assert value.shape == (batch_size,)
        assert not torch.isnan(logits).any()
        assert not torch.isnan(value).any()
        
        print(f"  âœ“ è§„æ¨¡ ({batch_size}, {num_uavs}, {num_targets}) æµ‹è¯•é€šè¿‡")


def test_local_attention_effectiveness():
    """æµ‹è¯•å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶çš„æœ‰æ•ˆæ€§"""
    print("\n=== æµ‹è¯•å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆæ€§ ===")
    
    obs_space = create_test_obs_space()
    action_space = MockSpace((10,))
    
    # åˆ›å»ºä¸¤ä¸ªæ¨¡å‹ï¼šä¸€ä¸ªå¯ç”¨å±€éƒ¨æ³¨æ„åŠ›ï¼Œä¸€ä¸ªç¦ç”¨
    config_with_local = {
        "embed_dim": 64,
        "num_heads": 4,
        "use_local_attention": True,
        "k_adaptive": True,
        "k_max": 4,
        "use_noisy_linear": False
    }
    
    config_without_local = {
        "embed_dim": 64,
        "num_heads": 4,
        "use_local_attention": False,
        "use_noisy_linear": False
    }
    
    model_with_local = TransformerGNN(obs_space, action_space, 10, config_with_local, "with_local")
    model_without_local = TransformerGNN(obs_space, action_space, 10, config_without_local, "without_local")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    obs = create_test_observation(batch_size=1)
    
    # å‰å‘ä¼ æ’­
    model_with_local.eval()
    model_without_local.eval()
    
    with torch.no_grad():
        logits_with, _ = model_with_local.forward({"obs": obs}, [], None)
        logits_without, _ = model_without_local.forward({"obs": obs}, [], None)
    
    # éªŒè¯ä¸¤ä¸ªæ¨¡å‹éƒ½èƒ½æ­£å¸¸å·¥ä½œ
    assert logits_with.shape == logits_without.shape
    assert not torch.isnan(logits_with).any()
    assert not torch.isnan(logits_without).any()
    
    # éªŒè¯è¾“å‡ºæœ‰å·®å¼‚ï¼ˆè¯´æ˜å±€éƒ¨æ³¨æ„åŠ›ç¡®å®åœ¨èµ·ä½œç”¨ï¼‰
    diff = torch.abs(logits_with - logits_without).mean()
    print(f"  - å¯ç”¨/ç¦ç”¨å±€éƒ¨æ³¨æ„åŠ›çš„è¾“å‡ºå·®å¼‚: {diff.item():.6f}")
    
    print(f"âœ“ å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆæ€§éªŒè¯é€šè¿‡")


def run_integration_tests():
    """è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
    print("å¼€å§‹TransformerGNNä¸å±€éƒ¨æ³¨æ„åŠ›é›†æˆæµ‹è¯•")
    print("=" * 60)
    
    try:
        # è¿è¡Œå„é¡¹æµ‹è¯•
        test_transformer_gnn_with_local_attention()
        test_forward_pass()
        test_gradient_computation()
        test_different_scales()
        test_local_attention_effectiveness()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼TransformerGNNä¸å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶é›†æˆæˆåŠŸã€‚")
        print("=" * 60)
        
        return True
        
    except Exception as e:
        print(f"\nâŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_integration_tests()
    exit(0 if success else 1)