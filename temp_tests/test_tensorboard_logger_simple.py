# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„TensorBoardè®°å½•å™¨æµ‹è¯•
æµ‹è¯•å°ºåº¦ä¸å˜æŒ‡æ ‡çš„TensorBoardé›†æˆåŠŸèƒ½
"""

import unittest
import tempfile
import shutil
import numpy as np
from pathlib import Path
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from scale_invariant_tensorboard_logger import ScaleInvariantTensorBoardLogger
    TENSORBOARD_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: TensorBoardæˆ–ç›¸å…³ä¾èµ–ä¸å¯ç”¨ï¼Œå°†è·³è¿‡TensorBoardæµ‹è¯•")
    TENSORBOARD_AVAILABLE = False


class MockTensorBoardLogger:
    """æ¨¡æ‹ŸTensorBoardè®°å½•å™¨ï¼Œç”¨äºæµ‹è¯•æ ¸å¿ƒé€»è¾‘"""
    
    def __init__(self, log_dir: str, experiment_name: str = "test"):
        self.log_dir = Path(log_dir)
        self.experiment_name = experiment_name
        self.metrics_history = {
            "per_agent_reward": [],
            "normalized_completion_score": [],
            "efficiency_metric": [],
            "scenario_scales": [],
            "timestamps": []
        }
        self.logged_metrics = []
        print(f"æ¨¡æ‹ŸTensorBoardè®°å½•å™¨åˆå§‹åŒ–: {log_dir}")
    
    def log_scale_invariant_metrics(self, metrics, step, scenario_info=None, stage_info=None):
        """è®°å½•å°ºåº¦ä¸å˜æŒ‡æ ‡"""
        # è®°å½•åŸºç¡€æŒ‡æ ‡
        for metric_name, value in metrics.items():
            if metric_name in self.metrics_history:
                self.metrics_history[metric_name].append(value)
        
        # è®°å½•åœºæ™¯è§„æ¨¡
        if scenario_info and "n_uavs" in scenario_info and "n_targets" in scenario_info:
            scale_factor = scenario_info["n_uavs"] * scenario_info["n_targets"]
            self.metrics_history["scenario_scales"].append(scale_factor)
        
        # ä¿å­˜è®°å½•çš„æŒ‡æ ‡ç”¨äºéªŒè¯
        log_entry = {
            "step": step,
            "metrics": metrics.copy(),
            "scenario_info": scenario_info.copy() if scenario_info else None,
            "stage_info": stage_info.copy() if stage_info else None
        }
        self.logged_metrics.append(log_entry)
        
        print(f"è®°å½•æŒ‡æ ‡ - Step {step}: {metrics}")
    
    def log_cross_scale_comparison(self, scale_performance_data, step):
        """è®°å½•è·¨å°ºåº¦æ€§èƒ½å¯¹æ¯”"""
        print(f"è®°å½•è·¨å°ºåº¦å¯¹æ¯” - Step {step}: {len(scale_performance_data)} ä¸ªè§„æ¨¡")
    
    def log_zero_shot_transfer_results(self, transfer_results, step):
        """è®°å½•é›¶æ ·æœ¬è¿ç§»ç»“æœ"""
        print(f"è®°å½•é›¶æ ·æœ¬è¿ç§» - Step {step}: {transfer_results.get('source_scale', 0)} â†’ {transfer_results.get('target_scale', 0)}")
    
    def create_training_summary_report(self, output_path=None):
        """åˆ›å»ºè®­ç»ƒæ‘˜è¦æŠ¥å‘Š"""
        if output_path is None:
            output_path = self.log_dir / f"{self.experiment_name}_summary.md"
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if not self.metrics_history["per_agent_reward"]:
            return str(output_path)
        
        stats = {}
        for metric_name in ["per_agent_reward", "normalized_completion_score", "efficiency_metric"]:
            if self.metrics_history[metric_name]:
                values = self.metrics_history[metric_name]
                stats[metric_name] = {
                    "mean": np.mean(values),
                    "std": np.std(values),
                    "max": np.max(values),
                    "min": np.min(values)
                }
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = f"""# å°ºåº¦ä¸å˜æŒ‡æ ‡è®­ç»ƒæ‘˜è¦æŠ¥å‘Š

## å®éªŒä¿¡æ¯
- **å®éªŒåç§°**: {self.experiment_name}
- **æ€»è®°å½•æ•°**: {len(self.logged_metrics)}

## æŒ‡æ ‡ç»Ÿè®¡æ‘˜è¦
"""
        
        for metric_name, stat in stats.items():
            report_content += f"""
### {metric_name}
- **å¹³å‡å€¼**: {stat["mean"]:.4f}
- **æ ‡å‡†å·®**: {stat["std"]:.4f}
- **æœ€å¤§å€¼**: {stat["max"]:.4f}
- **æœ€å°å€¼**: {stat["min"]:.4f}
"""
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å†™å…¥æŠ¥å‘Š
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"æ‘˜è¦æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
        return str(output_path)
    
    def close(self):
        """å…³é—­è®°å½•å™¨"""
        print("æ¨¡æ‹ŸTensorBoardè®°å½•å™¨å·²å…³é—­")


class TestTensorBoardLogger(unittest.TestCase):
    """æµ‹è¯•TensorBoardè®°å½•å™¨åŠŸèƒ½"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.temp_dir = tempfile.mkdtemp()
        
        # æ ¹æ®å¯ç”¨æ€§é€‰æ‹©è®°å½•å™¨
        if TENSORBOARD_AVAILABLE:
            try:
                self.logger = ScaleInvariantTensorBoardLogger(self.temp_dir, "test_experiment")
            except Exception as e:
                print(f"TensorBoardè®°å½•å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç‰ˆæœ¬: {e}")
                self.logger = MockTensorBoardLogger(self.temp_dir, "test_experiment")
        else:
            self.logger = MockTensorBoardLogger(self.temp_dir, "test_experiment")
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        self.logger.close()
        shutil.rmtree(self.temp_dir)
    
    def test_basic_metrics_logging(self):
        """æµ‹è¯•åŸºç¡€æŒ‡æ ‡è®°å½•"""
        metrics = {
            "per_agent_reward": 15.5,
            "normalized_completion_score": 0.75,
            "efficiency_metric": 0.002
        }
        
        scenario_info = {
            "n_uavs": 3,
            "n_targets": 2
        }
        
        stage_info = {
            "current_stage": 1,
            "stage_progress": 0.6
        }
        
        # è®°å½•æŒ‡æ ‡
        self.logger.log_scale_invariant_metrics(metrics, 100, scenario_info, stage_info)
        
        # éªŒè¯è®°å½•
        if hasattr(self.logger, 'metrics_history'):
            self.assertEqual(len(self.logger.metrics_history["per_agent_reward"]), 1)
            self.assertEqual(self.logger.metrics_history["per_agent_reward"][0], 15.5)
            
            if self.logger.metrics_history["scenario_scales"]:
                expected_scale = 3 * 2  # n_uavs * n_targets
                self.assertEqual(self.logger.metrics_history["scenario_scales"][0], expected_scale)
        
        print("âœ“ åŸºç¡€æŒ‡æ ‡è®°å½•æµ‹è¯•é€šè¿‡")
    
    def test_multiple_metrics_logging(self):
        """æµ‹è¯•å¤šæ¬¡æŒ‡æ ‡è®°å½•"""
        # è®°å½•å¤šä¸ªæ—¶é—´æ­¥çš„æŒ‡æ ‡
        for step in range(10):
            metrics = {
                "per_agent_reward": 10 + step + np.random.normal(0, 1),
                "normalized_completion_score": 0.5 + step * 0.03 + np.random.uniform(-0.05, 0.05),
                "efficiency_metric": 0.001 + step * 0.0001 + np.random.uniform(-0.0001, 0.0001)
            }
            
            scenario_info = {
                "n_uavs": 3 + (step // 3),
                "n_targets": 2 + (step // 5)
            }
            
            self.logger.log_scale_invariant_metrics(metrics, step, scenario_info)
        
        # éªŒè¯è®°å½•æ•°é‡
        if hasattr(self.logger, 'metrics_history'):
            self.assertEqual(len(self.logger.metrics_history["per_agent_reward"]), 10)
            self.assertEqual(len(self.logger.metrics_history["normalized_completion_score"]), 10)
            self.assertEqual(len(self.logger.metrics_history["efficiency_metric"]), 10)
        
        print("âœ“ å¤šæ¬¡æŒ‡æ ‡è®°å½•æµ‹è¯•é€šè¿‡")
    
    def test_cross_scale_comparison(self):
        """æµ‹è¯•è·¨å°ºåº¦æ€§èƒ½å¯¹æ¯”"""
        scale_data = {
            6: {"per_agent_reward": 10.0, "normalized_completion_score": 0.6, "efficiency_metric": 0.002},
            12: {"per_agent_reward": 8.0, "normalized_completion_score": 0.7, "efficiency_metric": 0.0015},
            20: {"per_agent_reward": 6.0, "normalized_completion_score": 0.8, "efficiency_metric": 0.001}
        }
        
        # è®°å½•è·¨å°ºåº¦å¯¹æ¯”
        self.logger.log_cross_scale_comparison(scale_data, 200)
        
        print("âœ“ è·¨å°ºåº¦å¯¹æ¯”è®°å½•æµ‹è¯•é€šè¿‡")
    
    def test_zero_shot_transfer_logging(self):
        """æµ‹è¯•é›¶æ ·æœ¬è¿ç§»ç»“æœè®°å½•"""
        transfer_results = {
            "source_stage": 0,
            "target_stage": 2,
            "source_scale": 6,
            "target_scale": 20,
            "transfer_performance": {
                "per_agent_reward": 7.5,
                "normalized_completion_score": 0.65,
                "efficiency_metric": 0.0018
            },
            "baseline_performance": {
                "per_agent_reward": 10.0,
                "normalized_completion_score": 0.8,
                "efficiency_metric": 0.002
            }
        }
        
        # è®°å½•é›¶æ ·æœ¬è¿ç§»ç»“æœ
        self.logger.log_zero_shot_transfer_results(transfer_results, 300)
        
        print("âœ“ é›¶æ ·æœ¬è¿ç§»è®°å½•æµ‹è¯•é€šè¿‡")
    
    def test_summary_report_generation(self):
        """æµ‹è¯•æ‘˜è¦æŠ¥å‘Šç”Ÿæˆ"""
        # æ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®
        for i in range(20):
            metrics = {
                "per_agent_reward": 10 + np.random.normal(0, 2),
                "normalized_completion_score": 0.7 + np.random.uniform(-0.1, 0.2),
                "efficiency_metric": 0.001 + np.random.uniform(0, 0.0005)
            }
            self.logger.log_scale_invariant_metrics(metrics, i)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_path = self.logger.create_training_summary_report()
        
        # éªŒè¯æŠ¥å‘Šæ–‡ä»¶å­˜åœ¨
        self.assertTrue(Path(report_path).exists())
        
        # éªŒè¯æŠ¥å‘Šå†…å®¹
        with open(report_path, 'r', encoding='utf-8') as f:
            content = f.read()
            self.assertIn("å°ºåº¦ä¸å˜æŒ‡æ ‡è®­ç»ƒæ‘˜è¦æŠ¥å‘Š", content)
            self.assertIn("Per-Agent Reward", content)  # ä¿®æ­£ä¸ºä¸­æ–‡æ ‡é¢˜
            self.assertIn("Normalized Completion Score", content)  # ä¿®æ­£ä¸ºä¸­æ–‡æ ‡é¢˜
            self.assertIn("Efficiency Metric", content)  # ä¿®æ­£ä¸ºä¸­æ–‡æ ‡é¢˜
        
        print(f"âœ“ æ‘˜è¦æŠ¥å‘Šç”Ÿæˆæµ‹è¯•é€šè¿‡: {report_path}")
    
    def test_metrics_validation(self):
        """æµ‹è¯•æŒ‡æ ‡éªŒè¯"""
        # æµ‹è¯•æ­£å¸¸æŒ‡æ ‡
        valid_metrics = {
            "per_agent_reward": 15.0,
            "normalized_completion_score": 0.8,
            "efficiency_metric": 0.003
        }
        
        try:
            self.logger.log_scale_invariant_metrics(valid_metrics, 1)
            print("âœ“ æœ‰æ•ˆæŒ‡æ ‡éªŒè¯é€šè¿‡")
        except Exception as e:
            self.fail(f"æœ‰æ•ˆæŒ‡æ ‡è®°å½•å¤±è´¥: {e}")
        
        # æµ‹è¯•è¾¹ç•Œå€¼
        boundary_metrics = {
            "per_agent_reward": 0.0,
            "normalized_completion_score": 0.0,
            "efficiency_metric": 0.0
        }
        
        try:
            self.logger.log_scale_invariant_metrics(boundary_metrics, 2)
            print("âœ“ è¾¹ç•Œå€¼æŒ‡æ ‡éªŒè¯é€šè¿‡")
        except Exception as e:
            self.fail(f"è¾¹ç•Œå€¼æŒ‡æ ‡è®°å½•å¤±è´¥: {e}")
    
    def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        # æµ‹è¯•ç©ºæŒ‡æ ‡
        try:
            self.logger.log_scale_invariant_metrics({}, 1)
            print("âœ“ ç©ºæŒ‡æ ‡å¤„ç†æµ‹è¯•é€šè¿‡")
        except Exception as e:
            print(f"ç©ºæŒ‡æ ‡å¤„ç†: {e}")
        
        # æµ‹è¯•Noneå€¼
        try:
            self.logger.log_scale_invariant_metrics(None, 2)
            print("âœ“ Noneå€¼å¤„ç†æµ‹è¯•é€šè¿‡")
        except Exception as e:
            print(f"Noneå€¼å¤„ç†: {e}")


class TestMetricsCalculationIntegration(unittest.TestCase):
    """æµ‹è¯•æŒ‡æ ‡è®¡ç®—ä¸è®°å½•çš„é›†æˆ"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.temp_dir = tempfile.mkdtemp()
        self.logger = MockTensorBoardLogger(self.temp_dir, "integration_test")
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        self.logger.close()
        shutil.rmtree(self.temp_dir)
    
    def test_end_to_end_workflow(self):
        """æµ‹è¯•ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹"""
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        stages = [
            {"n_uavs": 3, "n_targets": 2, "episodes": 5},
            {"n_uavs": 5, "n_targets": 3, "episodes": 5},
            {"n_uavs": 8, "n_targets": 5, "episodes": 5}
        ]
        
        step = 0
        for stage_id, stage_config in enumerate(stages):
            print(f"\næ¨¡æ‹Ÿè®­ç»ƒé˜¶æ®µ {stage_id}: {stage_config}")
            
            for episode in range(stage_config["episodes"]):
                # æ¨¡æ‹ŸæŒ‡æ ‡è®¡ç®—
                base_performance = 0.6 + stage_id * 0.1
                metrics = {
                    "per_agent_reward": 10 + stage_id * 2 + np.random.normal(0, 1),
                    "normalized_completion_score": base_performance + np.random.uniform(-0.1, 0.1),
                    "efficiency_metric": 0.001 + stage_id * 0.0005 + np.random.uniform(-0.0002, 0.0002)
                }
                
                scenario_info = {
                    "n_uavs": stage_config["n_uavs"],
                    "n_targets": stage_config["n_targets"]
                }
                
                stage_info = {
                    "current_stage": stage_id,
                    "stage_progress": episode / stage_config["episodes"]
                }
                
                # è®°å½•æŒ‡æ ‡
                self.logger.log_scale_invariant_metrics(metrics, step, scenario_info, stage_info)
                step += 1
        
        # éªŒè¯è®°å½•çš„å®Œæ•´æ€§
        total_episodes = sum(stage["episodes"] for stage in stages)
        if hasattr(self.logger, 'logged_metrics'):
            self.assertEqual(len(self.logger.logged_metrics), total_episodes)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        report_path = self.logger.create_training_summary_report()
        self.assertTrue(Path(report_path).exists())
        
        print(f"âœ“ ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡ï¼Œå…±è®°å½• {total_episodes} ä¸ªepisode")
    
    def test_performance_trend_analysis(self):
        """æµ‹è¯•æ€§èƒ½è¶‹åŠ¿åˆ†æ"""
        # æ¨¡æ‹Ÿæ€§èƒ½æ”¹è¿›è¶‹åŠ¿
        for step in range(50):
            # æ¨¡æ‹Ÿé€æ­¥æ”¹è¿›çš„æ€§èƒ½
            progress = step / 50.0
            metrics = {
                "per_agent_reward": 5 + progress * 10 + np.random.normal(0, 0.5),
                "normalized_completion_score": 0.3 + progress * 0.5 + np.random.uniform(-0.05, 0.05),
                "efficiency_metric": 0.0005 + progress * 0.002 + np.random.uniform(-0.0001, 0.0001)
            }
            
            self.logger.log_scale_invariant_metrics(metrics, step)
        
        # éªŒè¯è¶‹åŠ¿
        if hasattr(self.logger, 'metrics_history'):
            scores = self.logger.metrics_history["normalized_completion_score"]
            if len(scores) >= 10:
                early_avg = np.mean(scores[:10])
                late_avg = np.mean(scores[-10:])
                
                # åº”è¯¥æœ‰æ”¹è¿›è¶‹åŠ¿
                self.assertGreater(late_avg, early_avg)
                print(f"âœ“ æ€§èƒ½è¶‹åŠ¿åˆ†æ: æ—©æœŸå¹³å‡ {early_avg:.3f} â†’ åæœŸå¹³å‡ {late_avg:.3f}")


def run_tensorboard_test():
    """è¿è¡ŒTensorBoardæµ‹è¯•"""
    print("=" * 60)
    print("å¼€å§‹TensorBoardè®°å½•å™¨æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    test_suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestTensorBoardLogger))
    test_suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestMetricsCalculationIntegration))
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ‘˜è¦:")
    print(f"æ€»æµ‹è¯•æ•°: {result.testsRun}")
    print(f"æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"å¤±è´¥: {len(result.failures)}")
    print(f"é”™è¯¯: {len(result.errors)}")
    
    if result.failures:
        print("\nå¤±è´¥çš„æµ‹è¯•:")
        for test, traceback in result.failures:
            print(f"- {test}: {traceback}")
    
    if result.errors:
        print("\né”™è¯¯çš„æµ‹è¯•:")
        for test, traceback in result.errors:
            print(f"- {test}: {traceback}")
    
    print("=" * 60)
    
    return result.wasSuccessful()


if __name__ == "__main__":
    success = run_tensorboard_test()
    
    if success:
        print("ğŸ‰ TensorBoardè®°å½•å™¨æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
    else:
        print("âŒ éƒ¨åˆ†TensorBoardæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")
    
    exit(0 if success else 1)
