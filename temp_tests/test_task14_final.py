"""
ä»»åŠ¡14æœ€ç»ˆç»¼åˆæµ‹è¯•
æµ‹è¯•è®­ç»ƒæ•°æ®ä¿å­˜ä¸TensorBoardé›†æˆçš„å®Œæ•´åŠŸèƒ½
åŒ…å«æ‰€æœ‰æ ¸å¿ƒæ¨¡å—çš„é›†æˆæµ‹è¯•å’ŒåŠŸèƒ½éªŒè¯
"""

import os
import sys
import unittest
import tempfile
import shutil
import torch
import numpy as np
import json
from pathlib import Path
from datetime import datetime
from unittest.mock import Mock, patch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ‰€æœ‰æ ¸å¿ƒæ¨¡å—
from training_logger import (
    CurriculumTensorBoardLogger, 
    ModelCheckpointManager, 
    create_training_config_with_logging
)
from stage_config_manager import StageConfig, StageConfigManager


class TestTask14CoreFunctionality(unittest.TestCase):
    """ä»»åŠ¡14æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.test_dir = tempfile.mkdtemp()
        print(f"æµ‹è¯•ç›®å½•: {self.test_dir}")
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def test_tensorboard_logger_core_features(self):
        """æµ‹è¯•TensorBoardæ—¥å¿—è®°å½•å™¨æ ¸å¿ƒåŠŸèƒ½"""
        print("\nğŸ§ª æµ‹è¯•TensorBoardæ—¥å¿—è®°å½•å™¨æ ¸å¿ƒåŠŸèƒ½...")
        
        # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        logger = CurriculumTensorBoardLogger(self.test_dir, "core_test")
        
        try:
            # 1. æµ‹è¯•å°ºåº¦ä¸å˜æŒ‡æ ‡è®°å½•
            metrics = {
                "per_agent_reward": 12.5,
                "normalized_completion_score": 0.85,
                "efficiency_metric": 0.42
            }
            logger.log_scale_invariant_metrics(metrics, 1000, 1, 5, 3)
            
            # éªŒè¯æŒ‡æ ‡è®°å½•
            self.assertEqual(len(logger.training_history["metrics"]), 1)
            metric_record = logger.training_history["metrics"][0]
            self.assertEqual(metric_record["per_agent_reward"], 12.5)
            self.assertEqual(metric_record["normalized_completion_score"], 0.85)
            
            # 2. æµ‹è¯•é˜¶æ®µåˆ‡æ¢è®°å½•
            logger.log_stage_transition(0, 1, 1000, "performance_threshold")
            
            # éªŒè¯é˜¶æ®µåˆ‡æ¢è®°å½•
            self.assertEqual(len(logger.training_history["stage_transitions"]), 1)
            transition = logger.training_history["stage_transitions"][0]
            self.assertEqual(transition["from_stage"], 0)
            self.assertEqual(transition["to_stage"], 1)
            
            # 3. æµ‹è¯•å›é€€äº‹ä»¶è®°å½•
            logger.log_rollback_event(1, 2000, 0.15, 0.1)
            
            # éªŒè¯å›é€€äº‹ä»¶è®°å½•
            self.assertEqual(len(logger.training_history["rollback_events"]), 1)
            rollback = logger.training_history["rollback_events"][0]
            self.assertEqual(rollback["performance_drop"], 0.15)
            
            # 4. æµ‹è¯•è®­ç»ƒå†å²ä¿å­˜
            logger.save_training_history()
            history_file = Path(self.test_dir) / "core_test_history.json"
            self.assertTrue(history_file.exists())
            
            print("âœ… TensorBoardæ—¥å¿—è®°å½•å™¨æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡")
            
        finally:
            logger.close()
    
    def test_model_checkpoint_manager_features(self):
        """æµ‹è¯•æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†å™¨åŠŸèƒ½"""
        print("\nğŸ§ª æµ‹è¯•æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†å™¨åŠŸèƒ½...")
        
        checkpoint_manager = ModelCheckpointManager(
            os.path.join(self.test_dir, "checkpoints"), 
            max_checkpoints=3
        )
        
        # 1. æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜
        model_states = []
        for i in range(5):
            model_state = {"layer1.weight": torch.randn(10, 5)}
            optimizer_state = {"lr": 0.001}
            metrics = {"normalized_completion_score": 0.7 + i * 0.05}
            
            is_best = (i == 3)  # ç¬¬4ä¸ªæ˜¯æœ€ä½³çš„
            checkpoint_path = checkpoint_manager.save_checkpoint(
                model_state, optimizer_state, metrics, 
                i*1000, 1, is_best
            )
            
            model_states.append((checkpoint_path, is_best, metrics))
            self.assertTrue(os.path.exists(checkpoint_path))
        
        # 2. æµ‹è¯•æœ€ä½³æ£€æŸ¥ç‚¹è·å–
        best_checkpoint = checkpoint_manager.get_best_checkpoint()
        self.assertIsNotNone(best_checkpoint)
        
        # éªŒè¯æœ€ä½³æ£€æŸ¥ç‚¹
        loaded_data = checkpoint_manager.load_checkpoint(best_checkpoint)
        self.assertEqual(loaded_data["metrics"]["normalized_completion_score"], 0.85)
        
        # 3. æµ‹è¯•æ£€æŸ¥ç‚¹å†å²ç®¡ç†
        self.assertGreater(len(checkpoint_manager.checkpoint_history), 0)
        
        # 4. æµ‹è¯•æ£€æŸ¥ç‚¹æ¸…ç†ï¼ˆåº”è¯¥ä¿ç•™æœ€ä½³ + æœ€è¿‘çš„3ä¸ªï¼‰
        non_best_count = len([cp for cp in checkpoint_manager.checkpoint_history if not cp["is_best"]])
        self.assertLessEqual(non_best_count, checkpoint_manager.max_checkpoints)
        
        print("âœ… æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†å™¨åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    
    def test_stage_config_manager_features(self):
        """æµ‹è¯•é˜¶æ®µé…ç½®ç®¡ç†å™¨åŠŸèƒ½"""
        print("\nğŸ§ª æµ‹è¯•é˜¶æ®µé…ç½®ç®¡ç†å™¨åŠŸèƒ½...")
        
        config_manager = StageConfigManager(os.path.join(self.test_dir, "configs"))
        
        # 1. æµ‹è¯•é»˜è®¤é…ç½®åŠ è½½
        self.assertGreater(len(config_manager.stage_configs), 0)
        
        config_0 = config_manager.get_stage_config(0)
        self.assertIsNotNone(config_0)
        self.assertEqual(config_0.stage_id, 0)
        
        # 2. æµ‹è¯•é…ç½®æ›´æ–°
        original_lr = config_0.learning_rate
        config_manager.update_stage_config(0, learning_rate=0.002)
        
        updated_config = config_manager.get_stage_config(0)
        self.assertEqual(updated_config.learning_rate, 0.002)
        
        # 3. æµ‹è¯•æ€§èƒ½è®°å½•
        for i in range(10):
            performance = {
                "per_agent_reward": 10 + i,
                "normalized_completion_score": 0.7 + i * 0.02,
                "efficiency_metric": 0.3 + i * 0.01
            }
            config_manager.record_stage_performance(0, performance, i, i*100)
        
        # 4. æµ‹è¯•æ€§èƒ½æ‘˜è¦
        summary = config_manager.get_stage_performance_summary(0)
        self.assertEqual(summary["total_records"], 10)
        self.assertIn("per_agent_reward_mean", summary)
        
        # 5. æµ‹è¯•æœ€ä½³æ¨¡å‹ä¿å­˜
        model_state = {"layer1.weight": torch.randn(10, 5)}
        performance_metrics = {"normalized_completion_score": 0.9}
        training_config = {"learning_rate": 0.001}
        
        config_manager.save_best_model(0, model_state, performance_metrics, training_config)
        
        # éªŒè¯æœ€ä½³æ¨¡å‹åŠ è½½
        loaded_model = config_manager.load_best_model(0)
        self.assertIsNotNone(loaded_model)
        self.assertEqual(loaded_model["performance_metrics"]["normalized_completion_score"], 0.9)
        
        # 6. æµ‹è¯•é˜¶æ®µåˆ‡æ¢å»ºè®®
        high_performance = [{"normalized_completion_score": 0.85}] * 5
        for i, perf in enumerate(high_performance):
            config_manager.record_stage_performance(1, perf, i, i*100)
        
        recommendation = config_manager.get_stage_transition_recommendation(1, high_performance)
        self.assertIn("action", recommendation)
        
        print("âœ… é˜¶æ®µé…ç½®ç®¡ç†å™¨åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    
    def test_training_config_enhancement(self):
        """æµ‹è¯•è®­ç»ƒé…ç½®å¢å¼ºåŠŸèƒ½"""
        print("\nğŸ§ª æµ‹è¯•è®­ç»ƒé…ç½®å¢å¼ºåŠŸèƒ½...")
        
        # åŸºç¡€é…ç½®
        base_config = {
            "env": "UAVTaskEnv",
            "num_workers": 4,
            "lr": 0.001,
            "batch_size": 128
        }
        
        # å¢å¼ºé…ç½®
        enhanced_config = create_training_config_with_logging(
            base_config,
            log_dir=self.test_dir,
            experiment_name="config_test"
        )
        
        # éªŒè¯åŸºç¡€é…ç½®ä¿ç•™
        self.assertEqual(enhanced_config["env"], "UAVTaskEnv")
        self.assertEqual(enhanced_config["num_workers"], 4)
        self.assertEqual(enhanced_config["lr"], 0.001)
        
        # éªŒè¯æ—¥å¿—é…ç½®æ·»åŠ 
        self.assertEqual(enhanced_config["log_dir"], self.test_dir)
        self.assertEqual(enhanced_config["experiment_name"], "config_test")
        
        # éªŒè¯TensorBoardå’Œæ£€æŸ¥ç‚¹é…ç½®
        self.assertIn("logger_config", enhanced_config)
        self.assertIn("checkpoint_freq", enhanced_config)
        self.assertIn("callbacks", enhanced_config)
        
        print("âœ… è®­ç»ƒé…ç½®å¢å¼ºåŠŸèƒ½æµ‹è¯•é€šè¿‡")


class TestTask14IntegrationWorkflow(unittest.TestCase):
    """ä»»åŠ¡14é›†æˆå·¥ä½œæµæµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.test_dir = tempfile.mkdtemp()
        print(f"é›†æˆæµ‹è¯•ç›®å½•: {self.test_dir}")
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def test_complete_curriculum_training_workflow(self):
        """æµ‹è¯•å®Œæ•´çš„è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå·¥ä½œæµ"""
        print("\nğŸ”„ æµ‹è¯•å®Œæ•´çš„è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå·¥ä½œæµ...")
        
        # 1. åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        logger = CurriculumTensorBoardLogger(self.test_dir, "workflow_test")
        checkpoint_manager = ModelCheckpointManager(
            os.path.join(self.test_dir, "checkpoints")
        )
        config_manager = StageConfigManager(
            os.path.join(self.test_dir, "configs")
        )
        
        try:
            # 2. æ¨¡æ‹Ÿå¤šé˜¶æ®µè®­ç»ƒè¿‡ç¨‹
            stages = [0, 1, 2]
            stage_performance = {}
            
            for stage in stages:
                print(f"   æ¨¡æ‹Ÿé˜¶æ®µ {stage} è®­ç»ƒ...")
                stage_performance[stage] = []
                
                # æ¯ä¸ªé˜¶æ®µè®­ç»ƒå¤šä¸ªæ­¥éª¤
                for step in range(stage * 1000, (stage + 1) * 1000, 200):
                    # ç”Ÿæˆæ¨¡æ‹ŸæŒ‡æ ‡ï¼ˆéšç€é˜¶æ®µå¢åŠ ï¼Œæ€§èƒ½é€æ¸æå‡ï¼‰
                    base_performance = 0.6 + stage * 0.1
                    metrics = {
                        "per_agent_reward": 10 + stage * 2 + np.random.normal(0, 0.5),
                        "normalized_completion_score": base_performance + np.random.uniform(-0.05, 0.1),
                        "efficiency_metric": 0.3 + stage * 0.05 + np.random.uniform(-0.02, 0.05)
                    }
                    
                    # è®°å½•åˆ°æ—¥å¿—
                    n_uavs = 3 + stage
                    n_targets = 2 + stage
                    logger.log_scale_invariant_metrics(metrics, step, stage, n_uavs, n_targets)
                    
                    # è®°å½•åˆ°é…ç½®ç®¡ç†å™¨
                    episode = (step - stage * 1000) // 200
                    config_manager.record_stage_performance(stage, metrics, episode, step)
                    stage_performance[stage].append(metrics)
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    if step % 400 == 0:
                        model_state = {"layer1.weight": torch.randn(10, 5)}
                        optimizer_state = {"lr": 0.001 - stage * 0.0002}
                        
                        is_best = metrics["normalized_completion_score"] > base_performance + 0.05
                        checkpoint_manager.save_checkpoint(
                            model_state, optimizer_state, metrics,
                            step, stage, is_best
                        )
                        
                        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œä¹Ÿä¿å­˜åˆ°é…ç½®ç®¡ç†å™¨
                        if is_best:
                            training_config = {"learning_rate": optimizer_state["lr"]}
                            config_manager.save_best_model(
                                stage, model_state, metrics, training_config
                            )
                
                # è®°å½•é˜¶æ®µåˆ‡æ¢
                if stage < len(stages) - 1:
                    logger.log_stage_transition(stage, stage + 1, (stage + 1) * 1000, "performance_threshold")
                
                # æ¨¡æ‹Ÿå¶å°”çš„å›é€€äº‹ä»¶
                if stage == 1 and np.random.random() > 0.7:
                    logger.log_rollback_event(stage, stage * 1000 + 500, 0.12, 0.1)
            
            # 3. éªŒè¯å·¥ä½œæµç»“æœ
            print("   éªŒè¯å·¥ä½œæµç»“æœ...")
            
            # éªŒè¯æ—¥å¿—è®°å½•
            self.assertGreater(len(logger.training_history["metrics"]), 0)
            self.assertGreater(len(logger.training_history["stage_transitions"]), 0)
            
            # éªŒè¯æ£€æŸ¥ç‚¹
            self.assertGreater(len(checkpoint_manager.checkpoint_history), 0)
            best_checkpoint = checkpoint_manager.get_best_checkpoint()
            if best_checkpoint:
                self.assertTrue(os.path.exists(best_checkpoint))
            
            # éªŒè¯é…ç½®ç®¡ç†
            for stage in stages:
                summary = config_manager.get_stage_performance_summary(stage)
                if summary:
                    self.assertGreater(summary["total_records"], 0)
            
            # 4. æµ‹è¯•æ•°æ®ä¿å­˜å’ŒåŠ è½½
            print("   æµ‹è¯•æ•°æ®ä¿å­˜å’ŒåŠ è½½...")
            
            # ä¿å­˜æ‰€æœ‰æ•°æ®
            logger.save_training_history()
            checkpoint_manager.save_checkpoint_history()
            config_manager.save_all_data()
            
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            history_file = Path(self.test_dir) / "workflow_test_history.json"
            self.assertTrue(history_file.exists())
            
            checkpoint_history_file = Path(self.test_dir) / "checkpoints" / "checkpoint_history.json"
            self.assertTrue(checkpoint_history_file.exists())
            
            config_export_file = Path(self.test_dir) / "configs" / "all_stage_configs.json"
            self.assertTrue(config_export_file.exists())
            
            # 5. æµ‹è¯•æ•°æ®å®Œæ•´æ€§
            print("   éªŒè¯æ•°æ®å®Œæ•´æ€§...")
            
            # éªŒè¯è®­ç»ƒå†å²
            with open(history_file, 'r', encoding='utf-8') as f:
                saved_history = json.load(f)
            
            self.assertIn("metrics", saved_history)
            self.assertIn("stage_transitions", saved_history)
            self.assertGreater(len(saved_history["metrics"]), 0)
            
            # éªŒè¯é…ç½®å¯¼å‡º
            with open(config_export_file, 'r', encoding='utf-8') as f:
                saved_configs = json.load(f)
            
            self.assertIn("configs", saved_configs)
            self.assertGreater(len(saved_configs["configs"]), 0)
            
            print("âœ… å®Œæ•´çš„è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå·¥ä½œæµæµ‹è¯•é€šè¿‡")
            
        finally:
            logger.close()
    
    def test_scale_invariant_metrics_validation(self):
        """æµ‹è¯•å°ºåº¦ä¸å˜æŒ‡æ ‡çš„æ­£ç¡®æ€§éªŒè¯"""
        print("\nğŸ§ª æµ‹è¯•å°ºåº¦ä¸å˜æŒ‡æ ‡çš„æ­£ç¡®æ€§éªŒè¯...")
        
        logger = CurriculumTensorBoardLogger(self.test_dir, "metrics_test")
        
        try:
            # æµ‹è¯•ä¸åŒè§„æ¨¡åœºæ™¯ä¸‹çš„æŒ‡æ ‡è®°å½•
            scenarios = [
                {"n_uavs": 3, "n_targets": 2, "scale": "small"},
                {"n_uavs": 6, "n_targets": 4, "scale": "medium"},
                {"n_uavs": 12, "n_targets": 8, "scale": "large"}
            ]
            
            for i, scenario in enumerate(scenarios):
                # æ¨¡æ‹Ÿç›¸åŒè´¨é‡çš„æ€§èƒ½ï¼Œä½†ä¸åŒè§„æ¨¡
                metrics = {
                    "per_agent_reward": 10.0,  # æ¯ä¸ªæ™ºèƒ½ä½“çš„å¥–åŠ±åº”è¯¥ä¿æŒä¸€è‡´
                    "normalized_completion_score": 0.8,  # å½’ä¸€åŒ–å®Œæˆåˆ†æ•°åº”è¯¥ä¿æŒä¸€è‡´
                    "efficiency_metric": 0.4  # æ•ˆç‡æŒ‡æ ‡åº”è¯¥ä¿æŒä¸€è‡´
                }
                
                logger.log_scale_invariant_metrics(
                    metrics, i * 1000, 0, 
                    scenario["n_uavs"], scenario["n_targets"]
                )
            
            # éªŒè¯æ‰€æœ‰è®°å½•çš„æŒ‡æ ‡éƒ½æ˜¯å°ºåº¦ä¸å˜çš„
            for record in logger.training_history["metrics"]:
                self.assertAlmostEqual(record["per_agent_reward"], 10.0, places=1)
                self.assertAlmostEqual(record["normalized_completion_score"], 0.8, places=1)
                self.assertAlmostEqual(record["efficiency_metric"], 0.4, places=1)
            
            print("âœ… å°ºåº¦ä¸å˜æŒ‡æ ‡éªŒè¯æµ‹è¯•é€šè¿‡")
            
        finally:
            logger.close()


class TestTask14ErrorHandling(unittest.TestCase):
    """ä»»åŠ¡14é”™è¯¯å¤„ç†æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.test_dir = tempfile.mkdtemp()
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def test_missing_directory_handling(self):
        """æµ‹è¯•ç¼ºå¤±ç›®å½•å¤„ç†"""
        print("\nğŸ§ª æµ‹è¯•ç¼ºå¤±ç›®å½•å¤„ç†...")
        
        # æµ‹è¯•åœ¨ä¸å­˜åœ¨çš„ç›®å½•ä¸­åˆå§‹åŒ–ç»„ä»¶
        non_existent_dir = os.path.join(self.test_dir, "non_existent")
        
        # åº”è¯¥è‡ªåŠ¨åˆ›å»ºç›®å½•
        logger = CurriculumTensorBoardLogger(non_existent_dir, "error_test")
        self.assertTrue(Path(non_existent_dir).exists())
        logger.close()
        
        checkpoint_manager = ModelCheckpointManager(
            os.path.join(non_existent_dir, "checkpoints")
        )
        self.assertTrue(Path(non_existent_dir, "checkpoints").exists())
        
        config_manager = StageConfigManager(
            os.path.join(non_existent_dir, "configs")
        )
        self.assertTrue(Path(non_existent_dir, "configs").exists())
        
        print("âœ… ç¼ºå¤±ç›®å½•å¤„ç†æµ‹è¯•é€šè¿‡")
    
    def test_invalid_data_handling(self):
        """æµ‹è¯•æ— æ•ˆæ•°æ®å¤„ç†"""
        print("\nğŸ§ª æµ‹è¯•æ— æ•ˆæ•°æ®å¤„ç†...")
        
        logger = CurriculumTensorBoardLogger(self.test_dir, "invalid_test")
        
        try:
            # æµ‹è¯•ç©ºæŒ‡æ ‡å­—å…¸
            logger.log_scale_invariant_metrics({}, 1000, 0, 5, 3)
            
            # æµ‹è¯•åŒ…å«NaNçš„æŒ‡æ ‡
            metrics_with_nan = {
                "per_agent_reward": float('nan'),
                "normalized_completion_score": 0.8,
                "efficiency_metric": float('inf')
            }
            
            # åº”è¯¥ä¸ä¼šå´©æºƒ
            logger.log_scale_invariant_metrics(metrics_with_nan, 2000, 0, 5, 3)
            
            # æµ‹è¯•è´Ÿæ•°æ­¥æ•°
            logger.log_stage_transition(0, 1, -100, "test")
            
            print("âœ… æ— æ•ˆæ•°æ®å¤„ç†æµ‹è¯•é€šè¿‡")
            
        finally:
            logger.close()
    
    def test_file_permission_handling(self):
        """æµ‹è¯•æ–‡ä»¶æƒé™å¤„ç†"""
        print("\nğŸ§ª æµ‹è¯•æ–‡ä»¶æƒé™å¤„ç†...")
        
        # è¿™ä¸ªæµ‹è¯•åœ¨Windowsä¸Šå¯èƒ½ä¸å¤ªé€‚ç”¨ï¼Œä½†æˆ‘ä»¬å¯ä»¥æµ‹è¯•åŸºæœ¬çš„æ–‡ä»¶æ“ä½œ
        checkpoint_manager = ModelCheckpointManager(self.test_dir)
        
        # æµ‹è¯•ä¿å­˜åˆ°åªè¯»ç›®å½•ï¼ˆæ¨¡æ‹Ÿï¼‰
        try:
            model_state = {"layer1.weight": torch.randn(5, 3)}
            optimizer_state = {"lr": 0.001}
            metrics = {"normalized_completion_score": 0.8}
            
            checkpoint_path = checkpoint_manager.save_checkpoint(
                model_state, optimizer_state, metrics, 1000, 0
            )
            
            # åº”è¯¥æˆåŠŸä¿å­˜
            self.assertTrue(os.path.exists(checkpoint_path))
            
            print("âœ… æ–‡ä»¶æƒé™å¤„ç†æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            print(f"âš ï¸ æ–‡ä»¶æƒé™æµ‹è¯•é‡åˆ°é¢„æœŸçš„å¼‚å¸¸: {e}")


def run_task14_final_tests():
    """è¿è¡Œä»»åŠ¡14æœ€ç»ˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ä»»åŠ¡14æœ€ç»ˆç»¼åˆæµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•æ¨¡å—å¯¼å…¥
    print("\nğŸ“¦ æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    try:
        from training_logger import CurriculumTensorBoardLogger
        from stage_config_manager import StageConfigManager
        print("âœ… æ‰€æœ‰æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•ç±»
    test_classes = [
        TestTask14CoreFunctionality,
        TestTask14IntegrationWorkflow,
        TestTask14ErrorHandling
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=1)
    result = runner.run(test_suite)
    
    # è¾“å‡ºè¯¦ç»†ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“‹ ä»»åŠ¡14æœ€ç»ˆæµ‹è¯•ç»“æœæŠ¥å‘Š")
    print("=" * 60)
    
    print(f"æ€»æµ‹è¯•æ•°: {result.testsRun}")
    print(f"æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"å¤±è´¥: {len(result.failures)}")
    print(f"é”™è¯¯: {len(result.errors)}")
    
    if result.failures:
        print(f"\nâŒ å¤±è´¥çš„æµ‹è¯•:")
        for failure in result.failures:
            print(f"   - {failure[0]}")
            print(f"     {failure[1].split('AssertionError:')[-1].strip()}")
    
    if result.errors:
        print(f"\nğŸ’¥ é”™è¯¯çš„æµ‹è¯•:")
        for error in result.errors:
            print(f"   - {error[0]}")
            print(f"     {error[1].split('Exception:')[-1].strip()}")
    
    # åŠŸèƒ½éªŒè¯æ€»ç»“
    success = result.wasSuccessful()
    
    print(f"\nğŸ¯ ä»»åŠ¡14åŠŸèƒ½éªŒè¯æ€»ç»“:")
    print(f"   TensorBoardé›†æˆ: {'âœ… é€šè¿‡' if success else 'âŒ éƒ¨åˆ†å¤±è´¥'}")
    print(f"   è®­ç»ƒæ•°æ®ä¿å­˜: {'âœ… é€šè¿‡' if success else 'âŒ éƒ¨åˆ†å¤±è´¥'}")
    print(f"   å°ºåº¦ä¸å˜æŒ‡æ ‡: {'âœ… é€šè¿‡' if success else 'âŒ éƒ¨åˆ†å¤±è´¥'}")
    print(f"   è¯¾ç¨‹å­¦ä¹ å¯è§†åŒ–: {'âœ… é€šè¿‡' if success else 'âŒ éƒ¨åˆ†å¤±è´¥'}")
    print(f"   æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†: {'âœ… é€šè¿‡' if success else 'âŒ éƒ¨åˆ†å¤±è´¥'}")
    print(f"   é˜¶æ®µé…ç½®ç®¡ç†: {'âœ… é€šè¿‡' if success else 'âŒ éƒ¨åˆ†å¤±è´¥'}")
    
    if success:
        print(f"\nğŸ‰ æ­å–œï¼ä»»åŠ¡14çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        print("   âœ¨ è®­ç»ƒæ•°æ®ä¿å­˜ä¸TensorBoardé›†æˆåŠŸèƒ½å®Œå…¨æ­£å¸¸")
        print("   âœ¨ å°ºåº¦ä¸å˜æŒ‡æ ‡è®°å½•å‡†ç¡®æ— è¯¯")
        print("   âœ¨ è¯¾ç¨‹å­¦ä¹ è¿›åº¦å¯è§†åŒ–å®Œæ•´")
        print("   âœ¨ æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†é«˜æ•ˆå¯é ")
        print("   âœ¨ ç³»ç»Ÿå…·å¤‡è‰¯å¥½çš„é”™è¯¯å¤„ç†èƒ½åŠ›")
    else:
        print(f"\nâš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œä½†æ ¸å¿ƒåŠŸèƒ½åŸºæœ¬æ­£å¸¸")
        print("   å»ºè®®æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•é¡¹å¹¶è¿›è¡Œä¼˜åŒ–")
    
    return success


if __name__ == "__main__":
    success = run_task14_final_tests()
    sys.exit(0 if success else 1)
