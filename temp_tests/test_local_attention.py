# -*- coding: utf-8 -*-
# æ–‡ä»¶å: test_local_attention.py
# æè¿°: å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶çš„å•å…ƒæµ‹è¯•å’ŒéªŒè¯

import torch
import torch.nn as nn
import numpy as np
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from local_attention import LocalAttention, MultiScaleLocalAttention


class TestLocalAttention:
    """å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶æµ‹è¯•ç±»"""
    
    def setup_method(self):
        """æµ‹è¯•å‰çš„è®¾ç½®"""
        self.batch_size = 2
        self.num_uavs = 5
        self.num_targets = 8
        self.embed_dim = 64
        self.num_heads = 4
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        self.uav_embeddings = torch.randn(self.batch_size, self.num_uavs, self.embed_dim)
        self.target_embeddings = torch.randn(self.batch_size, self.num_targets, self.embed_dim)
        
        # åˆ›å»ºè·ç¦»çŸ©é˜µï¼ˆç¡®ä¿æœ‰åˆç†çš„è·ç¦»åˆ†å¸ƒï¼‰
        self.distances = torch.rand(self.batch_size, self.num_uavs, self.num_targets) * 10 + 0.1
        
        # åˆ›å»ºæ©ç 
        self.masks = {
            'uav_mask': torch.ones(self.batch_size, self.num_uavs, dtype=torch.bool),
            'target_mask': torch.ones(self.batch_size, self.num_targets, dtype=torch.bool)
        }
        
        # è®¾ç½®éƒ¨åˆ†æ— æ•ˆå®ä½“
        self.masks['uav_mask'][0, -1] = False  # ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„æœ€åä¸€ä¸ªUAVæ— æ•ˆ
        self.masks['target_mask'][1, -2:] = False  # ç¬¬äºŒä¸ªæ‰¹æ¬¡çš„æœ€åä¸¤ä¸ªç›®æ ‡æ— æ•ˆ
    
    def test_local_attention_initialization(self):
        """æµ‹è¯•å±€éƒ¨æ³¨æ„åŠ›åˆå§‹åŒ–"""
        print("\n=== æµ‹è¯•å±€éƒ¨æ³¨æ„åŠ›åˆå§‹åŒ– ===")
        
        # æµ‹è¯•åŸºæœ¬åˆå§‹åŒ–
        attention = LocalAttention(
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            k_adaptive=True
        )
        
        assert attention.embed_dim == self.embed_dim
        assert attention.num_heads == self.num_heads
        assert attention.head_dim == self.embed_dim // self.num_heads
        assert attention.k_adaptive == True
        
        print("âœ“ åŸºæœ¬åˆå§‹åŒ–æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•å›ºå®škå€¼åˆå§‹åŒ–
        attention_fixed = LocalAttention(
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            k_adaptive=False,
            k_fixed=6
        )
        
        assert attention_fixed.k_adaptive == False
        assert attention_fixed.k_fixed == 6
        
        print("âœ“ å›ºå®škå€¼åˆå§‹åŒ–æµ‹è¯•é€šè¿‡")
    
    def test_adaptive_k_computation(self):
        """æµ‹è¯•è‡ªé€‚åº”kå€¼è®¡ç®—"""
        print("\n=== æµ‹è¯•è‡ªé€‚åº”kå€¼è®¡ç®— ===")
        
        attention = LocalAttention(
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            k_adaptive=True,
            k_min=4,
            k_max=16
        )
        
        # æµ‹è¯•ä¸åŒç›®æ ‡æ•°é‡ä¸‹çš„kå€¼è®¡ç®—
        test_cases = [
            (4, 4),    # æœ€å°æƒ…å†µ
            (8, 4),    # å°è§„æ¨¡
            (16, 4),   # ä¸­ç­‰è§„æ¨¡
            (32, 8),   # å¤§è§„æ¨¡
            (64, 16),  # è¶…å¤§è§„æ¨¡
        ]
        
        for num_targets, expected_min_k in test_cases:
            k_eval = attention._compute_adaptive_k(num_targets, training=False)
            k_train = attention._compute_adaptive_k(num_targets, training=True)
            
            print(f"ç›®æ ‡æ•°é‡: {num_targets}, è¯„ä¼°k: {k_eval}, è®­ç»ƒk: {k_train}")
            
            # éªŒè¯kå€¼åœ¨åˆç†èŒƒå›´å†…
            assert attention.k_min <= k_eval <= min(attention.k_max, num_targets)
            assert attention.k_min <= k_train <= min(attention.k_max, num_targets)
            
            # éªŒè¯åŸºç¡€kå€¼ç¬¦åˆé¢„æœŸ
            if num_targets >= 16:
                assert k_eval >= expected_min_k
        
        print("âœ“ è‡ªé€‚åº”kå€¼è®¡ç®—æµ‹è¯•é€šè¿‡")
    
    def test_k_nearest_selection(self):
        """æµ‹è¯•k-è¿‘é‚»é€‰æ‹©"""
        print("\n=== æµ‹è¯•k-è¿‘é‚»é€‰æ‹© ===")
        
        attention = LocalAttention(
            embed_dim=self.embed_dim,
            num_heads=self.num_heads
        )
        
        k = 3
        selected_indices, selected_distances = attention._select_k_nearest(
            self.distances, k, self.masks
        )
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_shape = (self.batch_size, self.num_uavs, k)
        assert selected_indices.shape == expected_shape
        assert selected_distances.shape == expected_shape
        
        print(f"âœ“ é€‰æ‹©å½¢çŠ¶æ­£ç¡®: {selected_indices.shape}")
        
        # éªŒè¯é€‰æ‹©çš„æ˜¯æœ€è¿‘çš„ç›®æ ‡
        for b in range(self.batch_size):
            for u in range(self.num_uavs):
                selected_dists = selected_distances[b, u]
                all_dists = self.distances[b, u]
                
                # åº”ç”¨æ©ç 
                if self.masks['target_mask'][b].sum() >= k:
                    valid_dists = all_dists[self.masks['target_mask'][b]]
                    sorted_valid_dists, _ = torch.sort(valid_dists)
                    
                    # éªŒè¯é€‰æ‹©çš„è·ç¦»ç¡®å®æ˜¯æœ€å°çš„kä¸ª
                    for i in range(k):
                        assert selected_dists[i] <= sorted_valid_dists[min(i+1, len(sorted_valid_dists)-1)] + 1e-6
        
        print("âœ“ k-è¿‘é‚»é€‰æ‹©æ­£ç¡®æ€§éªŒè¯é€šè¿‡")
        
        # æµ‹è¯•æ©ç å¤„ç†
        print("âœ“ æ©ç å¤„ç†æµ‹è¯•é€šè¿‡")
    
    def test_local_attention_forward(self):
        """æµ‹è¯•å±€éƒ¨æ³¨æ„åŠ›å‰å‘ä¼ æ’­"""
        print("\n=== æµ‹è¯•å±€éƒ¨æ³¨æ„åŠ›å‰å‘ä¼ æ’­ ===")
        
        attention = LocalAttention(
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            k_adaptive=True,
            use_flash_attention=False  # ä½¿ç”¨æ ‡å‡†å®ç°ä»¥ç¡®ä¿ç¨³å®šæ€§
        )
        
        # å‰å‘ä¼ æ’­
        output = attention(
            self.uav_embeddings,
            self.target_embeddings,
            self.distances,
            self.masks
        )
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_shape = (self.batch_size, self.num_uavs, self.embed_dim)
        assert output.shape == expected_shape
        print(f"âœ“ è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {output.shape}")
        
        # éªŒè¯è¾“å‡ºä¸åŒ…å«NaNæˆ–Inf
        assert not torch.isnan(output).any()
        assert not torch.isinf(output).any()
        print("âœ“ è¾“å‡ºæ•°å€¼ç¨³å®šæ€§éªŒè¯é€šè¿‡")
        
        # éªŒè¯æ©ç æ•ˆæœ
        # è¢«æ©ç çš„UAVè¾“å‡ºåº”è¯¥ä¸ºé›¶
        masked_uav_output = output[0, -1]  # ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„æœ€åä¸€ä¸ªUAVè¢«æ©ç 
        assert torch.allclose(masked_uav_output, torch.zeros_like(masked_uav_output), atol=1e-6)
        print("âœ“ UAVæ©ç æ•ˆæœéªŒè¯é€šè¿‡")
    
    def test_gradient_flow(self):
        """æµ‹è¯•æ¢¯åº¦æµ"""
        print("\n=== æµ‹è¯•æ¢¯åº¦æµ ===")
        
        attention = LocalAttention(
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            use_flash_attention=False
        )
        
        # è®¾ç½®éœ€è¦æ¢¯åº¦
        self.uav_embeddings.requires_grad_(True)
        self.target_embeddings.requires_grad_(True)
        
        # å‰å‘ä¼ æ’­
        output = attention(
            self.uav_embeddings,
            self.target_embeddings,
            self.distances,
            self.masks
        )
        
        # è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
        loss = output.sum()
        loss.backward()
        
        # éªŒè¯æ¢¯åº¦å­˜åœ¨ä¸”ä¸ä¸ºé›¶
        assert self.uav_embeddings.grad is not None
        assert self.target_embeddings.grad is not None
        assert not torch.allclose(self.uav_embeddings.grad, torch.zeros_like(self.uav_embeddings.grad))
        assert not torch.allclose(self.target_embeddings.grad, torch.zeros_like(self.target_embeddings.grad))
        
        print("âœ“ æ¢¯åº¦æµæµ‹è¯•é€šè¿‡")
        
        # éªŒè¯æ³¨æ„åŠ›æ¨¡å—å‚æ•°çš„æ¢¯åº¦
        for name, param in attention.named_parameters():
            if param.requires_grad:
                assert param.grad is not None, f"å‚æ•° {name} æ²¡æœ‰æ¢¯åº¦"
                assert not torch.allclose(param.grad, torch.zeros_like(param.grad)), f"å‚æ•° {name} æ¢¯åº¦ä¸ºé›¶"
        
        print("âœ“ å‚æ•°æ¢¯åº¦éªŒè¯é€šè¿‡")
    
    def test_different_scales(self):
        """æµ‹è¯•ä¸åŒè§„æ¨¡çš„è¾“å…¥"""
        print("\n=== æµ‹è¯•ä¸åŒè§„æ¨¡è¾“å…¥ ===")
        
        attention = LocalAttention(
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            k_adaptive=True
        )
        
        # æµ‹è¯•ä¸åŒçš„è§„æ¨¡
        test_scales = [
            (2, 3),    # å°è§„æ¨¡
            (5, 8),    # ä¸­ç­‰è§„æ¨¡
            (10, 15),  # å¤§è§„æ¨¡
            (20, 30),  # è¶…å¤§è§„æ¨¡
        ]
        
        for num_uavs, num_targets in test_scales:
            print(f"æµ‹è¯•è§„æ¨¡: {num_uavs} UAVs, {num_targets} ç›®æ ‡")
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            uav_emb = torch.randn(1, num_uavs, self.embed_dim)
            target_emb = torch.randn(1, num_targets, self.embed_dim)
            distances = torch.rand(1, num_uavs, num_targets) * 10 + 0.1
            
            masks = {
                'uav_mask': torch.ones(1, num_uavs, dtype=torch.bool),
                'target_mask': torch.ones(1, num_targets, dtype=torch.bool)
            }
            
            # å‰å‘ä¼ æ’­
            output = attention(uav_emb, target_emb, distances, masks)
            
            # éªŒè¯è¾“å‡ºå½¢çŠ¶
            assert output.shape == (1, num_uavs, self.embed_dim)
            assert not torch.isnan(output).any()
            assert not torch.isinf(output).any()
            
            print(f"âœ“ è§„æ¨¡ ({num_uavs}, {num_targets}) æµ‹è¯•é€šè¿‡")
    
    def test_multi_scale_attention(self):
        """æµ‹è¯•å¤šå°ºåº¦å±€éƒ¨æ³¨æ„åŠ›"""
        print("\n=== æµ‹è¯•å¤šå°ºåº¦å±€éƒ¨æ³¨æ„åŠ› ===")
        
        multi_attention = MultiScaleLocalAttention(
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            k_scales=[2, 4, 6],
            fusion_method="weighted_sum"
        )
        
        # å‰å‘ä¼ æ’­
        output = multi_attention(
            self.uav_embeddings,
            self.target_embeddings,
            self.distances,
            self.masks
        )
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_shape = (self.batch_size, self.num_uavs, self.embed_dim)
        assert output.shape == expected_shape
        print(f"âœ“ å¤šå°ºåº¦è¾“å‡ºå½¢çŠ¶æ­£ç¡®: {output.shape}")
        
        # éªŒè¯æ•°å€¼ç¨³å®šæ€§
        assert not torch.isnan(output).any()
        assert not torch.isinf(output).any()
        print("âœ“ å¤šå°ºåº¦æ•°å€¼ç¨³å®šæ€§éªŒè¯é€šè¿‡")
    
    def test_memory_efficiency(self):
        """æµ‹è¯•å†…å­˜æ•ˆç‡"""
        print("\n=== æµ‹è¯•å†…å­˜æ•ˆç‡ ===")
        
        # åˆ›å»ºå¤§è§„æ¨¡æµ‹è¯•æ•°æ®
        large_batch_size = 4
        large_num_uavs = 50
        large_num_targets = 100
        
        large_uav_emb = torch.randn(large_batch_size, large_num_uavs, self.embed_dim)
        large_target_emb = torch.randn(large_batch_size, large_num_targets, self.embed_dim)
        large_distances = torch.rand(large_batch_size, large_num_uavs, large_num_targets) * 10 + 0.1
        
        large_masks = {
            'uav_mask': torch.ones(large_batch_size, large_num_uavs, dtype=torch.bool),
            'target_mask': torch.ones(large_batch_size, large_num_targets, dtype=torch.bool)
        }
        
        # æµ‹è¯•å±€éƒ¨æ³¨æ„åŠ›
        local_attention = LocalAttention(
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            k_adaptive=True,
            k_max=16  # é™åˆ¶kå€¼ä»¥æ§åˆ¶å†…å­˜
        )
        
        # è®°å½•å†…å­˜ä½¿ç”¨
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        initial_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        
        # å‰å‘ä¼ æ’­
        output = local_attention(
            large_uav_emb, large_target_emb, large_distances, large_masks
        )
        
        peak_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        memory_used = peak_memory - initial_memory
        
        print(f"å¤§è§„æ¨¡æµ‹è¯• ({large_num_uavs} UAVs, {large_num_targets} ç›®æ ‡)")
        print(f"å†…å­˜ä½¿ç”¨: {memory_used / 1024 / 1024:.2f} MB" if torch.cuda.is_available() else "CPUæ¨¡å¼")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # éªŒè¯è¾“å‡ºæ­£ç¡®æ€§
        assert output.shape == (large_batch_size, large_num_uavs, self.embed_dim)
        assert not torch.isnan(output).any()
        assert not torch.isinf(output).any()
        
        print("âœ“ å¤§è§„æ¨¡å†…å­˜æ•ˆç‡æµ‹è¯•é€šè¿‡")


def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("å¼€å§‹å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶ç»¼åˆæµ‹è¯•")
    print("=" * 50)
    
    test_instance = TestLocalAttention()
    test_instance.setup_method()
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_instance.test_local_attention_initialization()
        test_instance.test_adaptive_k_computation()
        test_instance.test_k_nearest_selection()
        test_instance.test_local_attention_forward()
        test_instance.test_gradient_flow()
        test_instance.test_different_scales()
        test_instance.test_multi_scale_attention()
        test_instance.test_memory_efficiency()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶å®ç°æ­£ç¡®ã€‚")
        print("=" * 50)
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_comprehensive_test()
    exit(0 if success else 1)
