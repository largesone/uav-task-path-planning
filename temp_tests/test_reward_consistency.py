# -*- coding: utf-8 -*-
"""
æµ‹è¯•Per-Agentå¥–åŠ±å½’ä¸€åŒ–çš„ä¸€è‡´æ€§
éªŒè¯ä¸åŒæ— äººæœºæ•°é‡ä¸‹çš„å¥–åŠ±ä¸€è‡´æ€§
"""

import numpy as np
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from entities import UAV, Target
from environment import DirectedGraph, UAVTaskEnv

class MockConfig:
    """æ¨¡æ‹Ÿé…ç½®ç±»ï¼Œç”¨äºæµ‹è¯•"""
    def __init__(self):
        self.GRAPH_N_PHI = 8
        self.MAP_SIZE = 1000.0
        self.USE_PHRRT_DURING_TRAINING = False
        self.ENABLE_REWARD_LOGGING = False
        self.UAV_COMM_FAILURE_RATE = 0.0
        self.UAV_SENSING_FAILURE_RATE = 0.0

def create_test_scenario_with_congestion(n_uavs: int):
    """åˆ›å»ºå¸¦æ‹¥å µçš„æµ‹è¯•åœºæ™¯"""
    config = MockConfig()
    
    # åˆ›å»ºUAVåˆ—è¡¨
    uavs = []
    for i in range(n_uavs):
        uav = UAV(
            id=i,
            position=(100 + i * 20, 100 + i * 10),  # è®©UAVæ›´æ¥è¿‘ï¼Œå¢åŠ æ‹¥å µ
            heading=0.0,
            resources=np.array([50.0, 40.0]),
            max_distance=1000,
            velocity_range=(30, 100),
            economic_speed=60.0
        )
        uavs.append(uav)
    
    # åˆ›å»ºå•ä¸ªç›®æ ‡
    target = Target(
        id=0,
        position=(500, 300),
        resources=np.array([200.0, 150.0]),  # å¤§èµ„æºéœ€æ±‚ï¼Œéœ€è¦å¤šä¸ªUAV
        value=100.0
    )
    targets = [target]
    
    # æ¨¡æ‹Ÿå¤šä¸ªUAVå·²åˆ†é…åˆ°åŒä¸€ç›®æ ‡ï¼ˆåˆ¶é€ æ‹¥å µï¼‰
    if n_uavs >= 4:
        target.allocated_uavs = [(0, 0), (1, 1), (2, 2), (3, 3)]
    elif n_uavs >= 2:
        target.allocated_uavs = [(0, 0), (1, 1)]
    
    # åˆ›å»ºå›¾
    obstacles = []
    graph = DirectedGraph(uavs, targets, config.GRAPH_N_PHI, obstacles, config)
    
    return uavs, targets, graph, obstacles, config

def test_reward_consistency():
    """æµ‹è¯•å¥–åŠ±ä¸€è‡´æ€§"""
    print("=== å¥–åŠ±ä¸€è‡´æ€§æµ‹è¯• ===")
    
    # æµ‹è¯•ä¸åŒçš„UAVæ•°é‡
    uav_counts = [3, 6, 9, 12]
    rewards = []
    
    # å›ºå®šçš„æµ‹è¯•å‚æ•°
    test_params = {
        'actual_contribution': np.array([25.0, 20.0]),
        'path_len': 400.0,
        'travel_time': 10.0,
        'was_satisfied': False,
        'done': False
    }
    
    print("æµ‹è¯•å‚æ•°:")
    print(f"  è´¡çŒ®: {test_params['actual_contribution']}")
    print(f"  è·¯å¾„é•¿åº¦: {test_params['path_len']}")
    print(f"  æ—…è¡Œæ—¶é—´: {test_params['travel_time']}")
    print()
    
    for n_uavs in uav_counts:
        uavs, targets, graph, obstacles, config = create_test_scenario_with_congestion(n_uavs)
        env = UAVTaskEnv(uavs, targets, graph, obstacles, config)
        
        # è®¡ç®—å¥–åŠ±
        reward = env._calculate_reward(
            targets[0], uavs[0], 
            test_params['actual_contribution'],
            test_params['path_len'],
            test_params['was_satisfied'],
            test_params['travel_time'],
            test_params['done']
        )
        
        rewards.append(reward)
        
        # è·å–å¥–åŠ±ç»„ä»¶ä¿¡æ¯
        components = env.last_reward_components
        norm_info = components['per_agent_normalization']
        
        print(f"UAVæ•°é‡: {n_uavs:2d}")
        print(f"  æœ‰æ•ˆUAV: {norm_info['n_active_uavs']}")
        print(f"  å¥–åŠ±: {reward:.4f}")
        print(f"  å½’ä¸€åŒ–ç»„ä»¶: {norm_info['components_normalized']}")
        
        # æ˜¾ç¤ºå½’ä¸€åŒ–å½±å“
        impact = norm_info['normalization_impact']
        if impact['normalization_savings'] > 0:
            print(f"  å½’ä¸€åŒ–èŠ‚çœ: {impact['normalization_savings']:.4f}")
        
        print()
    
    # åˆ†æå¥–åŠ±ä¸€è‡´æ€§
    rewards = np.array(rewards)
    mean_reward = np.mean(rewards)
    std_reward = np.std(rewards)
    cv = std_reward / abs(mean_reward) if mean_reward != 0 else float('inf')
    
    print("å¥–åŠ±ä¸€è‡´æ€§åˆ†æ:")
    print(f"  å¥–åŠ±èŒƒå›´: {np.min(rewards):.4f} ~ {np.max(rewards):.4f}")
    print(f"  å¹³å‡å¥–åŠ±: {mean_reward:.4f}")
    print(f"  æ ‡å‡†å·®: {std_reward:.4f}")
    print(f"  å˜å¼‚ç³»æ•°: {cv:.4f}")
    
    # éªŒè¯ä¸€è‡´æ€§
    cv_threshold = 0.15  # 15%çš„å˜å¼‚ç³»æ•°é˜ˆå€¼
    if cv < cv_threshold:
        print(f"âœ“ å¥–åŠ±ä¸€è‡´æ€§è‰¯å¥½ (CV={cv:.4f} < {cv_threshold})")
    else:
        print(f"âš  å¥–åŠ±ä¸€è‡´æ€§éœ€è¦æ”¹è¿› (CV={cv:.4f} >= {cv_threshold})")
    
    return cv < cv_threshold

def test_normalization_effectiveness():
    """æµ‹è¯•å½’ä¸€åŒ–æ•ˆæœ"""
    print("\n=== å½’ä¸€åŒ–æ•ˆæœæµ‹è¯• ===")
    
    # åˆ›å»ºé«˜æ‹¥å µåœºæ™¯
    n_uavs = 8
    uavs, targets, graph, obstacles, config = create_test_scenario_with_congestion(n_uavs)
    env = UAVTaskEnv(uavs, targets, graph, obstacles, config)
    
    # å¢åŠ æ›´å¤šæ‹¥å µ
    targets[0].allocated_uavs = [(i, i % 8) for i in range(6)]  # 6ä¸ªUAVåˆ†é…åˆ°åŒä¸€ç›®æ ‡
    
    # è®¡ç®—å¥–åŠ±
    reward = env._calculate_reward(
        targets[0], uavs[0],
        np.array([30.0, 25.0]),
        500.0, False, 12.0, False
    )
    
    components = env.last_reward_components
    norm_info = components['per_agent_normalization']
    impact = norm_info['normalization_impact']
    
    print(f"é«˜æ‹¥å µåœºæ™¯ (UAVæ•°é‡: {n_uavs}):")
    print(f"  åˆ†é…åˆ°ç›®æ ‡çš„UAVæ•°é‡: {len(targets[0].allocated_uavs)}")
    print(f"  æœ‰æ•ˆUAVæ•°é‡: {norm_info['n_active_uavs']}")
    print(f"  æœ€ç»ˆå¥–åŠ±: {reward:.4f}")
    print(f"  å½’ä¸€åŒ–ç»„ä»¶: {norm_info['components_normalized']}")
    print(f"  å½’ä¸€åŒ–èŠ‚çœ: {impact['normalization_savings']:.4f}")
    
    # æ˜¾ç¤ºå„ç»„ä»¶çš„å½’ä¸€åŒ–æ•ˆæœ
    for component, details in impact['components_impact'].items():
        print(f"  {component}:")
        print(f"    åŸå§‹å€¼: {details['raw']:.4f}")
        print(f"    å½’ä¸€åŒ–å€¼: {details['normalized']:.4f}")
        print(f"    å‡å°‘é‡: {details['reduction']:.4f}")
    
    # éªŒè¯å½’ä¸€åŒ–ç¡®å®äº§ç”Ÿäº†æ•ˆæœ
    has_normalization_effect = impact['normalization_savings'] > 0
    if has_normalization_effect:
        print("âœ“ å½’ä¸€åŒ–äº§ç”Ÿäº†é¢„æœŸæ•ˆæœ")
    else:
        print("âš  å½’ä¸€åŒ–æ•ˆæœä¸æ˜æ˜¾")
    
    return has_normalization_effect

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹Per-Agentå¥–åŠ±å½’ä¸€åŒ–ä¸€è‡´æ€§æµ‹è¯•...\n")
    
    try:
        # æµ‹è¯•å¥–åŠ±ä¸€è‡´æ€§
        consistency_ok = test_reward_consistency()
        
        # æµ‹è¯•å½’ä¸€åŒ–æ•ˆæœ
        normalization_ok = test_normalization_effectiveness()
        
        print("\n" + "="*50)
        print("æµ‹è¯•æ€»ç»“:")
        print(f"âœ“ å¥–åŠ±ä¸€è‡´æ€§: {'é€šè¿‡' if consistency_ok else 'éœ€è¦æ”¹è¿›'}")
        print(f"âœ“ å½’ä¸€åŒ–æ•ˆæœ: {'æœ‰æ•ˆ' if normalization_ok else 'éœ€è¦æ”¹è¿›'}")
        
        if consistency_ok and normalization_ok:
            print("\nğŸ‰ Per-Agentå¥–åŠ±å½’ä¸€åŒ–åŠŸèƒ½éªŒè¯é€šè¿‡ï¼")
            return True
        else:
            print("\nâš  éƒ¨åˆ†æµ‹è¯•éœ€è¦æ”¹è¿›")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)