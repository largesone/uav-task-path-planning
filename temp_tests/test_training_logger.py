"""
è®­ç»ƒæ—¥å¿—è®°å½•å™¨æµ‹è¯•
æµ‹è¯•TensorBoardé›†æˆã€æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜å’Œè¯¾ç¨‹å­¦ä¹ å›è°ƒåŠŸèƒ½
"""

import os
import sys
import unittest
import tempfile
import shutil
import torch
import numpy as np
from unittest.mock import Mock, patch, MagicMock
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from training_logger import (
    CurriculumTensorBoardLogger, 
    ModelCheckpointManager, 
    CurriculumTrainingCallbacks,
    create_training_config_with_logging
)


class TestCurriculumTensorBoardLogger(unittest.TestCase):
    """æµ‹è¯•è¯¾ç¨‹å­¦ä¹ TensorBoardæ—¥å¿—è®°å½•å™¨"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.test_dir = tempfile.mkdtemp()
        self.logger = CurriculumTensorBoardLogger(self.test_dir, "test_experiment")
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        self.logger.close()
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def test_initialization(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        self.assertTrue(Path(self.test_dir).exists())
        self.assertEqual(self.logger.experiment_name, "test_experiment")
        self.assertEqual(self.logger.current_stage, 0)
        self.assertIsNotNone(self.logger.writer)
    
    def test_log_stage_transition(self):
        """æµ‹è¯•é˜¶æ®µåˆ‡æ¢è®°å½•"""
        self.logger.log_stage_transition(0, 1, 1000, "performance_threshold")
        
        # æ£€æŸ¥è®°å½•æ˜¯å¦æ­£ç¡®ä¿å­˜
        self.assertEqual(len(self.logger.training_history["stage_transitions"]), 1)
        transition = self.logger.training_history["stage_transitions"][0]
        
        self.assertEqual(transition["from_stage"], 0)
        self.assertEqual(transition["to_stage"], 1)
        self.assertEqual(transition["step"], 1000)
        self.assertEqual(transition["reason"], "performance_threshold")
        self.assertEqual(self.logger.current_stage, 1)
    
    def test_log_rollback_event(self):
        """æµ‹è¯•å›é€€äº‹ä»¶è®°å½•"""
        self.logger.log_rollback_event(2, 5000, 0.15, 0.1)
        
        # æ£€æŸ¥å›é€€äº‹ä»¶è®°å½•
        self.assertEqual(len(self.logger.training_history["rollback_events"]), 1)
        rollback = self.logger.training_history["rollback_events"][0]
        
        self.assertEqual(rollback["stage"], 2)
        self.assertEqual(rollback["step"], 5000)
        self.assertEqual(rollback["performance_drop"], 0.15)
        self.assertEqual(rollback["threshold"], 0.1)
    
    def test_log_scale_invariant_metrics(self):
        """æµ‹è¯•å°ºåº¦ä¸å˜æŒ‡æ ‡è®°å½•"""
        metrics = {
            "per_agent_reward": 12.5,
            "normalized_completion_score": 0.85,
            "efficiency_metric": 0.42
        }
        
        self.logger.log_scale_invariant_metrics(metrics, 2000, 1, 5, 3)
        
        # æ£€æŸ¥æŒ‡æ ‡è®°å½•
        self.assertEqual(len(self.logger.training_history["metrics"]), 1)
        metric_record = self.logger.training_history["metrics"][0]
        
        self.assertEqual(metric_record["step"], 2000)
        self.assertEqual(metric_record["stage"], 1)
        self.assertEqual(metric_record["n_uavs"], 5)
        self.assertEqual(metric_record["n_targets"], 3)
        self.assertEqual(metric_record["per_agent_reward"], 12.5)
    
    def test_save_training_history(self):
        """æµ‹è¯•è®­ç»ƒå†å²ä¿å­˜"""
        # æ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®
        self.logger.log_stage_transition(0, 1, 1000)
        self.logger.log_scale_invariant_metrics({"per_agent_reward": 10.0}, 1500, 1, 4, 2)
        
        # ä¿å­˜å†å²
        self.logger.save_training_history()
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        history_file = Path(self.test_dir) / "test_experiment_history.json"
        self.assertTrue(history_file.exists())
        
        # æ£€æŸ¥æ–‡ä»¶å†…å®¹
        import json
        with open(history_file, 'r', encoding='utf-8') as f:
            saved_history = json.load(f)
        
        self.assertEqual(len(saved_history["stage_transitions"]), 1)
        self.assertEqual(len(saved_history["metrics"]), 1)


class TestModelCheckpointManager(unittest.TestCase):
    """æµ‹è¯•æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.test_dir = tempfile.mkdtemp()
        self.manager = ModelCheckpointManager(self.test_dir, max_checkpoints=3)
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def test_initialization(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        self.assertTrue(Path(self.test_dir).exists())
        self.assertEqual(self.manager.max_checkpoints, 3)
        self.assertEqual(len(self.manager.checkpoint_history), 0)
    
    def test_save_checkpoint(self):
        """æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜"""
        # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹çŠ¶æ€
        model_state = {"layer1.weight": torch.randn(10, 5)}
        optimizer_state = {"lr": 0.001}
        metrics = {"normalized_completion_score": 0.85}
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint_path = self.manager.save_checkpoint(
            model_state, optimizer_state, metrics, 1000, 1, is_best=True
        )
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        self.assertTrue(os.path.exists(checkpoint_path))
        
        # æ£€æŸ¥å†å²è®°å½•
        self.assertEqual(len(self.manager.checkpoint_history), 1)
        checkpoint_info = self.manager.checkpoint_history[0]
        
        self.assertEqual(checkpoint_info["step"], 1000)
        self.assertEqual(checkpoint_info["stage"], 1)
        self.assertTrue(checkpoint_info["is_best"])
        self.assertEqual(checkpoint_info["metrics"]["normalized_completion_score"], 0.85)
    
    def test_load_checkpoint(self):
        """æµ‹è¯•æ£€æŸ¥ç‚¹åŠ è½½"""
        # å…ˆä¿å­˜ä¸€ä¸ªæ£€æŸ¥ç‚¹
        model_state = {"layer1.weight": torch.randn(10, 5)}
        optimizer_state = {"lr": 0.001}
        metrics = {"normalized_completion_score": 0.85}
        
        checkpoint_path = self.manager.save_checkpoint(
            model_state, optimizer_state, metrics, 1000, 1
        )
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        loaded_data = self.manager.load_checkpoint(checkpoint_path)
        
        # éªŒè¯åŠ è½½çš„æ•°æ®
        self.assertIn("model_state_dict", loaded_data)
        self.assertIn("optimizer_state_dict", loaded_data)
        self.assertIn("metrics", loaded_data)
        self.assertEqual(loaded_data["step"], 1000)
        self.assertEqual(loaded_data["stage"], 1)
    
    def test_get_best_checkpoint(self):
        """æµ‹è¯•è·å–æœ€ä½³æ£€æŸ¥ç‚¹"""
        # ä¿å­˜å¤šä¸ªæ£€æŸ¥ç‚¹
        for i, score in enumerate([0.7, 0.85, 0.8]):
            model_state = {"layer1.weight": torch.randn(10, 5)}
            optimizer_state = {"lr": 0.001}
            metrics = {"normalized_completion_score": score}
            
            self.manager.save_checkpoint(
                model_state, optimizer_state, metrics, 
                i*1000, 1, is_best=(score == 0.85)
            )
        
        # è·å–æœ€ä½³æ£€æŸ¥ç‚¹
        best_path = self.manager.get_best_checkpoint()
        self.assertIsNotNone(best_path)
        
        # éªŒè¯æ˜¯æœ€ä½³çš„
        loaded_data = self.manager.load_checkpoint(best_path)
        self.assertEqual(loaded_data["metrics"]["normalized_completion_score"], 0.85)
    
    def test_cleanup_old_checkpoints(self):
        """æµ‹è¯•æ—§æ£€æŸ¥ç‚¹æ¸…ç†"""
        # ä¿å­˜è¶…è¿‡æœ€å¤§æ•°é‡çš„æ£€æŸ¥ç‚¹
        for i in range(5):
            model_state = {"layer1.weight": torch.randn(10, 5)}
            optimizer_state = {"lr": 0.001}
            metrics = {"normalized_completion_score": 0.7 + i * 0.05}
            
            self.manager.save_checkpoint(
                model_state, optimizer_state, metrics, 
                i*1000, 1, is_best=(i == 4)
            )
        
        # æ£€æŸ¥æ˜¯å¦æ­£ç¡®æ¸…ç†ï¼ˆä¿ç•™æœ€ä½³ + æœ€è¿‘çš„3ä¸ªï¼‰
        non_best_count = len([cp for cp in self.manager.checkpoint_history if not cp["is_best"]])
        self.assertLessEqual(non_best_count, self.manager.max_checkpoints)


class TestCurriculumTrainingCallbacks(unittest.TestCase):
    """æµ‹è¯•è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå›è°ƒ"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.test_dir = tempfile.mkdtemp()
        self.callbacks = CurriculumTrainingCallbacks()
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        if self.callbacks.logger:
            self.callbacks.logger.close()
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    @patch('training_logger.CurriculumTensorBoardLogger')
    @patch('training_logger.ModelCheckpointManager')
    def test_on_algorithm_init(self, mock_checkpoint_manager, mock_logger):
        """æµ‹è¯•ç®—æ³•åˆå§‹åŒ–å›è°ƒ"""
        # åˆ›å»ºæ¨¡æ‹Ÿç®—æ³•
        mock_algorithm = Mock()
        mock_algorithm.config = {
            "log_dir": self.test_dir,
            "experiment_name": "test_experiment"
        }
        
        # è°ƒç”¨åˆå§‹åŒ–å›è°ƒ
        self.callbacks.on_algorithm_init(algorithm=mock_algorithm)
        
        # éªŒè¯æ—¥å¿—è®°å½•å™¨å’Œæ£€æŸ¥ç‚¹ç®¡ç†å™¨è¢«åˆ›å»º
        mock_logger.assert_called_once_with(self.test_dir, "test_experiment")
        mock_checkpoint_manager.assert_called_once()
    
    def test_calculate_scale_invariant_metrics(self):
        """æµ‹è¯•å°ºåº¦ä¸å˜æŒ‡æ ‡è®¡ç®—"""
        # åˆ›å»ºæ¨¡æ‹Ÿç¯å¢ƒå’Œå›åˆ
        mock_env = Mock()
        mock_env.n_active_uavs = 5
        mock_env.n_targets = 3
        mock_env.completed_targets = 2
        mock_env.total_flight_distance = 100.0
        mock_env.average_congestion = 0.1
        
        mock_episode = Mock()
        mock_episode.total_reward = 50.0
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self.callbacks._calculate_scale_invariant_metrics(mock_episode, mock_env)
        
        # éªŒè¯è®¡ç®—ç»“æœ
        expected_per_agent_reward = 50.0 / 5  # 10.0
        expected_completion_rate = 2 / 3  # 0.667
        expected_normalized_score = expected_completion_rate * (1 - 0.1)  # 0.6
        expected_efficiency = 2 / 100.0  # 0.02
        
        self.assertAlmostEqual(metrics["per_agent_reward"], expected_per_agent_reward)
        self.assertAlmostEqual(metrics["normalized_completion_score"], expected_normalized_score, places=3)
        self.assertAlmostEqual(metrics["efficiency_metric"], expected_efficiency)
    
    def test_is_best_model(self):
        """æµ‹è¯•æœ€ä½³æ¨¡å‹åˆ¤æ–­"""
        # ç¬¬ä¸€æ¬¡è°ƒç”¨åº”è¯¥è¿”å›Trueï¼ˆæ²¡æœ‰å†å²è®°å½•ï¼‰
        current_metrics = {"normalized_completion_score": 0.8}
        self.assertTrue(self.callbacks._is_best_model(current_metrics, 0))
        
        # æ›´å¥½çš„æ€§èƒ½åº”è¯¥è¿”å›True
        better_metrics = {"normalized_completion_score": 0.9}
        self.assertTrue(self.callbacks._is_best_model(better_metrics, 0))
        
        # æ›´å·®çš„æ€§èƒ½åº”è¯¥è¿”å›False
        worse_metrics = {"normalized_completion_score": 0.7}
        self.assertFalse(self.callbacks._is_best_model(worse_metrics, 0))


class TestTrainingConfigCreation(unittest.TestCase):
    """æµ‹è¯•è®­ç»ƒé…ç½®åˆ›å»º"""
    
    def test_create_training_config_with_logging(self):
        """æµ‹è¯•åˆ›å»ºåŒ…å«æ—¥å¿—è®°å½•çš„è®­ç»ƒé…ç½®"""
        base_config = {
            "env": "test_env",
            "num_workers": 4,
            "lr": 0.001
        }
        
        enhanced_config = create_training_config_with_logging(
            base_config, 
            log_dir="./test_logs",
            experiment_name="test_experiment"
        )
        
        # éªŒè¯åŸºç¡€é…ç½®ä¿ç•™
        self.assertEqual(enhanced_config["env"], "test_env")
        self.assertEqual(enhanced_config["num_workers"], 4)
        self.assertEqual(enhanced_config["lr"], 0.001)
        
        # éªŒè¯æ—¥å¿—é…ç½®æ·»åŠ 
        self.assertEqual(enhanced_config["log_dir"], "./test_logs")
        self.assertEqual(enhanced_config["experiment_name"], "test_experiment")
        self.assertEqual(enhanced_config["callbacks"], CurriculumTrainingCallbacks)
        
        # éªŒè¯TensorBoardé…ç½®
        self.assertIn("logger_config", enhanced_config)
        self.assertIn("checkpoint_freq", enhanced_config)


class TestIntegrationScenarios(unittest.TestCase):
    """é›†æˆæµ‹è¯•åœºæ™¯"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.test_dir = tempfile.mkdtemp()
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def test_complete_training_logging_workflow(self):
        """æµ‹è¯•å®Œæ•´çš„è®­ç»ƒæ—¥å¿—è®°å½•å·¥ä½œæµ"""
        # 1. åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        logger = CurriculumTensorBoardLogger(self.test_dir, "integration_test")
        
        # 2. åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
        checkpoint_manager = ModelCheckpointManager(
            os.path.join(self.test_dir, "checkpoints")
        )
        
        # 3. æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        for step in range(0, 1000, 100):
            stage = step // 300
            
            # è®°å½•æŒ‡æ ‡
            metrics = {
                "per_agent_reward": 10 + np.random.normal(0, 1),
                "normalized_completion_score": 0.7 + np.random.uniform(-0.1, 0.2),
                "efficiency_metric": 0.3 + np.random.uniform(-0.05, 0.1)
            }
            
            logger.log_scale_invariant_metrics(metrics, step, stage, 5, 3)
            
            # æ¨¡æ‹Ÿé˜¶æ®µåˆ‡æ¢
            if step in [300, 600, 900]:
                logger.log_stage_transition(stage-1, stage, step)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if step % 200 == 0:
                model_state = {"layer1.weight": torch.randn(10, 5)}
                optimizer_state = {"lr": 0.001}
                
                is_best = metrics["normalized_completion_score"] > 0.8
                checkpoint_manager.save_checkpoint(
                    model_state, optimizer_state, metrics, 
                    step, stage, is_best
                )
        
        # 4. éªŒè¯ç»“æœ
        # æ£€æŸ¥æ—¥å¿—è®°å½•
        self.assertGreater(len(logger.training_history["metrics"]), 0)
        self.assertGreater(len(logger.training_history["stage_transitions"]), 0)
        
        # æ£€æŸ¥æ£€æŸ¥ç‚¹
        self.assertGreater(len(checkpoint_manager.checkpoint_history), 0)
        
        # æ£€æŸ¥æœ€ä½³æ£€æŸ¥ç‚¹
        best_checkpoint = checkpoint_manager.get_best_checkpoint()
        if best_checkpoint:
            self.assertTrue(os.path.exists(best_checkpoint))
        
        # 5. æ¸…ç†
        logger.close()
        checkpoint_manager.save_checkpoint_history()
        
        print("âœ… å®Œæ•´è®­ç»ƒæ—¥å¿—è®°å½•å·¥ä½œæµæµ‹è¯•é€šè¿‡")


def run_training_logger_tests():
    """è¿è¡Œæ‰€æœ‰è®­ç»ƒæ—¥å¿—è®°å½•å™¨æµ‹è¯•"""
    print("ğŸ§ª å¼€å§‹è®­ç»ƒæ—¥å¿—è®°å½•å™¨æµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•ç±»
    test_classes = [
        TestCurriculumTensorBoardLogger,
        TestModelCheckpointManager,
        TestCurriculumTrainingCallbacks,
        TestTrainingConfigCreation,
        TestIntegrationScenarios
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    # è¾“å‡ºç»“æœæ‘˜è¦
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
    print(f"   æ€»æµ‹è¯•æ•°: {result.testsRun}")
    print(f"   æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"   å¤±è´¥: {len(result.failures)}")
    print(f"   é”™è¯¯: {len(result.errors)}")
    
    if result.failures:
        print(f"\nâŒ å¤±è´¥çš„æµ‹è¯•:")
        for failure in result.failures:
            print(f"   - {failure[0]}: {failure[1].split('AssertionError:')[-1].strip()}")
    
    if result.errors:
        print(f"\nğŸ’¥ é”™è¯¯çš„æµ‹è¯•:")
        for error in result.errors:
            print(f"   - {error[0]}: {error[1].split('Exception:')[-1].strip()}")
    
    return result.wasSuccessful()


if __name__ == "__main__":
    success = run_training_logger_tests()
    sys.exit(0 if success else 1)
