"""
ç®€åŒ–ç‰ˆè®­ç»ƒæ—¥å¿—è®°å½•å™¨æµ‹è¯•
æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½ï¼Œä¸ä¾èµ–å¤–éƒ¨å¯è§†åŒ–åº“
"""

import os
import sys
import unittest
import tempfile
import shutil
import torch
import numpy as np
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from training_logger import (
    CurriculumTensorBoardLogger, 
    ModelCheckpointManager, 
    create_training_config_with_logging
)


class TestCurriculumTensorBoardLoggerSimple(unittest.TestCase):
    """æµ‹è¯•è¯¾ç¨‹å­¦ä¹ TensorBoardæ—¥å¿—è®°å½•å™¨æ ¸å¿ƒåŠŸèƒ½"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.test_dir = tempfile.mkdtemp()
        self.logger = CurriculumTensorBoardLogger(self.test_dir, "test_experiment")
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        self.logger.close()
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def test_initialization(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        self.assertTrue(Path(self.test_dir).exists())
        self.assertEqual(self.logger.experiment_name, "test_experiment")
        self.assertEqual(self.logger.current_stage, 0)
        self.assertIsNotNone(self.logger.writer)
        print("âœ… TensorBoardæ—¥å¿—è®°å½•å™¨åˆå§‹åŒ–æµ‹è¯•é€šè¿‡")
    
    def test_log_stage_transition(self):
        """æµ‹è¯•é˜¶æ®µåˆ‡æ¢è®°å½•"""
        self.logger.log_stage_transition(0, 1, 1000, "performance_threshold")
        
        # æ£€æŸ¥è®°å½•æ˜¯å¦æ­£ç¡®ä¿å­˜
        self.assertEqual(len(self.logger.training_history["stage_transitions"]), 1)
        transition = self.logger.training_history["stage_transitions"][0]
        
        self.assertEqual(transition["from_stage"], 0)
        self.assertEqual(transition["to_stage"], 1)
        self.assertEqual(transition["step"], 1000)
        self.assertEqual(transition["reason"], "performance_threshold")
        self.assertEqual(self.logger.current_stage, 1)
        print("âœ… é˜¶æ®µåˆ‡æ¢è®°å½•æµ‹è¯•é€šè¿‡")
    
    def test_log_rollback_event(self):
        """æµ‹è¯•å›é€€äº‹ä»¶è®°å½•"""
        self.logger.log_rollback_event(2, 5000, 0.15, 0.1)
        
        # æ£€æŸ¥å›é€€äº‹ä»¶è®°å½•
        self.assertEqual(len(self.logger.training_history["rollback_events"]), 1)
        rollback = self.logger.training_history["rollback_events"][0]
        
        self.assertEqual(rollback["stage"], 2)
        self.assertEqual(rollback["step"], 5000)
        self.assertEqual(rollback["performance_drop"], 0.15)
        self.assertEqual(rollback["threshold"], 0.1)
        print("âœ… å›é€€äº‹ä»¶è®°å½•æµ‹è¯•é€šè¿‡")
    
    def test_log_scale_invariant_metrics(self):
        """æµ‹è¯•å°ºåº¦ä¸å˜æŒ‡æ ‡è®°å½•"""
        metrics = {
            "per_agent_reward": 12.5,
            "normalized_completion_score": 0.85,
            "efficiency_metric": 0.42
        }
        
        self.logger.log_scale_invariant_metrics(metrics, 2000, 1, 5, 3)
        
        # æ£€æŸ¥æŒ‡æ ‡è®°å½•
        self.assertEqual(len(self.logger.training_history["metrics"]), 1)
        metric_record = self.logger.training_history["metrics"][0]
        
        self.assertEqual(metric_record["step"], 2000)
        self.assertEqual(metric_record["stage"], 1)
        self.assertEqual(metric_record["n_uavs"], 5)
        self.assertEqual(metric_record["n_targets"], 3)
        self.assertEqual(metric_record["per_agent_reward"], 12.5)
        print("âœ… å°ºåº¦ä¸å˜æŒ‡æ ‡è®°å½•æµ‹è¯•é€šè¿‡")
    
    def test_save_training_history(self):
        """æµ‹è¯•è®­ç»ƒå†å²ä¿å­˜"""
        # æ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®
        self.logger.log_stage_transition(0, 1, 1000)
        self.logger.log_scale_invariant_metrics({"per_agent_reward": 10.0}, 1500, 1, 4, 2)
        
        # ä¿å­˜å†å²
        self.logger.save_training_history()
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        history_file = Path(self.test_dir) / "test_experiment_history.json"
        self.assertTrue(history_file.exists())
        
        # æ£€æŸ¥æ–‡ä»¶å†…å®¹
        import json
        with open(history_file, 'r', encoding='utf-8') as f:
            saved_history = json.load(f)
        
        self.assertEqual(len(saved_history["stage_transitions"]), 1)
        self.assertEqual(len(saved_history["metrics"]), 1)
        print("âœ… è®­ç»ƒå†å²ä¿å­˜æµ‹è¯•é€šè¿‡")


class TestModelCheckpointManagerSimple(unittest.TestCase):
    """æµ‹è¯•æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†å™¨æ ¸å¿ƒåŠŸèƒ½"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.test_dir = tempfile.mkdtemp()
        self.manager = ModelCheckpointManager(self.test_dir, max_checkpoints=3)
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def test_initialization(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        self.assertTrue(Path(self.test_dir).exists())
        self.assertEqual(self.manager.max_checkpoints, 3)
        self.assertEqual(len(self.manager.checkpoint_history), 0)
        print("âœ… æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†å™¨åˆå§‹åŒ–æµ‹è¯•é€šè¿‡")
    
    def test_save_checkpoint(self):
        """æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜"""
        # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹çŠ¶æ€
        model_state = {"layer1.weight": torch.randn(10, 5)}
        optimizer_state = {"lr": 0.001}
        metrics = {"normalized_completion_score": 0.85}
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint_path = self.manager.save_checkpoint(
            model_state, optimizer_state, metrics, 1000, 1, is_best=True
        )
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        self.assertTrue(os.path.exists(checkpoint_path))
        
        # æ£€æŸ¥å†å²è®°å½•
        self.assertEqual(len(self.manager.checkpoint_history), 1)
        checkpoint_info = self.manager.checkpoint_history[0]
        
        self.assertEqual(checkpoint_info["step"], 1000)
        self.assertEqual(checkpoint_info["stage"], 1)
        self.assertTrue(checkpoint_info["is_best"])
        self.assertEqual(checkpoint_info["metrics"]["normalized_completion_score"], 0.85)
        print("âœ… æ£€æŸ¥ç‚¹ä¿å­˜æµ‹è¯•é€šè¿‡")
    
    def test_load_checkpoint(self):
        """æµ‹è¯•æ£€æŸ¥ç‚¹åŠ è½½"""
        # å…ˆä¿å­˜ä¸€ä¸ªæ£€æŸ¥ç‚¹
        model_state = {"layer1.weight": torch.randn(10, 5)}
        optimizer_state = {"lr": 0.001}
        metrics = {"normalized_completion_score": 0.85}
        
        checkpoint_path = self.manager.save_checkpoint(
            model_state, optimizer_state, metrics, 1000, 1
        )
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        loaded_data = self.manager.load_checkpoint(checkpoint_path)
        
        # éªŒè¯åŠ è½½çš„æ•°æ®
        self.assertIn("model_state_dict", loaded_data)
        self.assertIn("optimizer_state_dict", loaded_data)
        self.assertIn("metrics", loaded_data)
        self.assertEqual(loaded_data["step"], 1000)
        self.assertEqual(loaded_data["stage"], 1)
        print("âœ… æ£€æŸ¥ç‚¹åŠ è½½æµ‹è¯•é€šè¿‡")
    
    def test_get_best_checkpoint(self):
        """æµ‹è¯•è·å–æœ€ä½³æ£€æŸ¥ç‚¹"""
        # ä¿å­˜å¤šä¸ªæ£€æŸ¥ç‚¹
        for i, score in enumerate([0.7, 0.85, 0.8]):
            model_state = {"layer1.weight": torch.randn(10, 5)}
            optimizer_state = {"lr": 0.001}
            metrics = {"normalized_completion_score": score}
            
            self.manager.save_checkpoint(
                model_state, optimizer_state, metrics, 
                i*1000, 1, is_best=(score == 0.85)
            )
        
        # è·å–æœ€ä½³æ£€æŸ¥ç‚¹
        best_path = self.manager.get_best_checkpoint()
        self.assertIsNotNone(best_path)
        
        # éªŒè¯æ˜¯æœ€ä½³çš„
        loaded_data = self.manager.load_checkpoint(best_path)
        self.assertEqual(loaded_data["metrics"]["normalized_completion_score"], 0.85)
        print("âœ… æœ€ä½³æ£€æŸ¥ç‚¹è·å–æµ‹è¯•é€šè¿‡")


class TestTrainingConfigCreationSimple(unittest.TestCase):
    """æµ‹è¯•è®­ç»ƒé…ç½®åˆ›å»º"""
    
    def test_create_training_config_with_logging(self):
        """æµ‹è¯•åˆ›å»ºåŒ…å«æ—¥å¿—è®°å½•çš„è®­ç»ƒé…ç½®"""
        base_config = {
            "env": "test_env",
            "num_workers": 4,
            "lr": 0.001
        }
        
        enhanced_config = create_training_config_with_logging(
            base_config, 
            log_dir="./test_logs",
            experiment_name="test_experiment"
        )
        
        # éªŒè¯åŸºç¡€é…ç½®ä¿ç•™
        self.assertEqual(enhanced_config["env"], "test_env")
        self.assertEqual(enhanced_config["num_workers"], 4)
        self.assertEqual(enhanced_config["lr"], 0.001)
        
        # éªŒè¯æ—¥å¿—é…ç½®æ·»åŠ 
        self.assertEqual(enhanced_config["log_dir"], "./test_logs")
        self.assertEqual(enhanced_config["experiment_name"], "test_experiment")
        
        # éªŒè¯TensorBoardé…ç½®
        self.assertIn("logger_config", enhanced_config)
        self.assertIn("checkpoint_freq", enhanced_config)
        print("âœ… è®­ç»ƒé…ç½®åˆ›å»ºæµ‹è¯•é€šè¿‡")


class TestIntegrationWorkflowSimple(unittest.TestCase):
    """ç®€åŒ–ç‰ˆé›†æˆæµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.test_dir = tempfile.mkdtemp()
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def test_complete_training_logging_workflow(self):
        """æµ‹è¯•å®Œæ•´çš„è®­ç»ƒæ—¥å¿—è®°å½•å·¥ä½œæµ"""
        print("ğŸ”„ å¼€å§‹å®Œæ•´å·¥ä½œæµæµ‹è¯•...")
        
        # 1. åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        logger = CurriculumTensorBoardLogger(self.test_dir, "integration_test")
        
        # 2. åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
        checkpoint_manager = ModelCheckpointManager(
            os.path.join(self.test_dir, "checkpoints")
        )
        
        # 3. æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        for step in range(0, 1000, 100):
            stage = step // 300
            
            # è®°å½•æŒ‡æ ‡
            metrics = {
                "per_agent_reward": 10 + np.random.normal(0, 1),
                "normalized_completion_score": 0.7 + np.random.uniform(-0.1, 0.2),
                "efficiency_metric": 0.3 + np.random.uniform(-0.05, 0.1)
            }
            
            logger.log_scale_invariant_metrics(metrics, step, stage, 5, 3)
            
            # æ¨¡æ‹Ÿé˜¶æ®µåˆ‡æ¢
            if step in [300, 600, 900]:
                logger.log_stage_transition(stage-1, stage, step)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if step % 200 == 0:
                model_state = {"layer1.weight": torch.randn(10, 5)}
                optimizer_state = {"lr": 0.001}
                
                is_best = metrics["normalized_completion_score"] > 0.8
                checkpoint_manager.save_checkpoint(
                    model_state, optimizer_state, metrics, 
                    step, stage, is_best
                )
        
        # 4. éªŒè¯ç»“æœ
        # æ£€æŸ¥æ—¥å¿—è®°å½•
        self.assertGreater(len(logger.training_history["metrics"]), 0)
        self.assertGreater(len(logger.training_history["stage_transitions"]), 0)
        
        # æ£€æŸ¥æ£€æŸ¥ç‚¹
        self.assertGreater(len(checkpoint_manager.checkpoint_history), 0)
        
        # æ£€æŸ¥æœ€ä½³æ£€æŸ¥ç‚¹
        best_checkpoint = checkpoint_manager.get_best_checkpoint()
        if best_checkpoint:
            self.assertTrue(os.path.exists(best_checkpoint))
        
        # 5. æ¸…ç†
        logger.close()
        checkpoint_manager.save_checkpoint_history()
        
        print("âœ… å®Œæ•´è®­ç»ƒæ—¥å¿—è®°å½•å·¥ä½œæµæµ‹è¯•é€šè¿‡")


def run_simple_training_logger_tests():
    """è¿è¡Œç®€åŒ–ç‰ˆè®­ç»ƒæ—¥å¿—è®°å½•å™¨æµ‹è¯•"""
    print("ğŸ§ª å¼€å§‹ç®€åŒ–ç‰ˆè®­ç»ƒæ—¥å¿—è®°å½•å™¨æµ‹è¯•...")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•ç±»
    test_classes = [
        TestCurriculumTensorBoardLoggerSimple,
        TestModelCheckpointManagerSimple,
        TestTrainingConfigCreationSimple,
        TestIntegrationWorkflowSimple
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=1)
    result = runner.run(test_suite)
    
    # è¾“å‡ºç»“æœæ‘˜è¦
    print(f"\nğŸ“Š ç®€åŒ–ç‰ˆæµ‹è¯•ç»“æœæ‘˜è¦:")
    print(f"   æ€»æµ‹è¯•æ•°: {result.testsRun}")
    print(f"   æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"   å¤±è´¥: {len(result.failures)}")
    print(f"   é”™è¯¯: {len(result.errors)}")
    
    if result.failures:
        print(f"\nâŒ å¤±è´¥çš„æµ‹è¯•:")
        for failure in result.failures:
            print(f"   - {failure[0]}: {failure[1].split('AssertionError:')[-1].strip()}")
    
    if result.errors:
        print(f"\nğŸ’¥ é”™è¯¯çš„æµ‹è¯•:")
        for error in result.errors:
            print(f"   - {error[0]}: {error[1].split('Exception:')[-1].strip()}")
    
    return result.wasSuccessful()


if __name__ == "__main__":
    success = run_simple_training_logger_tests()
    sys.exit(0 if success else 1)