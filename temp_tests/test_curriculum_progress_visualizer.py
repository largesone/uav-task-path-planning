"""
è¯¾ç¨‹å­¦ä¹ è¿›åº¦å¯è§†åŒ–å™¨æµ‹è¯•
æµ‹è¯•å„ç§å¯è§†åŒ–åŠŸèƒ½å’Œå›¾è¡¨ç”Ÿæˆ
"""

import os
import sys
import unittest
import tempfile
import shutil
import json
import numpy as np
from datetime import datetime
from pathlib import Path
from unittest.mock import Mock, patch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from curriculum_progress_visualizer import CurriculumProgressVisualizer


class TestCurriculumProgressVisualizer(unittest.TestCase):
    """æµ‹è¯•è¯¾ç¨‹å­¦ä¹ è¿›åº¦å¯è§†åŒ–å™¨"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.test_dir = tempfile.mkdtemp()
        self.visualizer = CurriculumProgressVisualizer(self.test_dir, "test_experiment")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        self.test_history = {
            "metrics": [
                {
                    "step": i,
                    "stage": i // 30,
                    "n_uavs": 3 + (i // 30),
                    "n_targets": 2 + (i // 30),
                    "per_agent_reward": 10 + np.random.normal(0, 1),
                    "normalized_completion_score": 0.7 + np.random.uniform(-0.1, 0.2),
                    "efficiency_metric": 0.3 + np.random.uniform(-0.05, 0.1),
                    "timestamp": datetime.now().isoformat()
                }
                for i in range(100)
            ],
            "stage_transitions": [
                {"step": 30, "from_stage": 0, "to_stage": 1, "reason": "performance_threshold"},
                {"step": 60, "from_stage": 1, "to_stage": 2, "reason": "performance_threshold"},
                {"step": 90, "from_stage": 2, "to_stage": 3, "reason": "performance_threshold"}
            ],
            "rollback_events": [
                {"step": 45, "stage": 1, "performance_drop": 0.15, "threshold": 0.1}
            ]
        }
        
        # ä¿å­˜æµ‹è¯•å†å²æ•°æ®
        history_file = Path(self.test_dir) / "test_experiment_history.json"
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(self.test_history, f, indent=2)
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def test_initialization(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        self.assertTrue(self.visualizer.viz_dir.exists())
        self.assertEqual(self.visualizer.experiment_name, "test_experiment")
    
    def test_load_training_history(self):
        """æµ‹è¯•è®­ç»ƒå†å²åŠ è½½"""
        history = self.visualizer.load_training_history()
        
        self.assertIn("metrics", history)
        self.assertIn("stage_transitions", history)
        self.assertIn("rollback_events", history)
        
        self.assertEqual(len(history["metrics"]), 100)
        self.assertEqual(len(history["stage_transitions"]), 3)
        self.assertEqual(len(history["rollback_events"]), 1)
    
    @patch('matplotlib.pyplot.savefig')
    @patch('matplotlib.pyplot.close')
    def test_plot_scale_invariant_metrics(self, mock_close, mock_savefig):
        """æµ‹è¯•å°ºåº¦ä¸å˜æŒ‡æ ‡ç»˜å›¾"""
        history = self.visualizer.load_training_history()
        save_path = self.visualizer.plot_scale_invariant_metrics(history)
        
        # éªŒè¯ä¿å­˜è·¯å¾„
        expected_path = self.visualizer.viz_dir / "scale_invariant_metrics.png"
        self.assertEqual(save_path, str(expected_path))
        
        # éªŒè¯matplotlibå‡½æ•°è¢«è°ƒç”¨
        mock_savefig.assert_called_once()
        mock_close.assert_called_once()
    
    @patch('matplotlib.pyplot.savefig')
    @patch('matplotlib.pyplot.close')
    def test_plot_curriculum_stages(self, mock_close, mock_savefig):
        """æµ‹è¯•è¯¾ç¨‹å­¦ä¹ é˜¶æ®µç»˜å›¾"""
        history = self.visualizer.load_training_history()
        save_path = self.visualizer.plot_curriculum_stages(history)
        
        # éªŒè¯ä¿å­˜è·¯å¾„
        expected_path = self.visualizer.viz_dir / "curriculum_stages.png"
        self.assertEqual(save_path, str(expected_path))
        
        # éªŒè¯matplotlibå‡½æ•°è¢«è°ƒç”¨
        mock_savefig.assert_called_once()
        mock_close.assert_called_once()
    
    @patch('matplotlib.pyplot.savefig')
    @patch('matplotlib.pyplot.close')
    def test_plot_scale_transfer_analysis(self, mock_close, mock_savefig):
        """æµ‹è¯•å°ºåº¦è¿ç§»åˆ†æç»˜å›¾"""
        history = self.visualizer.load_training_history()
        save_path = self.visualizer.plot_scale_transfer_analysis(history)
        
        # éªŒè¯ä¿å­˜è·¯å¾„
        expected_path = self.visualizer.viz_dir / "scale_transfer_analysis.png"
        self.assertEqual(save_path, str(expected_path))
        
        # éªŒè¯matplotlibå‡½æ•°è¢«è°ƒç”¨
        mock_savefig.assert_called_once()
        mock_close.assert_called_once()
    
    @patch('plotly.graph_objects.Figure.write_html')
    def test_create_interactive_dashboard(self, mock_write_html):
        """æµ‹è¯•äº¤äº’å¼ä»ªè¡¨æ¿åˆ›å»º"""
        history = self.visualizer.load_training_history()
        save_path = self.visualizer.create_interactive_dashboard(history)
        
        # éªŒè¯ä¿å­˜è·¯å¾„
        expected_path = self.visualizer.viz_dir / "interactive_dashboard.html"
        self.assertEqual(save_path, str(expected_path))
        
        # éªŒè¯plotlyå‡½æ•°è¢«è°ƒç”¨
        mock_write_html.assert_called_once()
    
    def test_generate_training_report(self):
        """æµ‹è¯•è®­ç»ƒæŠ¥å‘Šç”Ÿæˆ"""
        history = self.visualizer.load_training_history()
        report_path = self.visualizer.generate_training_report(history)
        
        # éªŒè¯æŠ¥å‘Šæ–‡ä»¶å­˜åœ¨
        self.assertTrue(os.path.exists(report_path))
        
        # éªŒè¯æŠ¥å‘Šå†…å®¹
        with open(report_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        self.assertIn("è¯¾ç¨‹å­¦ä¹ è®­ç»ƒæŠ¥å‘Š", content)
        self.assertIn("test_experiment", content)
        self.assertIn("é˜¶æ®µåˆ‡æ¢æ¬¡æ•°", content)
        self.assertIn("å›é€€äº‹ä»¶æ¬¡æ•°", content)
    
    @patch('curriculum_progress_visualizer.CurriculumProgressVisualizer.plot_scale_invariant_metrics')
    @patch('curriculum_progress_visualizer.CurriculumProgressVisualizer.plot_curriculum_stages')
    @patch('curriculum_progress_visualizer.CurriculumProgressVisualizer.plot_scale_transfer_analysis')
    @patch('curriculum_progress_visualizer.CurriculumProgressVisualizer.create_interactive_dashboard')
    @patch('curriculum_progress_visualizer.CurriculumProgressVisualizer.generate_training_report')
    def test_generate_all_visualizations(self, mock_report, mock_dashboard, 
                                       mock_transfer, mock_stages, mock_metrics):
        """æµ‹è¯•ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–"""
        # è®¾ç½®æ¨¡æ‹Ÿè¿”å›å€¼
        mock_metrics.return_value = "metrics.png"
        mock_stages.return_value = "stages.png"
        mock_transfer.return_value = "transfer.png"
        mock_dashboard.return_value = "dashboard.html"
        mock_report.return_value = "report.md"
        
        results = self.visualizer.generate_all_visualizations()
        
        # éªŒè¯æ‰€æœ‰æ–¹æ³•è¢«è°ƒç”¨
        mock_metrics.assert_called_once()
        mock_stages.assert_called_once()
        mock_transfer.assert_called_once()
        mock_dashboard.assert_called_once()
        mock_report.assert_called_once()
        
        # éªŒè¯è¿”å›ç»“æœ
        expected_keys = ['metrics', 'stages', 'transfer', 'dashboard', 'report']
        for key in expected_keys:
            self.assertIn(key, results)


class TestVisualizationDataProcessing(unittest.TestCase):
    """æµ‹è¯•å¯è§†åŒ–æ•°æ®å¤„ç†"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.test_dir = tempfile.mkdtemp()
        self.visualizer = CurriculumProgressVisualizer(self.test_dir, "test_experiment")
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def test_empty_data_handling(self):
        """æµ‹è¯•ç©ºæ•°æ®å¤„ç†"""
        empty_history = {
            "metrics": [],
            "stage_transitions": [],
            "rollback_events": []
        }
        
        # ä¿å­˜ç©ºå†å²æ•°æ®
        history_file = Path(self.test_dir) / "test_experiment_history.json"
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(empty_history, f)
        
        # æµ‹è¯•å„ç§å¯è§†åŒ–å‡½æ•°ä¸ä¼šå´©æºƒ
        with patch('matplotlib.pyplot.savefig'), patch('matplotlib.pyplot.close'):
            metrics_path = self.visualizer.plot_scale_invariant_metrics(empty_history)
            stages_path = self.visualizer.plot_curriculum_stages(empty_history)
            transfer_path = self.visualizer.plot_scale_transfer_analysis(empty_history)
        
        # éªŒè¯è·¯å¾„æ­£ç¡®è¿”å›
        self.assertTrue(metrics_path.endswith("scale_invariant_metrics.png"))
        self.assertTrue(stages_path.endswith("curriculum_stages.png"))
        self.assertTrue(transfer_path.endswith("scale_transfer_analysis.png"))
    
    def test_missing_columns_handling(self):
        """æµ‹è¯•ç¼ºå¤±åˆ—å¤„ç†"""
        incomplete_history = {
            "metrics": [
                {"step": 1, "stage": 0},  # ç¼ºå°‘å…¶ä»–æŒ‡æ ‡
                {"step": 2, "stage": 0, "per_agent_reward": 10.0}  # éƒ¨åˆ†æŒ‡æ ‡
            ],
            "stage_transitions": [],
            "rollback_events": []
        }
        
        # æµ‹è¯•ä¸ä¼šå› ä¸ºç¼ºå¤±åˆ—è€Œå´©æºƒ
        with patch('matplotlib.pyplot.savefig'), patch('matplotlib.pyplot.close'):
            try:
                self.visualizer.plot_scale_invariant_metrics(incomplete_history)
                self.visualizer.plot_scale_transfer_analysis(incomplete_history)
            except Exception as e:
                self.fail(f"å¯è§†åŒ–å‡½æ•°åœ¨å¤„ç†ä¸å®Œæ•´æ•°æ®æ—¶å´©æºƒ: {e}")


def run_visualizer_tests():
    """è¿è¡Œæ‰€æœ‰å¯è§†åŒ–å™¨æµ‹è¯•"""
    print("ğŸ¨ å¼€å§‹è¯¾ç¨‹å­¦ä¹ è¿›åº¦å¯è§†åŒ–å™¨æµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•ç±»
    test_classes = [
        TestCurriculumProgressVisualizer,
        TestVisualizationDataProcessing
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    # è¾“å‡ºç»“æœæ‘˜è¦
    print(f"\nğŸ“Š å¯è§†åŒ–å™¨æµ‹è¯•ç»“æœæ‘˜è¦:")
    print(f"   æ€»æµ‹è¯•æ•°: {result.testsRun}")
    print(f"   æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"   å¤±è´¥: {len(result.failures)}")
    print(f"   é”™è¯¯: {len(result.errors)}")
    
    return result.wasSuccessful()


if __name__ == "__main__":
    success = run_visualizer_tests()
    sys.exit(0 if success else 1)
