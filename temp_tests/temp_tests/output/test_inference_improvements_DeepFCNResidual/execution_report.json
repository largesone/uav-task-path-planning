{
  "scenario_name": "test_inference_improvements",
  "network_type": "DeepFCNResidual",
  "training_time": 5.728899717330933,
  "task_assignments_count": 4,
  "evaluation_metrics": {
    "completion_rate": 0.5416666666666667,
    "satisfied_targets_rate": 1.0,
    "target_satisfaction_rate": 0.5,
    "resource_satisfaction_rate": 0.5833333333333334,
    "resource_utilization_rate": 0.5833333333333334,
    "total_reward_score": 1408.455988548012,
    "marginal_utility_score": 41.66666666666666,
    "resource_efficiency_score": 1166.6666666666667,
    "distance_cost_score": -70.71067811865476,
    "target_completion_score": 270.83333333333337,
    "completion_bonus": 0,
    "completed_targets_count": 1,
    "total_targets_count": 2,
    "total_contribution": 7.0,
    "total_demand": 12,
    "total_initial_supply": 12
  },
  "training_statistics": {
    "reward_mean": -239.30057757411157,
    "reward_std": 379.21441021437477,
    "reward_max": 324.44671831738407,
    "reward_min": -851.5884045154962,
    "reward_final": -649.8221196181519,
    "reward_improvement": -93.01543769524301,
    "loss_mean": 3437.4312330268203,
    "loss_std": 2341.91194204617,
    "loss_final": 4984.578125,
    "loss_min": 0.0
  },
  "training_history_summary": {
    "total_episodes": 13,
    "final_reward": -649.8221196181519,
    "final_loss": 4984.578125,
    "final_epsilon": 0.8432232235918236,
    "final_completion_rate": 0.7083333333333333
  },
  "config_summary": {
    "episodes": 15,
    "learning_rate": 0.0001,
    "batch_size": 64,
    "gamma": 0.99,
    "epsilon_start": 0.9,
    "epsilon_end": 0.1,
    "epsilon_decay": 0.9995
  }
}