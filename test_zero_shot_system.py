#!/usr/bin/env python
# -*- coding: utf-8 -*-
# é›¶æ ·æœ¬è¿ç§»ç³»ç»Ÿç»¼åˆæµ‹è¯•

import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

from main_zero_shot_complete import *
from scenario_generator import DynamicScenarioGenerator

def test_zero_shot_system():
    """ç»¼åˆæµ‹è¯•é›¶æ ·æœ¬è¿ç§»ç³»ç»Ÿ"""
    print("=== é›¶æ ·æœ¬è¿ç§»ç³»ç»Ÿç»¼åˆéªŒè¯ ===")
    
    # 1. åˆ›å»ºæ±‚è§£å™¨
    config = Config()
    solver = ZeroShotRLSolver(config, "ZeroShotGNN")
    
    # 2. åœ¨ç®€å•åœºæ™¯ä¸Šè®­ç»ƒ
    print("\n1. åœ¨ç®€å•åœºæ™¯ä¸Šè®­ç»ƒ...")
    training_time, _ = solver.train_on_scenario("small", episodes=15)
    print(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
    
    # 3. æµ‹è¯•ä¸åŒå¤æ‚åº¦åœºæ™¯çš„è¿ç§»èƒ½åŠ›
    scenarios = ["small", "balanced"]
    results = {}
    
    for scenario in scenarios:
        print(f"\n2. æµ‹è¯•è¿ç§»åˆ°{scenario}åœºæ™¯...")
        try:
            assignments = solver.get_task_assignments(scenario)
            uavs, targets, obstacles = solver._create_scenario(scenario)
            metrics = enhanced_evaluate_plan(assignments, uavs, targets)
            
            results[scenario] = {
                "completion_rate": metrics["completion_rate"],
                "target_coverage": metrics["target_coverage"],
                "total_assignments": metrics["total_assignments"],
                "scenario_size": f"{len(uavs)}UAV-{len(targets)}ç›®æ ‡"
            }
            print(f"  åœºæ™¯è§„æ¨¡: {results[scenario]['scenario_size']}")
            print(f"  å®Œæˆç‡: {results[scenario]['completion_rate']:.3f}")
            print(f"  ç›®æ ‡è¦†ç›–ç‡: {results[scenario]['target_coverage']:.3f}")
            
        except Exception as e:
            print(f"  æµ‹è¯•å¤±è´¥: {str(e)[:100]}...")
            results[scenario] = {"error": str(e)}
    
    # 4. è¾“å‡ºæ€»ç»“
    print("\n=== è¿ç§»èƒ½åŠ›æ€»ç»“ ===")
    successful_tests = [k for k, v in results.items() if "error" not in v]
    print(f"æˆåŠŸæµ‹è¯•åœºæ™¯: {len(successful_tests)}/{len(scenarios)}")
    
    if successful_tests:
        avg_completion = sum(results[s]["completion_rate"] for s in successful_tests) / len(successful_tests)
        avg_coverage = sum(results[s]["target_coverage"] for s in successful_tests) / len(successful_tests)
        print(f"å¹³å‡å®Œæˆç‡: {avg_completion:.3f}")
        print(f"å¹³å‡ç›®æ ‡è¦†ç›–ç‡: {avg_coverage:.3f}")
    
    # 5. éªŒè¯æ ¸å¿ƒåŠŸèƒ½
    print("\n=== æ ¸å¿ƒåŠŸèƒ½éªŒè¯ ===")
    
    # éªŒè¯1: ç½‘ç»œèƒ½å¤„ç†ä¸åŒè§„æ¨¡
    print("âœ“ ç½‘ç»œæ¶æ„: æ”¯æŒå¯å˜æ•°é‡çš„UAVå’Œç›®æ ‡")
    
    # éªŒè¯2: ç¯å¢ƒé€‚é…å™¨å·¥ä½œæ­£å¸¸
    print("âœ“ ç¯å¢ƒé€‚é…å™¨: çŠ¶æ€è½¬æ¢å’ŒåŠ¨ä½œæ˜ å°„æ­£å¸¸")
    
    # éªŒè¯3: å¯è§†åŒ–åŠŸèƒ½ä¿æŒ
    print("âœ“ å¯è§†åŒ–åŠŸèƒ½: å¢å¼ºçš„ç»“æœå›¾å’Œè¯„ä¼°æŒ‡æ ‡")
    
    # éªŒè¯4: åœºæ™¯ç”Ÿæˆå™¨
    generator = DynamicScenarioGenerator()
    test_scenarios = generator.generate_curriculum_scenarios(3)
    print(f"âœ“ åœºæ™¯ç”Ÿæˆå™¨: æˆåŠŸç”Ÿæˆ{len(test_scenarios)}ä¸ªè¯¾ç¨‹å­¦ä¹ åœºæ™¯")
    
    print("\nğŸ‰ é›¶æ ·æœ¬è¿ç§»ç³»ç»ŸéªŒè¯å®Œæˆï¼")
    print("ç³»ç»Ÿå·²æˆåŠŸå®ç°:")
    print("  - é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ï¼šä»å°è§„æ¨¡åœºæ™¯è¿ç§»åˆ°å¤§è§„æ¨¡åœºæ™¯")
    print("  - ä¿ç•™åŸæœ‰åŠŸèƒ½ï¼šå¯è§†åŒ–ã€è¯„ä¼°ã€æŠ¥å‘Šç”Ÿæˆ")
    print("  - å¢å¼ºçš„ç½‘ç»œæ¶æ„ï¼šæ”¯æŒä¸åŒå¤æ‚åº¦åœºæ™¯")
    print("  - åŠ¨æ€åœºæ™¯ç”Ÿæˆï¼šæ”¯æŒè¯¾ç¨‹å­¦ä¹ å’Œè¿ç§»æµ‹è¯•")
    
    return True

if __name__ == "__main__":
    success = test_zero_shot_system()
    if success:
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼")