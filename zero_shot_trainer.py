#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›¶æ ·æœ¬è®­ç»ƒå™¨ - é›†æˆè®­ç»ƒç­–ç•¥åˆ°ä¸»ç¨‹åº

ä½¿ç”¨æ–¹æ³•ï¼š
1. ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶è¿›è¡Œé›¶æ ·æœ¬è®­ç»ƒ
2. æˆ–åœ¨main.pyä¸­å¯¼å…¥å¹¶ä½¿ç”¨
"""

import sys
import os
import numpy as np
import torch
from typing import Dict, List, Tuple
import time
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from zero_shot_training_strategy import ZeroShotTrainingStrategy, ZeroShotMetrics, create_zero_shot_training_config
from main import GraphRLSolver
from entities import UAV, Target
from environment import UAVTaskEnv, DirectedGraph
from config import Config
from scenarios import get_small_scenario

class ZeroShotTrainer:
    """é›¶æ ·æœ¬è®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.strategy = ZeroShotTrainingStrategy(config)
        self.metrics = ZeroShotMetrics()
        
    def create_dynamic_scenario(self, n_uavs: int, n_targets: int) -> Tuple[List[UAV], List[Target], List]:
        """
        åŠ¨æ€åˆ›å»ºè®­ç»ƒåœºæ™¯ - ä¿®å¤ç‰ˆæœ¬ï¼Œç¡®ä¿åŠ¨ä½œç©ºé—´å…¼å®¹
        
        Args:
            n_uavs: UAVæ•°é‡
            n_targets: ç›®æ ‡æ•°é‡
            
        Returns:
            Tuple: (UAVåˆ—è¡¨, ç›®æ ‡åˆ—è¡¨, éšœç¢ç‰©åˆ—è¡¨)
        """
        # é™åˆ¶åœºæ™¯è§„æ¨¡ä»¥ç¡®ä¿åŠ¨ä½œç©ºé—´ä¸è¶…è¿‡ç½‘ç»œè¾“å‡ºç»´åº¦
        n_phi = 6  # æ–¹å‘æ•°é‡
        max_output_dim = 1000  # ZeroShotGNNçš„è¾“å‡ºç»´åº¦
        max_possible_actions = max_output_dim
        
        # è®¡ç®—æœ€å¤§å®ä½“æ•°é‡
        max_total_pairs = max_possible_actions // n_phi
        max_entities_sqrt = int(np.sqrt(max_total_pairs))
        
        # ä¿å®ˆåœ°é™åˆ¶UAVå’Œç›®æ ‡æ•°é‡
        max_uavs = min(n_uavs, max_entities_sqrt)
        max_targets = min(n_targets, max_entities_sqrt)
        
        # è¿›ä¸€æ­¥ç¡®ä¿åŠ¨ä½œç©ºé—´ä¸è¶…é™
        while max_uavs * max_targets * n_phi > max_output_dim:
            if max_uavs > max_targets:
                max_uavs -= 1
            else:
                max_targets -= 1
        
        n_uavs = max(1, max_uavs)
        n_targets = max(1, max_targets)
        
        # åªåœ¨åœºæ™¯è§„æ¨¡è¢«è°ƒæ•´æ—¶æ˜¾ç¤ºè­¦å‘Š
        if n_uavs != max_uavs or n_targets != max_targets:
            print(f"åœºæ™¯è§„æ¨¡è°ƒæ•´: {n_uavs} UAV, {n_targets} ç›®æ ‡ (åŠ¨ä½œç©ºé—´: {n_uavs * n_targets * n_phi})")
        
        # åˆ›å»ºUAV
        uavs = []
        for i in range(n_uavs):
            position = np.array([
                random.uniform(0, 1000),
                random.uniform(0, 1000)
            ])
            heading = random.uniform(0, 2 * np.pi)
            resources = np.array([
                random.uniform(40, 80),
                random.uniform(40, 80)
            ])
            max_distance = random.uniform(800, 1200)
            velocity_range = (
                random.uniform(20, 40),
                random.uniform(60, 100)
            )
            economic_speed = random.uniform(50, 80)
            
            uav = UAV(i+1, position, heading, resources, max_distance, velocity_range, economic_speed)
            uavs.append(uav)
        
        # åˆ›å»ºç›®æ ‡
        targets = []
        for i in range(n_targets):
            position = np.array([
                random.uniform(200, 800),
                random.uniform(200, 800)
            ])
            resources = np.array([
                random.uniform(20, 60),
                random.uniform(20, 60)
            ])
            value = random.uniform(80, 120)
            
            target = Target(i+1, position, resources, value)
            targets.append(target)
        
        # ç®€å•éšœç¢ç‰©ï¼ˆå¯é€‰ï¼‰
        obstacles = []
        
        return uavs, targets, obstacles
    
    def train_with_strategy(self, output_dir: str = "output/zero_shot_training") -> Dict:
        """
        ä½¿ç”¨é›¶æ ·æœ¬è®­ç»ƒç­–ç•¥è¿›è¡Œè®­ç»ƒ
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Dict: è®­ç»ƒç»“æœ
        """
        print("ğŸš€ å¼€å§‹é›¶æ ·æœ¬è¿ç§»è®­ç»ƒ")
        print("=" * 60)
        print(self.strategy.get_training_summary())
        
        os.makedirs(output_dir, exist_ok=True)
        
        # è®­ç»ƒå†å²
        training_history = {
            'phases': [],
            'overall_metrics': [],
            'best_models': []
        }
        
        total_episodes = 0
        start_time = time.time()
        
        # åˆ†é˜¶æ®µè®­ç»ƒ
        for phase_idx in range(len(self.strategy.training_phases)):
            phase_config = self.strategy.get_current_phase_config()
            print(f"\nğŸ“š å¼€å§‹è®­ç»ƒé˜¶æ®µ {phase_idx + 1}: {phase_config['name']}")
            print(f"ç›®æ ‡: {phase_config['focus']}")
            print("-" * 40)
            
            # é˜¶æ®µè®­ç»ƒç»“æœ
            phase_results = self._train_phase(phase_config, output_dir, phase_idx)
            training_history['phases'].append(phase_results)
            
            total_episodes += phase_results['episodes_completed']
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
            if phase_results['should_advance']:
                print(f"âœ… é˜¶æ®µ {phase_idx + 1} å®Œæˆï¼Œè¿›å…¥ä¸‹ä¸€é˜¶æ®µ")
                self.strategy.advance_phase()
            else:
                print(f"âš ï¸ é˜¶æ®µ {phase_idx + 1} æœªè¾¾åˆ°é¢„æœŸç›®æ ‡ï¼Œä½†ç»§ç»­ä¸‹ä¸€é˜¶æ®µ")
                self.strategy.advance_phase()
        
        total_time = time.time() - start_time
        
        # æœ€ç»ˆè¯„ä¼°
        print(f"\nğŸ¯ è®­ç»ƒå®Œæˆæ€»ç»“")
        print("-" * 40)
        print(f"æ€»è®­ç»ƒè½®æ•°: {total_episodes}")
        print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f} å°æ—¶")
        
        # é›¶æ ·æœ¬è¿ç§»æµ‹è¯•
        transfer_results = self._evaluate_zero_shot_transfer(output_dir)
        
        final_results = {
            'training_history': training_history,
            'total_episodes': total_episodes,
            'total_time': total_time,
            'transfer_results': transfer_results,
            'final_transfer_score': self.metrics.compute_transfer_score([])
        }
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        import json
        with open(f"{output_dir}/training_results.json", 'w') as f:
            json.dump(final_results, f, indent=2, default=str)
        
        return final_results
    
    def _train_phase(self, phase_config: Dict, output_dir: str, phase_idx: int) -> Dict:
        """è®­ç»ƒå•ä¸ªé˜¶æ®µ"""
        phase_start_time = time.time()
        phase_episodes = 0
        phase_rewards = []
        phase_completion_rates = []
        best_phase_reward = float('-inf')
        
        # åˆ›å»ºåˆå§‹åœºæ™¯ç”¨äºåˆå§‹åŒ–solver
        n_uavs, n_targets = self.strategy.generate_training_scenario(phase_config)
        uavs, targets, obstacles = self.create_dynamic_scenario(n_uavs, n_targets)
        
        # åˆ›å»ºå›¾å’Œç¯å¢ƒ
        graph = DirectedGraph(uavs, targets, self.config.GRAPH_N_PHI, obstacles, self.config)
        
        # åˆ›å»ºsolver
        solver = GraphRLSolver(
            uavs, targets, graph, obstacles,
            i_dim=1,  # å ä½å€¼ï¼ŒZeroShotGNNä¼šå¿½ç•¥
            h_dim=[256, 128],
            o_dim=1000,  # è¶³å¤Ÿå¤§çš„è¾“å‡ºç»´åº¦
            config=self.config,
            network_type="ZeroShotGNN",
            tensorboard_dir=f"{output_dir}/phase_{phase_idx}",
            obs_mode="graph"
        )
        
        # åº”ç”¨é˜¶æ®µç‰¹å®šçš„é…ç½®
        self._apply_phase_config(solver, phase_config)
        
        print(f"é˜¶æ®µé…ç½®: UAVèŒƒå›´{phase_config['uav_range']}, ç›®æ ‡èŒƒå›´{phase_config['target_range']}")
        print(f"å­¦ä¹ ç‡: {phase_config['learning_rate']}, æ‰¹æ¬¡å¤§å°: {phase_config['batch_size']}")
        
        # é˜¶æ®µè®­ç»ƒå¾ªç¯
        for episode in range(phase_config['episodes']):
            try:
                # åŠ¨æ€ç”Ÿæˆæ–°åœºæ™¯ - ä½¿ç”¨æ›´ä¿å®ˆçš„é¢‘ç‡å’Œè§„æ¨¡é™åˆ¶
                if episode % 20 == 0:  # æ¯20è½®æ›´æ¢åœºæ™¯ï¼Œå‡å°‘é¢‘ç‡
                    n_uavs, n_targets = self.strategy.generate_training_scenario(phase_config)
                    # è¿›ä¸€æ­¥é™åˆ¶è§„æ¨¡ä»¥ç¡®ä¿ç¨³å®šæ€§
                    n_uavs = min(n_uavs, 6)
                    n_targets = min(n_targets, 8)
                    
                    uavs, targets, obstacles = self.create_dynamic_scenario(n_uavs, n_targets)
                    
                    # æ›´æ–°solverçš„ç¯å¢ƒ
                    graph = DirectedGraph(uavs, targets, self.config.GRAPH_N_PHI, obstacles, self.config)
                    solver.env = UAVTaskEnv(uavs, targets, graph, obstacles, self.config, obs_mode="graph")
                    solver.uavs = uavs
                    solver.targets = targets
                    solver.graph = graph
                
                # è®­ç»ƒä¸€è½®
                episode_result = self._train_episode(solver, phase_config)
                
                phase_episodes += 1
                phase_rewards.append(episode_result['reward'])
                phase_completion_rates.append(episode_result['completion_rate'])
                
                # æ›´æ–°æŒ‡æ ‡
                self.metrics.update(episode_result)
                
                # è®°å½•æœ€ä½³æ¨¡å‹
                if episode_result['reward'] > best_phase_reward:
                    best_phase_reward = episode_result['reward']
                    model_path = f"{output_dir}/phase_{phase_idx}_best_model.pth"
                    solver.save_model(model_path)
                
                # é˜¶æ®µè¿›åº¦æŠ¥å‘Š
                if episode % 50 == 0 and episode > 0:
                    avg_reward = np.mean(phase_rewards[-50:])
                    avg_completion = np.mean(phase_completion_rates[-50:])
                    print(f"  Episode {episode:4d}: å¹³å‡å¥–åŠ± {avg_reward:8.2f}, å®Œæˆç‡ {avg_completion:.3f}")
                    
            except Exception as e:
                # å®‰å…¨åœ°å¤„ç†å¼‚å¸¸ä¿¡æ¯ï¼Œé¿å…æ‰“å°tensorå¯¹è±¡
                error_msg = str(e) if not isinstance(e, torch.Tensor) else f"Tensorå¼‚å¸¸: shape={e.shape}"
                print(f"Episode {episode} è®­ç»ƒå‡ºé”™: {error_msg}")
                # ç»§ç»­ä¸‹ä¸€ä¸ªepisode
                continue
        
        phase_time = time.time() - phase_start_time
        
        # é˜¶æ®µç»“æœ
        phase_results = {
            'phase_name': phase_config['name'],
            'episodes_completed': phase_episodes,
            'phase_time': phase_time,
            'avg_reward': np.mean(phase_rewards),
            'best_reward': best_phase_reward,
            'avg_completion_rate': np.mean(phase_completion_rates),
            'final_completion_rate': phase_completion_rates[-1] if phase_completion_rates else 0,
            'should_advance': True  # ç®€åŒ–ç‰ˆæœ¬ï¼Œæ€»æ˜¯è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
        }
        
        print(f"é˜¶æ®µ {phase_idx + 1} å®Œæˆ:")
        print(f"  - è®­ç»ƒè½®æ•°: {phase_episodes}")
        print(f"  - å¹³å‡å¥–åŠ±: {phase_results['avg_reward']:.2f}")
        print(f"  - æœ€ä½³å¥–åŠ±: {phase_results['best_reward']:.2f}")
        print(f"  - å¹³å‡å®Œæˆç‡: {phase_results['avg_completion_rate']:.3f}")
        print(f"  - è®­ç»ƒæ—¶é—´: {phase_time/60:.1f} åˆ†é’Ÿ")
        
        return phase_results
    
    def _apply_phase_config(self, solver: GraphRLSolver, phase_config: Dict):
        """åº”ç”¨é˜¶æ®µç‰¹å®šé…ç½®"""
        # æ›´æ–°å­¦ä¹ ç‡
        for param_group in solver.optimizer.param_groups:
            param_group['lr'] = phase_config['learning_rate']
        
        # æ›´æ–°æ¢ç´¢ç‡
        solver.epsilon = phase_config['epsilon_start']
        solver.epsilon_min = phase_config['epsilon_end']
        
        # åº”ç”¨æ­£åˆ™åŒ–è®¾ç½®
        reg_settings = self.strategy.apply_regularization_techniques(solver.policy_net, phase_config)
        solver.grad_clip_norm = reg_settings['gradient_clip_norm']
    
    def _train_episode(self, solver: GraphRLSolver, phase_config: Dict) -> Dict:
        """è®­ç»ƒå•ä¸ªepisode - ä¿®å¤ç‰ˆæœ¬ï¼Œæ·»åŠ é”™è¯¯å¤„ç†"""
        state = solver.env.reset()
        episode_reward = 0
        episode_steps = 0
        
        for step in range(solver.env.max_steps):
            try:
                # å‡†å¤‡çŠ¶æ€
                state_tensor = solver._prepare_state_tensor(state)
                
                # é€‰æ‹©åŠ¨ä½œ
                action = solver.select_action(state_tensor)
                action_idx = action.item()
                
                # éªŒè¯åŠ¨ä½œæœ‰æ•ˆæ€§
                if action_idx >= solver.env.n_actions:
                    # ä½¿ç”¨æ¨¡è¿ç®—è°ƒæ•´åŠ¨ä½œ
                    action_idx = action_idx % solver.env.n_actions
                    action = torch.tensor([[action_idx]], device=solver.device, dtype=torch.long)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                try:
                    next_state, reward, done, truncated, _ = solver.env.step(action_idx)
                except Exception as step_error:
                    # å¦‚æœstepå‡ºé”™ï¼Œè·³è¿‡è¿™ä¸€æ­¥
                    error_msg = str(step_error) if not isinstance(step_error, torch.Tensor) else f"Tensorå¼‚å¸¸: shape={step_error.shape}"
                    print(f"ç¯å¢ƒstepå‡ºé”™: {error_msg}")
                    continue
                
                episode_reward += reward
                episode_steps += 1
                
                # å­˜å‚¨ç»éªŒ
                next_state_tensor = solver._prepare_state_tensor(next_state)
                
                if solver.use_per:
                    solver.memory.push(
                        state_tensor, action,
                        torch.tensor([reward], device=solver.device),
                        next_state_tensor, done
                    )
                else:
                    solver.memory.append((
                        state_tensor, action,
                        torch.tensor([reward], device=solver.device),
                        next_state_tensor, done
                    ))
                
                # ä¼˜åŒ–æ¨¡å‹
                if len(solver.memory) >= solver.config.BATCH_SIZE:
                    solver.optimize_model()
                
                state = next_state
                
                if done or truncated:
                    break
                    
            except Exception as e:
                # å®‰å…¨åœ°å¤„ç†å¼‚å¸¸ä¿¡æ¯ï¼Œé¿å…æ‰“å°tensorå¯¹è±¡
                error_msg = str(e) if not isinstance(e, torch.Tensor) else f"Tensorå¼‚å¸¸: shape={e.shape}"
                print(f"è®­ç»ƒæ­¥éª¤ {step} å‡ºé”™: {error_msg}")
                # è·³è¿‡è¿™ä¸€æ­¥ï¼Œç»§ç»­è®­ç»ƒ
                continue
        
        # è®¡ç®—å®Œæˆç‡
        if solver.env.targets:
            completed_targets = sum(1 for target in solver.env.targets 
                                  if np.all(target.remaining_resources <= 0))
            completion_rate = completed_targets / len(solver.env.targets)
        else:
            completion_rate = 0
        
        return {
            'reward': episode_reward,
            'steps': episode_steps,
            'completion_rate': completion_rate,
            'n_uavs': len(solver.env.uavs),
            'n_targets': len(solver.env.targets)
        }
    
    def _evaluate_zero_shot_transfer(self, output_dir: str) -> Dict:
        """è¯„ä¼°é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›"""
        print(f"\nğŸ”¬ è¯„ä¼°é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›")
        print("-" * 40)
        
        # æµ‹è¯•åœºæ™¯
        test_scenarios = [
            (3, 4),   # å°è§„æ¨¡
            (6, 8),   # ä¸­è§„æ¨¡  
            (10, 12), # å¤§è§„æ¨¡
            (15, 18), # è¶…å¤§è§„æ¨¡
        ]
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = f"{output_dir}/phase_3_best_model.pth"  # æœ€åé˜¶æ®µçš„æœ€ä½³æ¨¡å‹
        
        transfer_results = []
        
        for n_uavs, n_targets in test_scenarios:
            print(f"æµ‹è¯•åœºæ™¯: {n_uavs} UAV, {n_targets} ç›®æ ‡")
            
            try:
                # åˆ›å»ºæµ‹è¯•åœºæ™¯
                uavs, targets, obstacles = self.create_dynamic_scenario(n_uavs, n_targets)
                graph = DirectedGraph(uavs, targets, self.config.GRAPH_N_PHI, obstacles, self.config)
                
                # åˆ›å»ºæµ‹è¯•solver
                test_solver = GraphRLSolver(
                    uavs, targets, graph, obstacles,
                    i_dim=1, h_dim=[256, 128], o_dim=1000,
                    config=self.config, network_type="ZeroShotGNN",
                    obs_mode="graph"
                )
                
                # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if os.path.exists(best_model_path):
                    test_solver.load_model(best_model_path)
                
                # æµ‹è¯•æ€§èƒ½
                test_rewards = []
                test_completion_rates = []
                
                for test_episode in range(10):  # æµ‹è¯•10è½®
                    state = test_solver.env.reset()
                    episode_reward = 0
                    
                    for step in range(test_solver.env.max_steps):
                        state_tensor = test_solver._prepare_state_tensor(state)
                        
                        with torch.no_grad():
                            test_solver.policy_net.eval()
                            q_values = test_solver.policy_net(state_tensor)
                            action = q_values.max(1)[1].view(1, 1)
                        
                        next_state, reward, done, truncated, _ = test_solver.env.step(action.item())
                        episode_reward += reward
                        state = next_state
                        
                        if done or truncated:
                            break
                    
                    test_rewards.append(episode_reward)
                    
                    # è®¡ç®—å®Œæˆç‡
                    completed = sum(1 for t in test_solver.env.targets 
                                  if np.all(t.remaining_resources <= 0))
                    completion_rate = completed / len(test_solver.env.targets)
                    test_completion_rates.append(completion_rate)
                
                avg_reward = np.mean(test_rewards)
                avg_completion = np.mean(test_completion_rates)
                
                transfer_results.append({
                    'scenario': (n_uavs, n_targets),
                    'avg_reward': avg_reward,
                    'avg_completion_rate': avg_completion,
                    'success': True
                })
                
                print(f"  âœ“ å¹³å‡å¥–åŠ±: {avg_reward:.2f}, å®Œæˆç‡: {avg_completion:.3f}")
                
            except Exception as e:
                transfer_results.append({
                    'scenario': (n_uavs, n_targets),
                    'success': False,
                    'error': str(e)
                })
                print(f"  âœ— å¤±è´¥: {str(e)}")
        
        # è®¡ç®—æ€»ä½“è¿ç§»å¾—åˆ†
        successful_results = [r for r in transfer_results if r['success']]
        if successful_results:
            avg_transfer_reward = np.mean([r['avg_reward'] for r in successful_results])
            avg_transfer_completion = np.mean([r['avg_completion_rate'] for r in successful_results])
            transfer_success_rate = len(successful_results) / len(test_scenarios)
        else:
            avg_transfer_reward = 0
            avg_transfer_completion = 0
            transfer_success_rate = 0
        
        print(f"\né›¶æ ·æœ¬è¿ç§»æ€»ç»“:")
        print(f"  - æˆåŠŸç‡: {transfer_success_rate:.1%}")
        print(f"  - å¹³å‡å¥–åŠ±: {avg_transfer_reward:.2f}")
        print(f"  - å¹³å‡å®Œæˆç‡: {avg_transfer_completion:.3f}")
        
        return {
            'test_scenarios': test_scenarios,
            'results': transfer_results,
            'summary': {
                'success_rate': transfer_success_rate,
                'avg_reward': avg_transfer_reward,
                'avg_completion_rate': avg_transfer_completion
            }
        }

def main():
    """
    ä¸»å‡½æ•° - æ”¯æŒæ‰€æœ‰ç½‘ç»œç±»å‹å’Œé›¶æ ·æœ¬è®­ç»ƒ
    
    è¿è¡Œæ¨¡å¼ï¼š
    1. normal: æ­£å¸¸è®­ç»ƒ/æµ‹è¯•æ¨¡å¼ï¼Œæ”¯æŒæ‰€æœ‰ç½‘ç»œç±»å‹
    2. zero_shot_train: é›¶æ ·æœ¬è®­ç»ƒæ¨¡å¼ï¼Œä¸“é—¨ç”¨äºZeroShotGNN
    """
    print("å¤šæ— äººæœºååŒä»»åŠ¡åˆ†é…ä¸è·¯å¾„è§„åˆ’ç³»ç»Ÿ - å¢å¼ºç‰ˆ")
    print("=" * 60)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    import argparse
    parser = argparse.ArgumentParser(description='UAVä»»åŠ¡åˆ†é…ç³»ç»Ÿ')
    parser.add_argument('--mode', choices=['normal', 'zero_shot_train'], 
                       default='normal', help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--network', choices=['SimpleNetwork', 'DeepFCN', 'DeepFCNResidual', 'ZeroShotGNN'],
                       default='ZeroShotGNN', help='ç½‘ç»œç±»å‹')
    parser.add_argument('--scenario', choices=['small', 'balanced', 'complex', 'experimental', 'strategic_trap'],
                       default='experimental', help='åœºæ™¯ç±»å‹')
    parser.add_argument('--episodes', type=int, default=1000, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--force_retrain', action='store_true', help='å¼ºåˆ¶é‡æ–°è®­ç»ƒ')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = Config()
    config.NETWORK_TYPE = args.network
    
    print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"ç½‘ç»œç±»å‹: {args.network}")
    print(f"åœºæ™¯ç±»å‹: {args.scenario}")
    
    # === é›¶æ ·æœ¬è®­ç»ƒæ¨¡å¼ ===
    if args.mode == 'zero_shot_train':
        if args.network != 'ZeroShotGNN':
            print("âš ï¸  é›¶æ ·æœ¬è®­ç»ƒæ¨¡å¼åªæ”¯æŒZeroShotGNNç½‘ç»œ")
            print("è‡ªåŠ¨åˆ‡æ¢åˆ°ZeroShotGNNç½‘ç»œ...")
            args.network = 'ZeroShotGNN'
            config.NETWORK_TYPE = 'ZeroShotGNN'
        
        try:
            from zero_shot_trainer import ZeroShotTrainer
            from zero_shot_training_strategy import create_zero_shot_training_config
            
            print("\nğŸš€ å¯åŠ¨é›¶æ ·æœ¬è®­ç»ƒæ¨¡å¼")
            print("-" * 40)
            
            # åˆ›å»ºé›¶æ ·æœ¬è®­ç»ƒé…ç½®
            zero_shot_config = create_zero_shot_training_config(config)
            for key, value in zero_shot_config.items():
                if hasattr(config, key):
                    setattr(config, key, value)
            
            # å¼€å§‹é›¶æ ·æœ¬è®­ç»ƒ
            trainer = ZeroShotTrainer(config)
            results = trainer.train_with_strategy("output/zero_shot_training")
            
            print(f"\nğŸ‰ é›¶æ ·æœ¬è®­ç»ƒå®Œæˆ!")
            print(f"æœ€ç»ˆè¿ç§»å¾—åˆ†: {results['final_transfer_score']:.3f}")
            print(f"è®­ç»ƒæ€»æ—¶é—´: {results['total_time']/3600:.2f} å°æ—¶")
            print(f"æ€»è®­ç»ƒè½®æ•°: {results['total_episodes']}")
            
            return results
            
        except ImportError as e:
            print(f"âŒ é›¶æ ·æœ¬è®­ç»ƒæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿ zero_shot_trainer.py å’Œ zero_shot_training_strategy.py æ–‡ä»¶å­˜åœ¨")
            return None
        except Exception as e:
            print(f"âŒ é›¶æ ·æœ¬è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    # === æ­£å¸¸è®­ç»ƒ/æµ‹è¯•æ¨¡å¼ ===
    print(f"\nğŸ”§ å¯åŠ¨æ­£å¸¸è®­ç»ƒ/æµ‹è¯•æ¨¡å¼")
    print("-" * 40)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    print("æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
    cleanup_temp_files()
    
    # åŠ è½½åœºæ™¯æ•°æ®
    print("åŠ è½½åœºæ™¯æ•°æ®...")
    
    # æ ¹æ®åœºæ™¯ç±»å‹é€‰æ‹©åœºæ™¯
    scenario_functions = {
        'small': lambda: get_small_scenario(obstacle_tolerance=50.0),
        'balanced': lambda: get_balanced_scenario(obstacle_tolerance=50.0),
        'complex': lambda: get_complex_scenario(obstacle_tolerance=50.0),
        'experimental': lambda: get_new_experimental_scenario(obstacle_tolerance=50.0),
        'strategic_trap': lambda: get_strategic_trap_scenario(obstacle_tolerance=50.0)
    }
    
    try:
        if args.scenario in scenario_functions:
            base_uavs, base_targets, obstacles = scenario_functions[args.scenario]()
        else:
            print(f"âš ï¸  æœªçŸ¥åœºæ™¯ç±»å‹: {args.scenario}ï¼Œä½¿ç”¨é»˜è®¤å®éªŒåœºæ™¯")
            base_uavs, base_targets, obstacles = get_new_experimental_scenario(obstacle_tolerance=50.0)
        
        print(f"åœºæ™¯åŠ è½½æˆåŠŸ: {len(base_uavs)} UAV, {len(base_targets)} ç›®æ ‡, {len(obstacles)} éšœç¢ç‰©")
        
    except Exception as e:
        print(f"âŒ åœºæ™¯åŠ è½½å¤±è´¥: {e}")
        print("ä½¿ç”¨é»˜è®¤å°è§„æ¨¡åœºæ™¯...")
        base_uavs, base_targets, obstacles = get_small_scenario(obstacle_tolerance=50.0)
    
    # ç½‘ç»œç±»å‹ç‰¹å®šé…ç½®
    network_configs = {
        'SimpleNetwork': {
            'description': 'åŸºç¡€å…¨è¿æ¥ç½‘ç»œ',
            'obs_mode': 'flat',
            'recommended_episodes': 800,
            'features': ['BatchNorm', 'Dropout', 'Xavieråˆå§‹åŒ–']
        },
        'DeepFCN': {
            'description': 'æ·±åº¦å…¨è¿æ¥ç½‘ç»œ',
            'obs_mode': 'flat', 
            'recommended_episodes': 1000,
            'features': ['å¤šå±‚ç»“æ„', 'BatchNorm', 'Dropout']
        },
        'DeepFCNResidual': {
            'description': 'å¸¦æ®‹å·®è¿æ¥çš„æ·±åº¦ç½‘ç»œ',
            'obs_mode': 'flat',
            'recommended_episodes': 1200,
            'features': ['æ®‹å·®è¿æ¥', 'SEæ³¨æ„åŠ›', 'BatchNorm']
        },
        'ZeroShotGNN': {
            'description': 'é›¶æ ·æœ¬å›¾ç¥ç»ç½‘ç»œ',
            'obs_mode': 'graph',
            'recommended_episodes': 1500,
            'features': ['Transformeræ¶æ„', 'è‡ªæ³¨æ„åŠ›', 'äº¤å‰æ³¨æ„åŠ›', 'å‚æ•°å…±äº«', 'é›¶æ ·æœ¬è¿ç§»']
        }
    }
    
    # è·å–ç½‘ç»œé…ç½®
    net_config = network_configs.get(args.network, network_configs['ZeroShotGNN'])
    
    print(f"\nğŸ“Š ç½‘ç»œä¿¡æ¯:")
    print(f"  - ç±»å‹: {args.network}")
    print(f"  - æè¿°: {net_config['description']}")
    print(f"  - è§‚æµ‹æ¨¡å¼: {net_config['obs_mode']}")
    print(f"  - æ¨èè®­ç»ƒè½®æ•°: {net_config['recommended_episodes']}")
    print(f"  - ç‰¹æ€§: {', '.join(net_config['features'])}")
    
    # è°ƒæ•´è®­ç»ƒè½®æ•°
    if args.episodes == 1000:  # å¦‚æœä½¿ç”¨é»˜è®¤å€¼ï¼Œåˆ™é‡‡ç”¨æ¨èå€¼
        training_episodes = net_config['recommended_episodes']
        print(f"  - ä½¿ç”¨æ¨èè®­ç»ƒè½®æ•°: {training_episodes}")
    else:
        training_episodes = args.episodes
        print(f"  - ä½¿ç”¨æŒ‡å®šè®­ç»ƒè½®æ•°: {training_episodes}")
    
    # è¿è¡Œåœºæ™¯
    try:
        print(f"\nğŸ¯ å¼€å§‹è¿è¡Œåœºæ™¯...")
        
        final_plan, training_time, training_history, evaluation_metrics = run_scenario(
            config=config,
            base_uavs=base_uavs,
            base_targets=base_targets,
            obstacles=obstacles,
            scenario_name=f"{args.scenario}åœºæ™¯",
            network_type=args.network,
            save_visualization=True,
            show_visualization=False,  # åœ¨æ‰¹é‡æµ‹è¯•æ—¶å…³é—­æ˜¾ç¤º
            save_report=True,
            force_retrain=args.force_retrain,
            incremental_training=False,
            output_base_dir=None
        )
        
        # è®­ç»ƒç»“æœæ€»ç»“
        print(f"\nğŸ“ˆ è®­ç»ƒç»“æœæ€»ç»“:")
        print(f"  - ç½‘ç»œç±»å‹: {args.network}")
        print(f"  - åœºæ™¯ç±»å‹: {args.scenario}")
        print(f"  - è®­ç»ƒæ—¶é—´: {training_time/60:.2f} åˆ†é’Ÿ")
        
        if training_history:
            print(f"  - è®­ç»ƒè½®æ•°: {len(training_history.get('episode_rewards', []))}")
            if training_history.get('episode_rewards'):
                avg_reward = np.mean(training_history['episode_rewards'][-100:])  # æœ€å100è½®å¹³å‡
                print(f"  - æœ€ç»ˆå¹³å‡å¥–åŠ±: {avg_reward:.2f}")
            if training_history.get('completion_rates'):
                avg_completion = np.mean(training_history['completion_rates'][-100:])
                print(f"  - æœ€ç»ˆå®Œæˆç‡: {avg_completion:.3f}")
        
        if evaluation_metrics:
            print(f"  - è¯„ä¼°æŒ‡æ ‡: {evaluation_metrics}")
        
        # ç½‘ç»œç‰¹å®šçš„æ€§èƒ½åˆ†æ
        if args.network == 'ZeroShotGNN':
            print(f"\nğŸ”¬ ZeroShotGNN ç‰¹æ€§åˆ†æ:")
            print(f"  - æ”¯æŒå¯å˜æ•°é‡å®ä½“: âœ“")
            print(f"  - é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›: âœ“")
            print(f"  - å›¾ç»“æ„è§‚æµ‹: âœ“")
            print(f"  - Transformeræ³¨æ„åŠ›: âœ“")
            
            # å»ºè®®è¿›è¡Œé›¶æ ·æœ¬è®­ç»ƒ
            if not args.force_retrain:
                print(f"\nğŸ’¡ å»ºè®®:")
                print(f"  - å¯¹äºZeroShotGNNï¼Œå»ºè®®ä½¿ç”¨é›¶æ ·æœ¬è®­ç»ƒæ¨¡å¼è·å¾—æ›´å¥½çš„è¿ç§»èƒ½åŠ›")
                print(f"  - è¿è¡Œå‘½ä»¤: python main.py --mode zero_shot_train --network ZeroShotGNN")
        
        elif args.network in ['SimpleNetwork', 'DeepFCN', 'DeepFCNResidual']:
            print(f"\nğŸ”¬ ä¼ ç»Ÿç½‘ç»œç‰¹æ€§åˆ†æ:")
            print(f"  - å›ºå®šè¾“å…¥ç»´åº¦: âœ“")
            print(f"  - æ‰å¹³å‘é‡è§‚æµ‹: âœ“")
            print(f"  - åœºæ™¯ç‰¹å®šè®­ç»ƒ: âœ“")
            
            if args.network == 'DeepFCNResidual':
                print(f"  - æ®‹å·®è¿æ¥: âœ“")
                print(f"  - SEæ³¨æ„åŠ›æœºåˆ¶: âœ“")
        
        # æ€§èƒ½å¯¹æ¯”å»ºè®®
        print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”å»ºè®®:")
        print(f"  - å½“å‰ç½‘ç»œ: {args.network}")
        print(f"  - å¦‚éœ€å¯¹æ¯”å…¶ä»–ç½‘ç»œï¼Œå¯è¿è¡Œ:")
        for net_type in ['SimpleNetwork', 'DeepFCN', 'DeepFCNResidual', 'ZeroShotGNN']:
            if net_type != args.network:
                print(f"    python main.py --network {net_type} --scenario {args.scenario}")
        
        return {
            'network_type': args.network,
            'scenario_type': args.scenario,
            'training_time': training_time,
            'training_history': training_history,
            'evaluation_metrics': evaluation_metrics,
            'final_plan': final_plan
        }
        
    except Exception as e:
        print(f"âŒ åœºæ™¯è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

# æ·»åŠ è¾…åŠ©å‡½æ•°
def cleanup_temp_files():
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    import glob
    temp_patterns = [
        "*.tmp",
        "temp_*",
        "__pycache__/*",
        "*.pyc"
    ]
    
    for pattern in temp_patterns:
        for file in glob.glob(pattern):
            try:
                if os.path.isfile(file):
                    os.remove(file)
                elif os.path.isdir(file):
                    import shutil
                    shutil.rmtree(file)
            except:
                pass

def print_network_comparison():
    """æ‰“å°ç½‘ç»œå¯¹æ¯”ä¿¡æ¯"""
    print("\nğŸ“‹ æ”¯æŒçš„ç½‘ç»œç±»å‹å¯¹æ¯”:")
    print("-" * 80)
    print(f"{'ç½‘ç»œç±»å‹':<20} {'è§‚æµ‹æ¨¡å¼':<10} {'å¤æ‚åº¦':<8} {'ç‰¹è‰²åŠŸèƒ½':<30}")
    print("-" * 80)
    print(f"{'SimpleNetwork':<20} {'flat':<10} {'ä½':<8} {'åŸºç¡€å…¨è¿æ¥ï¼Œå¿«é€Ÿè®­ç»ƒ':<30}")
    print(f"{'DeepFCN':<20} {'flat':<10} {'ä¸­':<8} {'æ·±åº¦ç½‘ç»œï¼Œæ›´å¼ºè¡¨è¾¾èƒ½åŠ›':<30}")
    print(f"{'DeepFCNResidual':<20} {'flat':<10} {'ä¸­':<8} {'æ®‹å·®è¿æ¥ï¼ŒSEæ³¨æ„åŠ›':<30}")
    print(f"{'ZeroShotGNN':<20} {'graph':<10} {'é«˜':<8} {'é›¶æ ·æœ¬è¿ç§»ï¼ŒTransformer':<30}")
    print("-" * 80)

if __name__ == "__main__":
    # åœ¨ç¨‹åºå¼€å§‹æ—¶æ˜¾ç¤ºç½‘ç»œå¯¹æ¯”ä¿¡æ¯
    print_network_comparison()
    
    # è¿è¡Œä¸»ç¨‹åº
    result = main()
    
    if result:
        print(f"\nâœ… ç¨‹åºæ‰§è¡Œå®Œæˆ")
    else:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥")
        sys.exit(1)
