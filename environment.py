# -*- coding: utf-8 -*-
# æ–‡ä»¶å: environment.py
# æè¿°: å®šä¹‰å¼ºåŒ–å­¦ä¹ çš„ç¯å¢ƒï¼ŒåŒ…æ‹¬åœºæ™¯çš„æœ‰å‘å›¾è¡¨ç¤ºå’Œä»»åŠ¡ç¯å¢ƒæœ¬èº«ã€‚

import numpy as np
import itertools
from scipy.spatial.distance import cdist
from typing import Union, Dict, Any, Literal
import gymnasium as gym
from gymnasium import spaces

from entities import UAV, Target
from path_planning import PHCurveRRTPlanner

# =============================================================================
# section 3: åœºæ™¯å»ºæ¨¡ä¸å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
# =============================================================================

class DirectedGraph:
    """(å·²ä¿®è®¢) ä½¿ç”¨numpyé«˜æ•ˆæ„å»ºå’Œç®¡ç†ä»»åŠ¡åœºæ™¯çš„æœ‰å‘å›¾"""
    def __init__(self, uavs, targets, n_phi, obstacles, config):
        self.uavs, self.targets, self.config = uavs, targets, config
        self.n_phi = n_phi
        self.n_uavs, self.n_targets = len(uavs), len(targets)
        self.uav_positions = np.array([u.position for u in uavs])
        self.target_positions = np.array([t.position for t in targets])
        
        self.nodes = uavs + targets
        self.node_positions = np.vstack([self.uav_positions, self.target_positions])
        self.node_map = {node.id: i for i, node in enumerate(self.nodes)}

        self.dist_matrix = self._calculate_distances(obstacles)
        self.adj_matrix = self._build_adjacency_matrix()
        self.phi_matrix = self._calculate_phi_matrix()

    def _calculate_distances(self, obstacles):
        """è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹é—´çš„è·ç¦»ï¼Œå¯é€‰åœ°ä½¿ç”¨PH-RRTå¤„ç†éšœç¢ç‰©"""
        dist_matrix = cdist(self.node_positions, self.node_positions)
        if hasattr(self.config, 'USE_PHRRT_DURING_TRAINING') and self.config.USE_PHRRT_DURING_TRAINING and obstacles:
            for i, j in itertools.product(range(len(self.nodes)), repeat=2):
                if i == j: continue
                p1, p2 = self.node_positions[i], self.node_positions[j]
                planner = PHCurveRRTPlanner(p1, p2, 0, 0, obstacles, self.config)
                path_info = planner.plan()
                if path_info: dist_matrix[i, j] = path_info[1]
        return dist_matrix

    def _build_adjacency_matrix(self):
        """æ„å»ºé‚»æ¥çŸ©é˜µï¼ŒUAVå¯ä»¥é£åˆ°ä»»ä½•ç›®æ ‡ï¼Œç›®æ ‡ä¹‹é—´ä¸èƒ½äº’é£"""
        adj = np.zeros((len(self.nodes), len(self.nodes)))
        adj[:self.n_uavs, self.n_uavs:] = 1
        return adj

    def _calculate_phi_matrix(self):
        """(å·²ä¿®è®¢) é«˜æ•ˆè®¡ç®—æ‰€æœ‰èŠ‚ç‚¹å¯¹ä¹‹é—´çš„ç›¸å¯¹æ–¹å‘åˆ†åŒº(phiå€¼)"""
        delta = self.node_positions[:, np.newaxis, :] - self.node_positions[np.newaxis, :, :]
        angles = np.arctan2(delta[..., 1], delta[..., 0])
        phi_matrix = np.floor((angles % (2 * np.pi)) / (2 * np.pi / self.config.GRAPH_N_PHI))
        return phi_matrix.astype(int)

    def get_dist(self, from_node_id, to_node_id):
        """è·å–ä¸¤ä¸ªèŠ‚ç‚¹é—´çš„è·ç¦»"""
        return self.dist_matrix[self.node_map[from_node_id], self.node_map[to_node_id]]

class UAVTaskEnv:
    """
    (å·²ä¿®è®¢) æ— äººæœºååŒä»»åŠ¡åˆ†é…çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ
    
    æ”¯æŒåŒæ¨¡å¼è§‚æµ‹ç³»ç»Ÿï¼š
    - "flat" æ¨¡å¼ï¼šä¼ ç»Ÿæ‰å¹³å‘é‡è§‚æµ‹ï¼Œç¡®ä¿FCNå‘åå…¼å®¹æ€§
    - "graph" æ¨¡å¼ï¼šç»“æ„åŒ–å›¾è§‚æµ‹ï¼Œæ”¯æŒTransformerGNNæ¶æ„å’Œå¯å˜æ•°é‡å®ä½“
    """
    def __init__(self, uavs, targets, graph, obstacles, config, obs_mode: Literal["flat", "graph"] = "flat"):
        """
        åˆå§‹åŒ–UAVä»»åŠ¡ç¯å¢ƒ
        
        Args:
            uavs: UAVå®ä½“åˆ—è¡¨
            targets: ç›®æ ‡å®ä½“åˆ—è¡¨  
            graph: æœ‰å‘å›¾å¯¹è±¡
            obstacles: éšœç¢ç‰©åˆ—è¡¨
            config: é…ç½®å¯¹è±¡
            obs_mode: è§‚æµ‹æ¨¡å¼ï¼Œ"flat"ä¸ºæ‰å¹³å‘é‡æ¨¡å¼ï¼Œ"graph"ä¸ºå›¾ç»“æ„æ¨¡å¼
        """
        self.uavs = uavs
        self.targets = targets
        self.graph = graph
        self.obstacles = obstacles
        self.config = config
        self.obs_mode = obs_mode
        self.step_count = 0
        self.max_steps = len(targets) * len(uavs) * 2
        self.invalid_action_penalty = -5.0  # ä»-75.0å¤§å¹…å‡å°‘åˆ°-5.0
        
        # è®¡ç®—åŠ¨ä½œç©ºé—´å¤§å°
        self.n_actions = len(targets) * len(uavs) * self.graph.n_phi
        
        # åŠ¨æ€åˆ›å»ºè§‚æµ‹ç©ºé—´
        self.observation_space = self._create_observation_space()
        
        # å®šä¹‰åŠ¨ä½œç©ºé—´
        self.action_space = spaces.Discrete(self.n_actions)
    
    def _create_observation_space(self) -> spaces.Space:
        """
        åŠ¨æ€è§‚æµ‹ç©ºé—´åˆ›å»ºçš„å·¥å‚æ¨¡å¼
        
        æ ¹æ®obs_modeå‚æ•°åˆ›å»ºç›¸åº”çš„è§‚æµ‹ç©ºé—´ï¼š
        - "flat": æ‰å¹³å‘é‡è§‚æµ‹ç©ºé—´ï¼Œç¡®ä¿FCNå‘åå…¼å®¹æ€§
        - "graph": å­—å…¸ç»“æ„è§‚æµ‹ç©ºé—´ï¼Œæ”¯æŒå¯å˜æ•°é‡å®ä½“
        
        Returns:
            gym.spaces.Space: å¯¹åº”æ¨¡å¼çš„è§‚æµ‹ç©ºé—´
        """
        if self.obs_mode == "flat":
            return self._create_flat_observation_space()
        elif self.obs_mode == "graph":
            return self._create_graph_observation_space()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è§‚æµ‹æ¨¡å¼: {self.obs_mode}ã€‚æ”¯æŒçš„æ¨¡å¼: ['flat', 'graph']")
    
    def _create_flat_observation_space(self) -> spaces.Box:
        """
        åˆ›å»ºæ‰å¹³å‘é‡è§‚æµ‹ç©ºé—´ï¼Œç»´æŒç°æœ‰å®ç°çš„å‘åå…¼å®¹æ€§
        
        çŠ¶æ€ç»„æˆï¼š
        - ç›®æ ‡ä¿¡æ¯ï¼šposition(2) + resources(2) + value(1) + remaining_resources(2) = 7 * n_targets
        - UAVä¿¡æ¯ï¼šposition(2) + heading(1) + resources(2) + max_distance(1) + velocity_range(2) = 8 * n_uavs  
        - ååŒä¿¡æ¯ï¼šåˆ†é…çŠ¶æ€ = 1 * n_targets * n_uavs
        - å…¨å±€ä¿¡æ¯ï¼š10ä¸ªå…¨å±€çŠ¶æ€ç‰¹å¾
        
        Returns:
            spaces.Box: æ‰å¹³å‘é‡è§‚æµ‹ç©ºé—´
        """
        n_targets = len(self.targets)
        n_uavs = len(self.uavs)
        
        # è®¡ç®—çŠ¶æ€ç»´åº¦
        target_dim = 7 * n_targets  # æ¯ä¸ªç›®æ ‡7ä¸ªç‰¹å¾
        uav_dim = 8 * n_uavs        # æ¯ä¸ªUAV 8ä¸ªç‰¹å¾
        collaboration_dim = n_targets * n_uavs  # ååŒåˆ†é…çŠ¶æ€
        global_dim = 10             # å…¨å±€çŠ¶æ€ç‰¹å¾
        
        total_dim = target_dim + uav_dim + collaboration_dim + global_dim
        
        # åˆ›å»ºè§‚æµ‹ç©ºé—´ï¼Œä½¿ç”¨åˆç†çš„è¾¹ç•Œå€¼
        return spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(total_dim,),
            dtype=np.float32
        )
    
    def _create_graph_observation_space(self) -> spaces.Dict:
        """
        åˆ›å»ºå›¾ç»“æ„è§‚æµ‹ç©ºé—´ï¼Œæ”¯æŒå¯å˜æ•°é‡å®ä½“
        
        å›¾æ¨¡å¼çŠ¶æ€ç»“æ„ï¼š
        - uav_features: [N_uav, uav_feature_dim] - UAVå®ä½“ç‰¹å¾ï¼ˆå½’ä¸€åŒ–ï¼‰
        - target_features: [N_target, target_feature_dim] - ç›®æ ‡å®ä½“ç‰¹å¾ï¼ˆå½’ä¸€åŒ–ï¼‰
        - relative_positions: [N_uav, N_target, 2] - å½’ä¸€åŒ–ç›¸å¯¹ä½ç½®å‘é‡
        - distances: [N_uav, N_target] - å½’ä¸€åŒ–è·ç¦»çŸ©é˜µ
        - masks: æœ‰æ•ˆå®ä½“æ©ç å­—å…¸
        
        Returns:
            spaces.Dict: å›¾ç»“æ„è§‚æµ‹ç©ºé—´
        """
        n_uavs = len(self.uavs)
        n_targets = len(self.targets)
        
        # UAVç‰¹å¾ç»´åº¦ï¼šposition(2) + heading(1) + resources_ratio(2) + max_distance_norm(1) + 
        #              velocity_norm(2) + is_alive(1) = 9
        uav_feature_dim = 9
        
        # ç›®æ ‡ç‰¹å¾ç»´åº¦ï¼šposition(2) + resources_ratio(2) + value_norm(1) + 
        #              remaining_ratio(2) + is_visible(1) = 8  
        target_feature_dim = 8
        
        return spaces.Dict({
            # UAVå®ä½“ç‰¹å¾çŸ©é˜µ [N_uav, uav_feature_dim]
            "uav_features": spaces.Box(
                low=0.0, high=1.0,
                shape=(n_uavs, uav_feature_dim),
                dtype=np.float32
            ),
            
            # ç›®æ ‡å®ä½“ç‰¹å¾çŸ©é˜µ [N_target, target_feature_dim]
            "target_features": spaces.Box(
                low=0.0, high=1.0,
                shape=(n_targets, target_feature_dim),
                dtype=np.float32
            ),
            
            # ç›¸å¯¹ä½ç½®çŸ©é˜µ [N_uav, N_target, 2] - å½’ä¸€åŒ–ç›¸å¯¹ä½ç½®å‘é‡
            "relative_positions": spaces.Box(
                low=-1.0, high=1.0,
                shape=(n_uavs, n_targets, 2),
                dtype=np.float32
            ),
            
            # è·ç¦»çŸ©é˜µ [N_uav, N_target] - å½’ä¸€åŒ–è·ç¦»
            "distances": spaces.Box(
                low=0.0, high=1.0,
                shape=(n_uavs, n_targets),
                dtype=np.float32
            ),
            
            # æ©ç å­—å…¸
            "masks": spaces.Dict({
                # UAVæœ‰æ•ˆæ€§æ©ç  [N_uav] - 1è¡¨ç¤ºæœ‰æ•ˆï¼Œ0è¡¨ç¤ºæ— æ•ˆ
                "uav_mask": spaces.Box(
                    low=0, high=1,
                    shape=(n_uavs,),
                    dtype=np.int32
                ),
                
                # ç›®æ ‡æœ‰æ•ˆæ€§æ©ç  [N_target] - 1è¡¨ç¤ºæœ‰æ•ˆï¼Œ0è¡¨ç¤ºæ— æ•ˆ
                "target_mask": spaces.Box(
                    low=0, high=1,
                    shape=(n_targets,),
                    dtype=np.int32
                )
            })
        })
        
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        for uav in self.uavs:
            uav.reset()
        for target in self.targets:
            target.reset()
        self.step_count = 0
        return self._get_state()

    def _get_state(self) -> Union[np.ndarray, Dict[str, Any]]:
        """
        è·å–å½“å‰çŠ¶æ€ï¼Œæ ¹æ®obs_modeè¿”å›ä¸åŒæ ¼å¼
        
        Returns:
            Union[np.ndarray, Dict]: 
                - "flat"æ¨¡å¼ï¼šæ‰å¹³å‘é‡çŠ¶æ€
                - "graph"æ¨¡å¼ï¼šç»“æ„åŒ–å›¾çŠ¶æ€å­—å…¸
        """
        if self.obs_mode == "flat":
            return self._get_flat_state()
        elif self.obs_mode == "graph":
            return self._get_graph_state()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è§‚æµ‹æ¨¡å¼: {self.obs_mode}")
    
    def _get_flat_state(self) -> np.ndarray:
        """
        è·å–æ‰å¹³å‘é‡çŠ¶æ€ï¼Œç»´æŒç°æœ‰å®ç°çš„å‘åå…¼å®¹æ€§
        
        Returns:
            np.ndarray: æ‰å¹³å‘é‡çŠ¶æ€
        """
        state = []
        
        # ç›®æ ‡ä¿¡æ¯
        for target in self.targets:
            target_state = [
                target.position[0], target.position[1],
                target.resources[0], target.resources[1],
                target.value,
                target.remaining_resources[0], target.remaining_resources[1]
            ]
            state.extend(target_state)
        
        # UAVä¿¡æ¯
        for uav in self.uavs:
            uav_state = [
                uav.current_position[0], uav.current_position[1],
                uav.heading,
                uav.resources[0], uav.resources[1],
                uav.max_distance,
                uav.velocity_range[0], uav.velocity_range[1]
            ]
            state.extend(uav_state)
        
        # ååŒä¿¡æ¯
        for target in self.targets:
            for uav in self.uavs:
                is_assigned = any(
                    (uav.id, phi_idx) in target.allocated_uavs 
                    for phi_idx in range(self.graph.n_phi)
                )
                state.append(1.0 if is_assigned else 0.0)
        
        # å…¨å±€çŠ¶æ€ä¿¡æ¯
        total_targets = len(self.targets)
        completed_targets = sum(
            1 for target in self.targets 
            if np.all(target.remaining_resources <= 0)
        )
        completion_rate = completed_targets / total_targets if total_targets > 0 else 0.0
        
        global_state = [
            self.step_count,
            completion_rate,
            len([u for u in self.uavs if np.any(u.resources > 0)]),
            sum(np.sum(target.remaining_resources) for target in self.targets),
            sum(np.sum(uav.resources) for uav in self.uavs),
            completed_targets,
            total_targets,
            self.max_steps - self.step_count,
            np.mean([uav.heading for uav in self.uavs]),
            np.std([uav.heading for uav in self.uavs])
        ]
        state.extend(global_state)
        
        return np.array(state, dtype=np.float32)
    
    def _get_graph_state(self) -> Dict[str, Any]:
        """
        è·å–å›¾ç»“æ„çŠ¶æ€ï¼Œæ”¯æŒTransformerGNNæ¶æ„
        
        å®ç°å°ºåº¦ä¸å˜çš„çŠ¶æ€è¡¨ç¤ºï¼š
        - ç§»é™¤ç»å¯¹åæ ‡ï¼Œä½¿ç”¨å½’ä¸€åŒ–ç›¸å¯¹ä½ç½®
        - å®ä½“ç‰¹å¾ä»…åŒ…å«å½’ä¸€åŒ–çš„è‡ªèº«å±æ€§
        - æ·»åŠ é²æ£’æ€§æ©ç æœºåˆ¶ï¼Œæ”¯æŒé€šä¿¡/æ„ŸçŸ¥å¤±æ•ˆåœºæ™¯
        - ä½¿ç”¨å›ºå®šç»´åº¦ç¡®ä¿æ‰¹å¤„ç†å…¼å®¹æ€§
        
        Returns:
            Dict[str, Any]: å›¾ç»“æ„çŠ¶æ€å­—å…¸
        """
        # ä½¿ç”¨å›ºå®šçš„æœ€å¤§æ•°é‡ï¼Œç¡®ä¿ç»´åº¦ä¸€è‡´æ€§
        max_uavs = getattr(self.config, 'MAX_UAVS', 10)
        max_targets = getattr(self.config, 'MAX_TARGETS', 15)
        
        n_uavs = len(self.uavs)
        n_targets = len(self.targets)
        
        # è®¡ç®—åœ°å›¾å°ºåº¦ç”¨äºå½’ä¸€åŒ–ï¼ˆå‡è®¾åœ°å›¾ä¸ºæ­£æ–¹å½¢ï¼‰
        map_size = getattr(self.config, 'MAP_SIZE', 1000.0)
        
        # === 1. UAVç‰¹å¾çŸ©é˜µ [max_uavs, uav_feature_dim] ===
        uav_features = np.zeros((max_uavs, 9), dtype=np.float32)
        
        for i, uav in enumerate(self.uavs):
            # å½’ä¸€åŒ–ä½ç½® [0, 1]
            norm_pos = np.array(uav.current_position) / map_size
            
            # å½’ä¸€åŒ–æœå‘ [0, 1]
            norm_heading = uav.heading / (2 * np.pi)
            
            # èµ„æºæ¯”ä¾‹ [0, 1]
            initial_resources = getattr(uav, 'initial_resources', uav.resources + 1e-6)
            resource_ratio = uav.resources / (initial_resources + 1e-6)
            
            # å½’ä¸€åŒ–æœ€å¤§è·ç¦» [0, 1]
            norm_max_distance = uav.max_distance / map_size
            
            # å½’ä¸€åŒ–é€Ÿåº¦èŒƒå›´ [0, 1]
            max_velocity = 100.0  # å‡è®¾æœ€å¤§é€Ÿåº¦
            norm_velocity = np.array(uav.velocity_range) / max_velocity
            
            # é²æ£’æ€§æ©ç ï¼šis_aliveä½ï¼ˆ0/1ï¼‰ï¼Œæ ‡è¯†æ— äººæœºé€šä¿¡/æ„ŸçŸ¥çŠ¶æ€
            is_alive = self._calculate_uav_alive_status(uav, i)
            
            uav_features[i] = [
                norm_pos[0], norm_pos[1],           # å½’ä¸€åŒ–ä½ç½® (2)
                norm_heading,                       # å½’ä¸€åŒ–æœå‘ (1)
                resource_ratio[0], resource_ratio[1], # èµ„æºæ¯”ä¾‹ (2)
                norm_max_distance,                  # å½’ä¸€åŒ–æœ€å¤§è·ç¦» (1)
                norm_velocity[0], norm_velocity[1], # å½’ä¸€åŒ–é€Ÿåº¦ (2)
                is_alive                           # å­˜æ´»çŠ¶æ€ (1)
            ]
        
        # === 2. ç›®æ ‡ç‰¹å¾çŸ©é˜µ [max_targets, target_feature_dim] ===
        target_features = np.zeros((max_targets, 8), dtype=np.float32)
        
        for i, target in enumerate(self.targets):
            # å½’ä¸€åŒ–ä½ç½® [0, 1]
            norm_pos = np.array(target.position) / map_size
            
            # èµ„æºæ¯”ä¾‹ [0, 1]
            initial_resources = target.resources + 1e-6
            resource_ratio = target.resources / initial_resources
            
            # å½’ä¸€åŒ–ä»·å€¼ [0, 1]ï¼ˆå‡è®¾æœ€å¤§ä»·å€¼ä¸º1000ï¼‰
            max_value = 1000.0
            norm_value = min(target.value / max_value, 1.0)
            
            # å‰©ä½™èµ„æºæ¯”ä¾‹ [0, 1]
            remaining_ratio = target.remaining_resources / initial_resources
            
            # é²æ£’æ€§æ©ç ï¼šis_visibleä½ï¼ˆ0/1ï¼‰ï¼Œæ ‡è¯†ç›®æ ‡å¯è§æ€§çŠ¶æ€
            is_visible = self._calculate_target_visibility_status(target, i)
            
            target_features[i] = [
                norm_pos[0], norm_pos[1],                    # å½’ä¸€åŒ–ä½ç½® (2)
                resource_ratio[0], resource_ratio[1],        # èµ„æºæ¯”ä¾‹ (2)
                norm_value,                                  # å½’ä¸€åŒ–ä»·å€¼ (1)
                remaining_ratio[0], remaining_ratio[1],      # å‰©ä½™èµ„æºæ¯”ä¾‹ (2)
                is_visible                                   # å¯è§æ€§çŠ¶æ€ (1)
            ]
        
        # === 3. ç›¸å¯¹ä½ç½®çŸ©é˜µ [max_uavs, max_targets, 2] ===
        relative_positions = np.zeros((max_uavs, max_targets, 2), dtype=np.float32)
        
        for i, uav in enumerate(self.uavs):
            for j, target in enumerate(self.targets):
                # è®¡ç®—ç›¸å¯¹ä½ç½®å‘é‡ (pos_target - pos_uav)
                rel_pos = np.array(target.position) - np.array(uav.current_position)
                # å½’ä¸€åŒ–åˆ° [-1, 1] èŒƒå›´
                relative_positions[i, j] = rel_pos / map_size
        
        # === 4. è·ç¦»çŸ©é˜µ [max_uavs, max_targets] ===
        distances = np.zeros((max_uavs, max_targets), dtype=np.float32)
        
        for i, uav in enumerate(self.uavs):
            for j, target in enumerate(self.targets):
                # è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»å¹¶å½’ä¸€åŒ–
                dist = np.linalg.norm(
                    np.array(target.position) - np.array(uav.current_position)
                )
                distances[i, j] = min(dist / map_size, 1.0)  # å½’ä¸€åŒ–åˆ° [0, 1]
        
        # === 5. å¢å¼ºçš„æ©ç å­—å…¸ ===
        masks = self._calculate_robust_masks()
        
        # æ„å»ºå›¾çŠ¶æ€å­—å…¸
        graph_state = {
            "uav_features": uav_features,
            "target_features": target_features,
            "relative_positions": relative_positions,
            "distances": distances,
            "masks": masks
        }
        
        return graph_state

    def step(self, action):
        """æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ - æ”¯æŒå¯é€‰PBRSçš„ç¨³å®šç‰ˆæœ¬"""
        self.step_count += 1
        
        # === å¯é€‰PBRSï¼šè®°å½•åŠ¨ä½œå‰åŠ¿èƒ½ ===
        enable_pbrs = getattr(self.config, 'ENABLE_PBRS', False)
        potential_before = 0.0
        if enable_pbrs:
            pbrs_type = getattr(self.config, 'PBRS_TYPE', 'simple')  # 'simple' æˆ– 'progress'
            if pbrs_type == 'simple':
                potential_before = self._calculate_simple_potential()
            elif pbrs_type == 'progress':
                potential_before = self._calculate_progress_potential()
        
        # è½¬æ¢åŠ¨ä½œ
        target_idx, uav_idx, phi_idx = self._action_to_assignment(action)
        target = self.targets[target_idx]
        uav = self.uavs[uav_idx]
        
        # æ£€æŸ¥åŠ¨ä½œæœ‰æ•ˆæ€§
        if not self._is_valid_action(target, uav, phi_idx):
            return self._get_state(), self.invalid_action_penalty, False, False, {
                'invalid_action': True,
                'reason': 'invalid_assignment'
            }
        
        # è®¡ç®—å®é™…è´¡çŒ®
        actual_contribution = np.minimum(uav.resources, target.remaining_resources)

        # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…è´¡çŒ®
        if np.all(actual_contribution <= 0):
            return self._get_state(), self.invalid_action_penalty, False, False, {
                'invalid_action': True,
                'reason': 'no_contribution'
            }
        
        # è®°å½•ç›®æ ‡å®Œæˆå‰çš„çŠ¶æ€
        was_satisfied = np.all(target.remaining_resources <= 0)
        
        # è®¡ç®—è·¯å¾„é•¿åº¦
        path_len = np.linalg.norm(np.array(uav.current_position) - np.array(target.position))
        travel_time = path_len / uav.velocity_range[1] if uav.velocity_range[1] > 0 else 0.0
        
        # æ›´æ–°çŠ¶æ€
        uav.resources = uav.resources.astype(np.float64) - actual_contribution.astype(np.float64)
        target.remaining_resources = target.remaining_resources.astype(np.float64) - actual_contribution.astype(np.float64)
        
        if uav_idx not in {a[0] for a in target.allocated_uavs}:
            target.allocated_uavs.append((uav_idx, phi_idx))
        uav.task_sequence.append((target_idx, phi_idx))
        uav.current_position = np.array(target.position).copy()
        uav.heading = phi_idx * (2 * np.pi / self.graph.n_phi)
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆæ‰€æœ‰ç›®æ ‡
        total_satisfied = sum(np.all(t.remaining_resources <= 0) for t in self.targets)
        total_targets = len(self.targets)
        done = bool(total_satisfied == total_targets)
        
        # === å¯é€‰PBRSï¼šè®°å½•åŠ¨ä½œååŠ¿èƒ½å¹¶è®¡ç®—å¡‘å½¢å¥–åŠ± ===
        potential_after = 0.0
        shaping_reward = 0.0
        
        if enable_pbrs:
            if pbrs_type == 'simple':
                potential_after = self._calculate_simple_potential()
            elif pbrs_type == 'progress':
                potential_after = self._calculate_progress_potential()
            
            # è®¡ç®—PBRSå¡‘å½¢å¥–åŠ±ï¼šÎ³ * Î¦(s') - Î¦(s)
            gamma = getattr(self.config, 'GAMMA', 0.99)
            raw_shaping_reward = gamma * potential_after - potential_before
            
            # æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥å’Œè£å‰ª
            if np.isnan(raw_shaping_reward) or np.isinf(raw_shaping_reward):
                shaping_reward = 0.0
                print(f"è­¦å‘Š: å¡‘å½¢å¥–åŠ±ä¸ºNaN/Infï¼Œå·²é‡ç½®ä¸º0")
            else:
                # è£å‰ªå¡‘å½¢å¥–åŠ±åˆ°åˆç†èŒƒå›´
                clip_min = getattr(self.config, 'PBRS_REWARD_CLIP_MIN', -50.0)
                clip_max = getattr(self.config, 'PBRS_REWARD_CLIP_MAX', 50.0)
                shaping_reward = np.clip(raw_shaping_reward, clip_min, clip_max)
        
        # è®¡ç®—åŸºç¡€å¥–åŠ± - æ ¹æ®ç½‘ç»œç±»å‹é€‰æ‹©å¥–åŠ±å‡½æ•°
        network_type = getattr(self.config, 'NETWORK_TYPE', 'FCN')
        if network_type == 'ZeroShotGNN':
            # ä½¿ç”¨ååŒå¢æ•ˆç‰ˆå¥–åŠ±å‡½æ•°ï¼ˆä¸éœ€è¦infoå‚æ•°ï¼‰
            base_reward = self._calculate_synergistic_reward(target, uav, actual_contribution, path_len, 
                                                           was_satisfied, travel_time, done)
        else:
            # å…¶ä»–ç½‘ç»œä½¿ç”¨åŸæœ‰å¥–åŠ±å‡½æ•°
            base_reward = self._calculate_simple_reward(target, uav, actual_contribution, path_len, 
                                                       was_satisfied, travel_time, done)
        
        # æ€»å¥–åŠ± = åŸºç¡€å¥–åŠ± + å¡‘å½¢å¥–åŠ±
        total_reward = base_reward + shaping_reward
        
        # åº”ç”¨å¥–åŠ±å½’ä¸€åŒ–ï¼ˆç´§æ€¥ç¨³å®šæ€§ä¿®å¤ï¼‰
        if getattr(self.config, 'REWARD_NORMALIZATION', False):
            reward_scale = getattr(self.config, 'REWARD_SCALE', 1.0)
            total_reward *= reward_scale
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if getattr(self.config, 'ENABLE_NUMERICAL_STABILITY_CHECKS', False):
            if np.isnan(total_reward) or np.isinf(total_reward):
                print(f"è­¦å‘Š: æ€»å¥–åŠ±ä¸ºNaN/Inf ({total_reward})ï¼Œé‡ç½®ä¸º0")
                total_reward = 0.0
        
        # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        truncated = self.step_count >= self.max_steps
        
        # æ„å»ºè¯¦ç»†ä¿¡æ¯å­—å…¸
        info = {
            'target_id': int(target_idx),
            'uav_id': int(uav_idx),
            'phi_idx': int(phi_idx),
            'actual_contribution': float(np.sum(actual_contribution)),
            'path_length': float(path_len),
            'travel_time': float(travel_time),
            'done': bool(done),
            
            # PBRSç›¸å…³ä¿¡æ¯
            'pbrs_enabled': enable_pbrs,
            'base_reward': float(base_reward),
            'shaping_reward': float(shaping_reward),
            'potential_before': float(potential_before),
            'potential_after': float(potential_after),
            'total_reward': float(total_reward)
        }
        
        return self._get_state(), total_reward, done, truncated, info

    def _calculate_simple_reward(self, target, uav, actual_contribution, path_len, 
                                was_satisfied, travel_time, done):
        """
        ä¼˜åŒ–å¥–åŠ±å‡½æ•° - é¦–è¦æ»¡è¶³èµ„æºéœ€æ±‚ï¼Œå…¶æ¬¡è·¯å¾„æœ€çŸ­
        
        è®¾è®¡åŸåˆ™ï¼š
        1. é¦–è¦ç›®æ ‡ï¼šæ»¡è¶³ä»»åŠ¡èµ„æºéœ€æ±‚ (é«˜æƒé‡æ­£å¥–åŠ±)
        2. æ¬¡è¦ç›®æ ‡ï¼šè·¯å¾„æœ€çŸ­ (ä½æƒé‡è´Ÿå¥–åŠ±)
        3. ä»»ä½•èµ„æºè´¡çŒ®éƒ½ç»™äºˆæ­£å¥–åŠ±
        4. å¤§å¹…å‡å°‘æ— æ•ˆåŠ¨ä½œæƒ©ç½š
        """
        reward = 0.0
        
        # === é¦–è¦ç›®æ ‡ï¼šæ»¡è¶³èµ„æºéœ€æ±‚ (é«˜ä¼˜å…ˆçº§æ­£å¥–åŠ±) ===
        
        # 1. æœ€ç»ˆæˆåŠŸçš„å·¨å¤§å¥–åŠ±
        if done:
            reward += 100.0
        
        # 2. å•ä¸ªç›®æ ‡å®Œæˆå¥–åŠ±
        now_satisfied = np.all(target.remaining_resources <= 0)
        if now_satisfied and not was_satisfied:
            reward += 30.0  # ä»20.0å¢åŠ åˆ°30.0
        
        # 3. èµ„æºè´¡çŒ®å¥–åŠ± - æ ¸å¿ƒæ”¹è¿›ï¼šä»»ä½•è´¡çŒ®éƒ½ç»™å¥–åŠ±
        contribution_amount = np.sum(actual_contribution)
        if contribution_amount > 0:
            # åŸºç¡€è´¡çŒ®å¥–åŠ±
            base_contribution_reward = 5.0  # ä»1.0å¤§å¹…å¢åŠ åˆ°5.0
            
            # è´¡çŒ®é‡æ¯”ä¾‹å¥–åŠ± - è´¡çŒ®è¶Šå¤šå¥–åŠ±è¶Šé«˜
            target_total_need = np.sum(target.resources)
            if target_total_need > 0:
                contribution_ratio = contribution_amount / target_total_need
                ratio_reward = 10.0 * contribution_ratio  # æœ€é«˜10åˆ†çš„æ¯”ä¾‹å¥–åŠ±
            else:
                ratio_reward = 0.0
            
            # èµ„æºåŒ¹é…å¥–åŠ± - é¼“åŠ±é«˜æ•ˆçš„èµ„æºåˆ©ç”¨
            uav_total_resources = np.sum(uav.resources) + contribution_amount  # åŸå§‹èµ„æºé‡
            if uav_total_resources > 0:
                efficiency_ratio = contribution_amount / uav_total_resources
                efficiency_reward = 5.0 * efficiency_ratio  # æœ€é«˜5åˆ†çš„æ•ˆç‡å¥–åŠ±
            else:
                efficiency_reward = 0.0
            
            total_contribution_reward = base_contribution_reward + ratio_reward + efficiency_reward
            reward += total_contribution_reward
        
        # === æ¬¡è¦ç›®æ ‡ï¼šè·¯å¾„æœ€çŸ­ (ä½ä¼˜å…ˆçº§è´Ÿå¥–åŠ±) ===
        
        # 4. å¤§å¹…å‡å°‘çš„é£è¡Œæˆæœ¬ - åªæœ‰åœ¨æœ‰è´¡çŒ®æ—¶æ‰è€ƒè™‘è·¯å¾„ä¼˜åŒ–
        if contribution_amount > 0:
            # åªå¯¹æœ‰æ•ˆè´¡çŒ®çš„è¡ŒåŠ¨è€ƒè™‘è·¯å¾„æˆæœ¬ï¼Œä¸”æƒé‡å¤§å¹…é™ä½
            flight_cost = -0.002 * path_len  # ä»-0.01å‡å°‘åˆ°-0.002 (å‡å°‘80%)
            reward += flight_cost
        
        return reward

    def _calculate_synergistic_reward(self, target, uav, actual_contribution, path_len, 
                                    was_satisfied, travel_time, done):
        """
        [ååŒå¢æ•ˆç‰ˆ] èåˆäº†ä»»åŠ¡å®Œæˆä¸æ™ºèƒ½åä½œçš„å¥–åŠ±å‡½æ•°
        
        ä¸“é—¨é’ˆå¯¹ZeroShotGNNç½‘ç»œè®¾è®¡ï¼Œå¼•å…¥"ååŒå¢æ•ˆ"å¥–åŠ±å±‚ï¼Œ
        ç²¾ç¡®å¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ ä½•æ—¶ä»¥åŠå¦‚ä½•è¿›è¡Œé«˜æ•ˆçš„å›¢é˜Ÿåä½œã€‚
        
        æ ¸å¿ƒè®¾è®¡ç†å¿µï¼š
        1. ä¿æŒ"ä»»åŠ¡å®Œæˆ"ä¸ºæœ€é«˜ä¼˜å…ˆçº§
        2. å¯¹å¿…é¡»é€šè¿‡å¤šæ— äººæœºåä½œæ‰èƒ½å®Œæˆçš„"é«˜éœ€æ±‚"ç›®æ ‡ç»™äºˆå·¨å¤§å¥–åŠ±
        3. é¿å…"ä¸ºååŒè€ŒååŒ"çš„èµ„æºæµªè´¹
        4. ç¡®ä¿ä¸å…¶ä»–ç½‘ç»œç±»å‹çš„å¥–åŠ±è¿‡ç¨‹éš”ç¦»
        """
        
        # 1. å·¨å¤§çš„æœ€ç»ˆæˆåŠŸå¥–åŠ± (å½“æ‰€æœ‰ç›®æ ‡éƒ½å®Œæˆæ—¶) - ä¿æŒä¸å˜
        all_targets_satisfied = all(np.all(t.remaining_resources <= 0) for t in self.targets)
        if done and all_targets_satisfied:
            return 1000.0  # æœ€é«˜ä¼˜å…ˆçº§
        
        # 2. æ˜¾è‘—çš„ä¸­é—´æˆåŠŸå¥–åŠ±ï¼šæ–°å®Œæˆä¸€ä¸ªç›®æ ‡
        now_satisfied = np.all(target.remaining_resources <= 0)
        new_satisfied = now_satisfied and not was_satisfied
        base_completion_reward = 0.0
        
        if new_satisfied:
            base_completion_reward = 200.0
            
            # --- [æ ¸å¿ƒæ–°å¢] "ååŒå¢æ•ˆ"å¥–åŠ±è®¡ç®— ---
            # åªæœ‰åœ¨æ–°å®Œæˆäº†ä¸€ä¸ªç›®æ ‡æ—¶ï¼Œæ‰æ£€æŸ¥è¿™æ¬¡å®Œæˆæ˜¯å¦æ˜¯"ååŒå¢æ•ˆ"çš„
            
            # a. è·å–æ‰€æœ‰å‚ä¸æ”»å‡»è¯¥ç›®æ ‡çš„æ— äººæœº
            participating_uav_ids = {uav_info[0] for uav_info in target.allocated_uavs}
            
            # b. æ£€æŸ¥ååŒçš„"å¿…è¦æ€§"
            is_synergistic = False
            if len(participating_uav_ids) > 1:
                # æ£€æŸ¥ç›®æ ‡çš„æ€»éœ€æ±‚æ˜¯å¦è¶…è¿‡äº†ä»»ä½•ä¸€ä¸ªå‚ä¸è€…çš„å•æœºæœ€å¤§è¿è½½èƒ½åŠ›
                target_total_demand = np.sum(target.resources)
                max_single_payload = 0
                
                for uav_id in participating_uav_ids:
                    # ä»UAVåˆ—è¡¨ä¸­æ‰¾åˆ°å¯¹åº”çš„æ— äººæœºå¯¹è±¡
                    uav_obj = None
                    for u in self.uavs:
                        if u.id == uav_id:
                            uav_obj = u
                            break
                    
                    if uav_obj is not None:
                        max_single_payload = max(max_single_payload, uav_obj.max_payload)
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦ååŒï¼šç›®æ ‡éœ€æ±‚è¶…è¿‡å•æœºæœ€å¤§èƒ½åŠ›
                if target_total_demand > max_single_payload:
                    is_synergistic = True
            
            # c. å¦‚æœæ˜¯"ååŒå¢æ•ˆ"ï¼Œåˆ™ç»™äºˆå·¨å¤§å¥–åŠ±
            if is_synergistic:
                # è¿™ä¸ªå¥–åŠ±åº”è¯¥éå¸¸å¤§ï¼Œä»¥æ¿€åŠ±æ™ºèƒ½ä½“å»å­¦ä¹ è¿™ç§å¤æ‚çš„è¡Œä¸º
                synergy_bonus = 300.0
                base_completion_reward += synergy_bonus
                print(f"ğŸ¯ ååŒå¢æ•ˆæ”»å‡»æˆåŠŸï¼ç›®æ ‡ {target.id} è¢«æ‘§æ¯ï¼Œå‚ä¸UAV: {participating_uav_ids}ï¼Œè·å¾—é¢å¤–å¥–åŠ± {synergy_bonus}")
        
        # 3. åŸºç¡€è¡ŒåŠ¨å¥–åŠ±ï¼šå¯¹ç›®æ ‡é€ æˆæœ‰æ•ˆä¼¤å®³ï¼ˆæœ‰èµ„æºè´¡çŒ®ï¼‰
        action_reward = 0.0
        if np.sum(actual_contribution) > 0:
            # ç»™äºˆä¸€ä¸ªå¾®å°çš„æ­£å¥–åŠ±ï¼Œä»¥é¼“åŠ±"è¡ŒåŠ¨"
            # é™„å¸¦ä¸€ä¸ªå¾®å°çš„ã€ç™¾åˆ†æ¯”å½¢å¼çš„é£è¡Œæˆæœ¬æƒ©ç½š
            # æƒ©ç½šä¸åº”è¶…è¿‡åŸºç¡€å¥–åŠ±ï¼Œä»…ä½œä¸ºtie-breaker
            path_cost_penalty = (path_len / 5000.0) * 0.5
            action_reward = 1.0 - path_cost_penalty
        
        # 4. å¼ºæƒ©ç½šï¼šä»…æƒ©ç½šç»å¯¹çš„é”™è¯¯åŠ¨ä½œï¼ˆé›¶è´¡çŒ®ï¼‰ - ä¿æŒä¸å˜
        if np.sum(actual_contribution) <= 0:
            return -5.0
        
        return base_completion_reward + action_reward

    def _action_to_assignment(self, action):
        """å°†åŠ¨ä½œç´¢å¼•è½¬æ¢ä¸ºä»»åŠ¡åˆ†é… - ä¿®å¤ç‰ˆæœ¬ï¼Œæ·»åŠ è¾¹ç•Œæ£€æŸ¥"""
        n_uavs = len(self.uavs)
        n_targets = len(self.targets)
        n_phi = self.graph.n_phi
        
        # ç¡®ä¿åŠ¨ä½œåœ¨æœ‰æ•ˆèŒƒå›´å†…
        max_valid_action = n_targets * n_uavs * n_phi - 1
        if action > max_valid_action:
            print(f"è­¦å‘Š: åŠ¨ä½œ {action} è¶…å‡ºæœ‰æ•ˆèŒƒå›´ [0, {max_valid_action}]ï¼Œè°ƒæ•´ä¸ºæ¨¡è¿ç®—ç»“æœ")
            action = action % (max_valid_action + 1)
        
        target_idx = action // (n_uavs * n_phi)
        remaining = action % (n_uavs * n_phi)
        uav_idx = remaining // n_phi
        phi_idx = remaining % n_phi
        
        # å†æ¬¡éªŒè¯ç´¢å¼•è¾¹ç•Œ
        target_idx = min(target_idx, n_targets - 1)
        uav_idx = min(uav_idx, n_uavs - 1)
        phi_idx = min(phi_idx, n_phi - 1)
        
        # ç¡®ä¿ç´¢å¼•éè´Ÿ
        target_idx = max(0, target_idx)
        uav_idx = max(0, uav_idx)
        phi_idx = max(0, phi_idx)
        
        return target_idx, uav_idx, phi_idx
    
    def _is_valid_action(self, target, uav, phi_idx):
        """æ£€æŸ¥åŠ¨ä½œæ˜¯å¦æœ‰æ•ˆ"""
        if np.all(uav.resources <= 0):
            return False
        if np.all(target.remaining_resources <= 0):
            return False
        if (uav.id, phi_idx) in target.allocated_uavs:
            return False
        return True

    def calculate_simplified_reward(self, target, uav, actual_contribution, path_len, 
                                was_satisfied, travel_time, done):
        """
        ç®€åŒ–çš„å¥–åŠ±å‡½æ•°ï¼Œé‡ç‚¹å…³æ³¨ç›®æ ‡èµ„æºæ»¡è¶³å’Œæ­»é”é¿å…
        
        Args:
            target: ç›®æ ‡å¯¹è±¡
            uav: UAVå¯¹è±¡
            actual_contribution: å®é™…èµ„æºè´¡çŒ®
            path_len: è·¯å¾„é•¿åº¦
            was_satisfied: ä¹‹å‰æ˜¯å¦å·²æ»¡è¶³ç›®æ ‡
            travel_time: æ—…è¡Œæ—¶é—´
            done: æ˜¯å¦å®Œæˆæ‰€æœ‰ç›®æ ‡
            
        Returns:
            float: å½’ä¸€åŒ–çš„å¥–åŠ±å€¼
        """
        # 1. ä»»åŠ¡å®Œæˆå¥–åŠ± (æœ€é«˜ä¼˜å…ˆçº§)
        if done:
            return 10.0  # å½’ä¸€åŒ–åçš„æœ€é«˜å¥–åŠ±
        
        # 2. ç›®æ ‡æ»¡è¶³å¥–åŠ±
        now_satisfied = np.all(target.remaining_resources <= 0)
        new_satisfied = int(now_satisfied and not was_satisfied)
        target_completion_reward = 5.0 if new_satisfied else 0.0
        
        # 3. èµ„æºè´¡çŒ®å¥–åŠ± (æ ¸å¿ƒå¥–åŠ±)
        # è®¡ç®—è´¡çŒ®æ¯”ä¾‹è€Œä¸æ˜¯ç»å¯¹å€¼
        target_initial_total = np.sum(target.resources)
        contribution_ratio = np.sum(actual_contribution) / target_initial_total if target_initial_total > 0 else 0
        contribution_reward = contribution_ratio * 3.0  # æœ€é«˜3åˆ†
        
        # 4. é›¶è´¡çŒ®æƒ©ç½š (é¿å…æ­»é”)
        if np.all(actual_contribution <= 0):
            return -5.0  # ä¸¥é‡æƒ©ç½šé›¶è´¡çŒ®åŠ¨ä½œ
        
        # 5. è·ç¦»æƒ©ç½š (ç®€åŒ–ç‰ˆ)
        # ä½¿ç”¨ç›¸å¯¹è·ç¦»è€Œä¸æ˜¯ç»å¯¹è·ç¦»
        max_distance = 1000.0  # å‡è®¾çš„æœ€å¤§è·ç¦»
        distance_ratio = min(path_len / max_distance, 1.0)
        distance_penalty = -distance_ratio * 1.0  # æœ€å¤š-1åˆ†
        
        # æ€»å¥–åŠ± (å½’ä¸€åŒ–åˆ°[-5, 10]èŒƒå›´)
        total_reward = target_completion_reward + contribution_reward + distance_penalty
        
        return float(total_reward)
    
    def _calculate_reward_legacy(self, target, uav, actual_contribution, path_len, 
                         was_satisfied, travel_time, done):
        """
        Per-Agentå½’ä¸€åŒ–å¥–åŠ±å‡½æ•° - è§£å†³å°ºåº¦æ¼‚ç§»é—®é¢˜
        
        æ ¸å¿ƒè®¾è®¡ç†å¿µ:
        1. å·¨å¤§çš„æ­£å‘å¥–åŠ±ä½œä¸ºæ ¸å¿ƒæ¿€åŠ±
        2. æ‰€æœ‰æˆæœ¬ä½œä¸ºæ­£å¥–åŠ±çš„åŠ¨æ€ç™¾åˆ†æ¯”å‡é¡¹
        3. å¡‘å½¢å¥–åŠ±å¼•å¯¼æ¢ç´¢
        4. **Per-Agentå½’ä¸€åŒ–**: è¯†åˆ«ä¸æ— äººæœºæ•°é‡ç›¸å…³çš„å¥–åŠ±é¡¹ï¼Œé™¤ä»¥å½“å‰æœ‰æ•ˆæ— äººæœºæ•°é‡
        5. ç§»é™¤æ‰€æœ‰ç¡¬ç¼–ç çš„å·¨å¤§æƒ©ç½šå€¼
        
        å¥–åŠ±ç»“æ„:
        - ä»»åŠ¡å®Œæˆå¥–åŠ±: 100.0 (æ ¸å¿ƒæ­£å‘æ¿€åŠ±)
        - èµ„æºè´¡çŒ®å¥–åŠ±: 10.0-50.0 (åŸºäºè´¡çŒ®æ¯”ä¾‹)
        - å¡‘å½¢å¥–åŠ±: 0.1-2.0 (æ¥è¿‘ç›®æ ‡ã€åä½œç­‰)
        - åŠ¨æ€æˆæœ¬: æ­£å¥–åŠ±çš„3-8%ä½œä¸ºå‡é¡¹
        - **å½’ä¸€åŒ–å¤„ç†**: æ‹¥å µæƒ©ç½šç­‰ä¸UAVæ•°é‡ç›¸å…³çš„å¥–åŠ±é¡¹æŒ‰N_activeå½’ä¸€åŒ–
        """
        
        # ===== ç¬¬ä¸€éƒ¨åˆ†: è®¡ç®—å½“å‰æœ‰æ•ˆæ— äººæœºæ•°é‡ (Per-Agentå½’ä¸€åŒ–åŸºç¡€) =====
        n_active_uavs = self._calculate_active_uav_count()
        
        # ===== ç¬¬äºŒéƒ¨åˆ†: è®¡ç®—æ‰€æœ‰æ­£å‘å¥–åŠ± =====
        positive_rewards = 0.0
        reward_components = {
            'n_active_uavs': n_active_uavs,  # è®°å½•å½“å‰æœ‰æ•ˆæ— äººæœºæ•°é‡ç”¨äºè°ƒè¯•
            'normalization_applied': []      # è®°å½•å“ªäº›å¥–åŠ±é¡¹åº”ç”¨äº†å½’ä¸€åŒ–
        }
        
        # 1. ä»»åŠ¡å®Œæˆçš„å·¨å¤§æ­£å‘å¥–åŠ± (æ ¸å¿ƒæ¿€åŠ±)
        now_satisfied = np.all(target.remaining_resources <= 0)
        new_satisfied = now_satisfied and not was_satisfied
        
        if new_satisfied:
            task_completion_reward = 100.0  # å·¨å¤§çš„ä»»åŠ¡å®Œæˆå¥–åŠ±
            positive_rewards += task_completion_reward
            reward_components['task_completion'] = task_completion_reward
        
        # 2. èµ„æºè´¡çŒ®å¥–åŠ± (åŸºäºå®é™…è´¡çŒ®çš„æ­£å‘æ¿€åŠ±)
        contribution_reward = 0.0
        if np.sum(actual_contribution) > 0:
            target_initial_total = np.sum(target.resources)
            if target_initial_total > 0:
                # è®¡ç®—è´¡çŒ®æ¯”ä¾‹
                contribution_ratio = np.sum(actual_contribution) / target_initial_total
                
                # åŸºç¡€è´¡çŒ®å¥–åŠ±: 10-50åˆ†
                base_contribution = 10.0 + 40.0 * contribution_ratio
                
                # è¾¹é™…æ•ˆç”¨å¥–åŠ±: å¯¹å°è´¡çŒ®ä¹Ÿç»™äºˆé¼“åŠ±
                marginal_utility = 15.0 * np.sqrt(contribution_ratio)
                
                # é«˜æ•ˆè´¡çŒ®å¥–åŠ±: å¯¹å¤§æ¯”ä¾‹è´¡çŒ®ç»™äºˆé¢å¤–å¥–åŠ±
                efficiency_bonus = 0.0
                if contribution_ratio > 0.3:
                    efficiency_bonus = 10.0 * (contribution_ratio - 0.3)
                
                contribution_reward = base_contribution + marginal_utility + efficiency_bonus
                positive_rewards += contribution_reward
                reward_components['contribution'] = contribution_reward
        
        # 3. å¡‘å½¢å¥–åŠ± - å¼•å¯¼æ¢ç´¢å’Œåä½œ
        shaping_rewards = 0.0
        
        # 3.1 æ¥è¿‘ç›®æ ‡çš„å¡‘å½¢å¥–åŠ±
        approach_reward = self._calculate_approach_reward(uav, target)
        shaping_rewards += approach_reward
        reward_components['approach_shaping'] = approach_reward
        
        # 3.2 é¦–æ¬¡æ¥è§¦ç›®æ ‡å¥–åŠ±
        if len(target.allocated_uavs) == 1 and target.allocated_uavs[0][0] == uav.id:
            first_contact_reward = 5.0
            shaping_rewards += first_contact_reward
            reward_components['first_contact'] = first_contact_reward
        
        # 3.3 åä½œå¡‘å½¢å¥–åŠ± (Per-Agentå½’ä¸€åŒ–)
        collaboration_reward_raw = self._calculate_collaboration_reward(target, uav)
        # åä½œå¥–åŠ±ä¸UAVæ•°é‡ç›¸å…³ï¼Œéœ€è¦å½’ä¸€åŒ–
        collaboration_reward = collaboration_reward_raw / n_active_uavs
        shaping_rewards += collaboration_reward
        reward_components['collaboration_raw'] = collaboration_reward_raw
        reward_components['collaboration_normalized'] = collaboration_reward
        reward_components['normalization_applied'].append('collaboration')
        
        # 3.4 å…¨å±€å®Œæˆè¿›åº¦å¥–åŠ±
        global_progress_reward = self._calculate_global_progress_reward()
        shaping_rewards += global_progress_reward
        reward_components['global_progress'] = global_progress_reward
        
        positive_rewards += shaping_rewards
        
        # ===== ç¬¬ä¸‰éƒ¨åˆ†: åŠ¨æ€å°ºåº¦æˆæœ¬è®¡ç®— (åŒ…å«Per-Agentå½’ä¸€åŒ–) =====
        total_costs = 0.0
        
        # ç¡®ä¿æœ‰æœ€å°æ­£å‘å¥–åŠ±åŸºæ•°ï¼Œé¿å…é™¤é›¶
        reward_base = max(positive_rewards, 1.0)
        
        # 1. è·ç¦»æˆæœ¬ - æ­£å‘å¥–åŠ±çš„3-5%
        distance_cost_ratio = 0.03 + 0.02 * min(1.0, path_len / 3000.0)  # 3%-5%
        distance_cost_raw = reward_base * distance_cost_ratio
        total_costs += distance_cost_raw
        reward_components['distance_cost'] = -distance_cost_raw
        
        # 2. æ—¶é—´æˆæœ¬ - æ­£å‘å¥–åŠ±çš„2-3%
        time_cost_ratio = 0.02 + 0.01 * min(1.0, travel_time / 60.0)  # 2%-3%
        time_cost_raw = reward_base * time_cost_ratio
        total_costs += time_cost_raw
        reward_components['time_cost'] = -time_cost_raw
        
        # 3. æ‹¥å µæƒ©ç½š (æ–°å¢ - ä¸UAVæ•°é‡ç›´æ¥ç›¸å…³ï¼Œéœ€è¦Per-Agentå½’ä¸€åŒ–)
        congestion_penalty_raw = self._calculate_congestion_penalty(target, uav, n_active_uavs)
        congestion_penalty_normalized = congestion_penalty_raw / n_active_uavs
        total_costs += congestion_penalty_normalized
        reward_components['congestion_penalty_raw'] = -congestion_penalty_raw
        reward_components['congestion_penalty_normalized'] = -congestion_penalty_normalized
        if congestion_penalty_raw > 0:
            reward_components['normalization_applied'].append('congestion_penalty')
        
        # 4. èµ„æºæ•ˆç‡æˆæœ¬ - å¦‚æœè´¡çŒ®æ•ˆç‡ä½
        efficiency_cost = 0.0
        if np.sum(actual_contribution) > 0:
            # è®¡ç®—èµ„æºåˆ©ç”¨æ•ˆç‡
            uav_capacity = np.sum(uav.resources)
            if uav_capacity > 0:
                utilization_ratio = np.sum(actual_contribution) / uav_capacity
                if utilization_ratio < 0.5:  # åˆ©ç”¨ç‡ä½äº50%
                    efficiency_cost_ratio = 0.02 * (0.5 - utilization_ratio)  # æœ€å¤š2%
                    efficiency_cost = reward_base * efficiency_cost_ratio
                    total_costs += efficiency_cost
                    reward_components['efficiency_cost'] = -efficiency_cost
        
        # ===== ç¬¬å››éƒ¨åˆ†: ç‰¹æ®Šæƒ…å†µå¤„ç† =====
        
        # é›¶è´¡çŒ®çš„æ¸©å’Œå¼•å¯¼ (ä¸å†æ˜¯ç¡¬ç¼–ç çš„å·¨å¤§æƒ©ç½š)
        if np.sum(actual_contribution) <= 0:
            # ç»™äºˆæœ€å°çš„åŸºç¡€å¥–åŠ±ï¼Œä½†å¢åŠ æˆæœ¬æ¯”ä¾‹
            if positive_rewards == 0:
                positive_rewards = 0.5  # æœ€å°åŸºç¡€å¥–åŠ±
                reward_components['base_reward'] = 0.5
            
            # å¢åŠ æ— æ•ˆè¡ŒåŠ¨æˆæœ¬ (æ­£å‘å¥–åŠ±çš„10%)
            ineffective_cost = positive_rewards * 0.1
            total_costs += ineffective_cost
            reward_components['ineffective_cost'] = -ineffective_cost
        
        # å…¨å±€ä»»åŠ¡å®Œæˆçš„è¶…çº§å¥–åŠ±
        if done:
            all_targets_satisfied = all(np.all(t.remaining_resources <= 0) for t in self.targets)
            if all_targets_satisfied:
                global_completion_reward = 200.0  # è¶…çº§å®Œæˆå¥–åŠ±
                positive_rewards += global_completion_reward
                reward_components['global_completion'] = global_completion_reward
        
        # ===== ç¬¬äº”éƒ¨åˆ†: æœ€ç»ˆå¥–åŠ±è®¡ç®—ä¸å½’ä¸€åŒ–æ€»ç»“ =====
        final_reward = positive_rewards - total_costs
        
        # æ¸©å’Œçš„å¥–åŠ±èŒƒå›´é™åˆ¶ (ä¸å†ç¡¬æ€§è£å‰ª)
        final_reward = np.clip(final_reward, -10.0, 300.0)
        
        # è®°å½•è¯¦ç»†çš„å¥–åŠ±ç»„æˆ (å¢å¼ºç‰ˆ - æ”¯æŒPer-Agentå½’ä¸€åŒ–ç›‘æ§)
        reward_components.update({
            'total_positive': positive_rewards,
            'total_costs': total_costs,
            'final_reward': final_reward,
            'target_id': target.id,
            'uav_id': uav.id,
            'contribution_amount': float(np.sum(actual_contribution)),
            'path_length': float(path_len),
            'travel_time': float(travel_time),
            'done': done,
            
            # Per-Agentå½’ä¸€åŒ–ç›¸å…³ä¿¡æ¯
            'per_agent_normalization': {
                'n_active_uavs': n_active_uavs,
                'total_uavs': len(self.uavs),
                'normalization_factor': 1.0 / n_active_uavs,
                'components_normalized': reward_components['normalization_applied'],
                'normalization_impact': self._calculate_normalization_impact(reward_components)
            },
            
            # è°ƒè¯•ä¿¡æ¯
            'debug_info': {
                'step_count': self.step_count,
                'allocated_uavs_to_target': len(target.allocated_uavs),
                'target_remaining_resources': float(np.sum(target.remaining_resources)),
                'uav_remaining_resources': float(np.sum(uav.resources))
            }
        })
        
        # ä¿å­˜æœ€æ–°çš„å¥–åŠ±ç»„æˆç”¨äºè°ƒè¯•å’Œç›‘æ§
        self.last_reward_components = reward_components
        
        # å¦‚æœå¯ç”¨äº†è¯¦ç»†æ—¥å¿—è®°å½•ï¼Œè¾“å‡ºå½’ä¸€åŒ–ä¿¡æ¯
        if getattr(self.config, 'ENABLE_REWARD_LOGGING', False):
            self._log_reward_components(reward_components)
        
        return float(final_reward)
    
    # ===== PBRSç›¸å…³æ–¹æ³• - æ¸è¿›å¼å®‰å…¨å®ç° =====
    
    def _calculate_simple_potential(self):
        """
        æœ€ç®€å•çš„åŠ¿å‡½æ•°ï¼šÎ¦ = 100 * (å·²å®Œæˆç›®æ ‡æ•° / æ€»ç›®æ ‡æ•°)
        
        ç‰¹ç‚¹ï¼š
        - å•è°ƒé€’å¢ï¼šå®Œæˆæ›´å¤šç›®æ ‡åŠ¿èƒ½æ›´é«˜
        - ç›®æ ‡æ˜ç¡®ï¼šç›´æ¥å¯¹åº”æœ€ç»ˆç›®æ ‡
        - æœ€ä¸å¯èƒ½å‡ºé”™ï¼šé€»è¾‘ç®€å•æ¸…æ™°
        - ç†è®ºå®‰å…¨ï¼šä¸æ”¹å˜æœ€ä¼˜ç­–ç•¥
        
        Returns:
            float: åŠ¿èƒ½å€¼ [0, 100]
        """
        completed_targets = sum(1 for t in self.targets if np.all(t.remaining_resources <= 0))
        total_targets = len(self.targets)
        
        if total_targets == 0:
            return 0.0
        
        completion_ratio = completed_targets / total_targets
        potential = 100.0 * completion_ratio
        
        # åº”ç”¨ç¼©æ”¾å› å­
        scale = getattr(self.config, 'PBRS_POTENTIAL_SCALE', 1.0)
        potential *= scale
        
        return potential
    
    def _calculate_progress_potential(self):
        """
        è¿›åº¦åŠ¿å‡½æ•°ï¼šÎ¦ = 100 * (æ€»èµ„æºæ¶ˆè€—è¿›åº¦)
        
        ç‰¹ç‚¹ï¼š
        - è¿ç»­å˜åŒ–ï¼šæ¯æ¬¡èµ„æºæ¶ˆè€—éƒ½æœ‰åé¦ˆ
        - ç¨ å¯†ä¿¡å·ï¼šæä¾›æ›´å¤šå­¦ä¹ ä¿¡æ¯
        - å¹³æ»‘è¿‡æ¸¡ï¼šé¿å…å¥–åŠ±æ‚¬å´–
        
        Returns:
            float: åŠ¿èƒ½å€¼ [0, 100]
        """
        total_initial_demand = sum(np.sum(t.resources) for t in self.targets)
        total_remaining_demand = sum(np.sum(t.remaining_resources) for t in self.targets)
        
        if total_initial_demand <= 0:
            return 0.0
        
        progress_ratio = (total_initial_demand - total_remaining_demand) / total_initial_demand
        potential = 100.0 * progress_ratio
        
        # åº”ç”¨ç¼©æ”¾å› å­
        scale = getattr(self.config, 'PBRS_POTENTIAL_SCALE', 1.0)
        potential *= scale
        
        return potential

    # def _calculate_pbrs_base_reward(self, target, uav, actual_contribution, was_satisfied, all_targets_satisfied):
    #     """
    #     PBRSç³»ç»Ÿçš„åŸºç¡€å¥–åŠ±å‡½æ•° - å·²æ³¨é‡Šï¼Œæ¢å¤ç¨³å®šåŸºçº¿
    #     """
    #     pass

    def _calculate_approach_reward(self, uav, target):
        """
        è®¡ç®—æ¥è¿‘ç›®æ ‡çš„å¡‘å½¢å¥–åŠ±
        
        æ ¸å¿ƒæ€æƒ³: å¦‚æœæ— äººæœºç›¸æ¯”ä¸Šä¸€æ­¥æ›´æ¥è¿‘ä»»ä½•æœªå®Œæˆçš„ç›®æ ‡ï¼Œç»™äºˆå¾®å°æ­£å¥–åŠ±
        è¿™è§£å†³äº†ç›®æ ‡è¿‡è¿œå¯¼è‡´çš„æ¢ç´¢åˆæœŸæ— æ­£åé¦ˆé—®é¢˜
        """
        approach_reward = 0.0
        
        # è·å–å½“å‰ä½ç½®åˆ°ç›®æ ‡çš„è·ç¦»
        current_distance = np.linalg.norm(np.array(uav.current_position) - np.array(target.position))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å†å²ä½ç½®è®°å½•
        if hasattr(uav, 'previous_position') and uav.previous_position is not None:
            previous_distance = np.linalg.norm(np.array(uav.previous_position) - np.array(target.position))
            
            # å¦‚æœæ›´æ¥è¿‘ç›®æ ‡
            if current_distance < previous_distance:
                # è®¡ç®—æ¥è¿‘ç¨‹åº¦
                distance_improvement = previous_distance - current_distance
                max_improvement = 100.0  # å‡è®¾çš„æœ€å¤§æ”¹è¿›è·ç¦»
                
                # åŸºç¡€æ¥è¿‘å¥–åŠ±: 0.1-1.0
                base_approach = 0.1 + 0.9 * min(1.0, distance_improvement / max_improvement)
                
                # è·ç¦»è¶Šè¿‘ï¼Œå¥–åŠ±è¶Šé«˜
                proximity_bonus = 0.0
                if current_distance < 500.0:  # åœ¨500ç±³å†…
                    proximity_factor = (500.0 - current_distance) / 500.0
                    proximity_bonus = 0.5 * proximity_factor
                
                approach_reward = base_approach + proximity_bonus
        
        # æ›´æ–°ä½ç½®å†å²
        uav.previous_position = uav.current_position.copy()
        
        return approach_reward
    
    def _calculate_uav_alive_status(self, uav, uav_index):
        """
        è®¡ç®—æ— äººæœºçš„å­˜æ´»çŠ¶æ€ï¼ˆé²æ£’æ€§æ©ç ï¼‰
        
        Args:
            uav: UAVå¯¹è±¡
            uav_index: UAVç´¢å¼•
            
        Returns:
            float: å­˜æ´»çŠ¶æ€ (0.0 æˆ– 1.0)
        """
        # åŸºç¡€å­˜æ´»æ£€æŸ¥ï¼šèµ„æºæ˜¯å¦è€—å°½
        if np.all(uav.resources <= 0):
            return 0.0
        
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤æ‚çš„å­˜æ´»é€»è¾‘ï¼Œå¦‚ï¼š
        # - é€šä¿¡å¤±æ•ˆæ¦‚ç‡
        # - ä¼ æ„Ÿå™¨æ•…éšœæ¦‚ç‡
        # - è·ç¦»è¿‡è¿œå¯¼è‡´çš„ä¿¡å·ä¸¢å¤±
        
        return 1.0
    
    def _calculate_target_visibility_status(self, target, target_index):
        """
        è®¡ç®—ç›®æ ‡çš„å¯è§æ€§çŠ¶æ€ï¼ˆé²æ£’æ€§æ©ç ï¼‰
        
        Args:
            target: ç›®æ ‡å¯¹è±¡
            target_index: ç›®æ ‡ç´¢å¼•
            
        Returns:
            float: å¯è§æ€§çŠ¶æ€ (0.0 æˆ– 1.0)
        """
        # åŸºç¡€å¯è§æ€§æ£€æŸ¥ï¼šç›®æ ‡æ˜¯å¦å·²å®Œæˆ
        if np.all(target.remaining_resources <= 0):
            return 0.0
        
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤æ‚çš„å¯è§æ€§é€»è¾‘ï¼Œå¦‚ï¼š
        # - å¤©æ°”æ¡ä»¶å½±å“
        # - éšœç¢ç‰©é®æŒ¡
        # - ä¼ æ„Ÿå™¨èŒƒå›´é™åˆ¶
        
        return 1.0
    
    def _calculate_robust_masks(self):
        """
        è®¡ç®—å¢å¼ºçš„æ©ç å­—å…¸ï¼Œæ”¯æŒé²æ£’æ€§åœºæ™¯
        
        Returns:
            dict: åŒ…å«å„ç§æ©ç çš„å­—å…¸
        """
        n_uavs = len(self.uavs)
        n_targets = len(self.targets)
        
        # UAVæœ‰æ•ˆæ€§æ©ç 
        uav_mask = np.ones(n_uavs, dtype=np.int32)
        for i, uav in enumerate(self.uavs):
            uav_mask[i] = int(self._calculate_uav_alive_status(uav, i))
        
        # ç›®æ ‡æœ‰æ•ˆæ€§æ©ç 
        target_mask = np.ones(n_targets, dtype=np.int32)
        for i, target in enumerate(self.targets):
            target_mask[i] = int(self._calculate_target_visibility_status(target, i))
        
        return {
            "uav_mask": uav_mask,
            "target_mask": target_mask
        }
    
    def _calculate_active_uav_count(self):
        """
        è®¡ç®—å½“å‰æœ‰æ•ˆæ— äººæœºæ•°é‡ï¼ˆç”¨äºPer-Agentå½’ä¸€åŒ–ï¼‰
        
        Returns:
            int: æœ‰æ•ˆæ— äººæœºæ•°é‡
        """
        active_count = 0
        for uav in self.uavs:
            if np.any(uav.resources > 0):  # è‡³å°‘æœ‰ä¸€ç§èµ„æºå¤§äº0
                active_count += 1
        return max(active_count, 1)  # ç¡®ä¿è‡³å°‘ä¸º1ï¼Œé¿å…é™¤é›¶é”™è¯¯
    
    def _calculate_congestion_penalty(self, target, uav, n_active_uavs):
        """
        è®¡ç®—æ‹¥å µæƒ©ç½šï¼ˆä¸UAVæ•°é‡ç›¸å…³ï¼Œéœ€è¦å½’ä¸€åŒ–ï¼‰
        
        Args:
            target: ç›®æ ‡å¯¹è±¡
            uav: UAVå¯¹è±¡
            n_active_uavs: å½“å‰æœ‰æ•ˆæ— äººæœºæ•°é‡
            
        Returns:
            float: æ‹¥å µæƒ©ç½šå€¼
        """
        # è®¡ç®—åˆ†é…åˆ°åŒä¸€ç›®æ ‡çš„æ— äººæœºæ•°é‡
        uavs_on_target = len(target.allocated_uavs)
        
        # å¦‚æœå¤šä¸ªæ— äººæœºåˆ†é…åˆ°åŒä¸€ç›®æ ‡ï¼Œäº§ç”Ÿæ‹¥å µæƒ©ç½š
        if uavs_on_target > 1:
            # æƒ©ç½šä¸åˆ†é…çš„æ— äººæœºæ•°é‡æˆæ­£æ¯”
            congestion_factor = (uavs_on_target - 1) / n_active_uavs
            base_penalty = 2.0  # åŸºç¡€æƒ©ç½š
            return base_penalty * congestion_factor
        
        return 0.0
    
    def _calculate_global_progress_reward(self):
        """
        è®¡ç®—å…¨å±€å®Œæˆè¿›åº¦å¥–åŠ±
        
        Returns:
            float: å…¨å±€è¿›åº¦å¥–åŠ±
        """
        if not self.targets:
            return 0.0
        
        # è®¡ç®—æ€»ä½“å®Œæˆè¿›åº¦
        total_initial_resources = sum(np.sum(target.resources) for target in self.targets)
        total_remaining_resources = sum(np.sum(target.remaining_resources) for target in self.targets)
        
        if total_initial_resources <= 0:
            return 0.0
        
        progress_ratio = (total_initial_resources - total_remaining_resources) / total_initial_resources
        
        # ç»™äºˆæ¸è¿›å¼å¥–åŠ±
        if progress_ratio > 0.8:
            return 2.0 * (progress_ratio - 0.8) / 0.2  # 80%-100%æ—¶ç»™äºˆæœ€é«˜2åˆ†
        elif progress_ratio > 0.5:
            return 1.0 * (progress_ratio - 0.5) / 0.3  # 50%-80%æ—¶ç»™äºˆæœ€é«˜1åˆ†
        else:
            return 0.5 * progress_ratio / 0.5  # 0%-50%æ—¶ç»™äºˆæœ€é«˜0.5åˆ†
    
    def _calculate_normalization_impact(self, reward_components):
        """
        è®¡ç®—å½’ä¸€åŒ–å¯¹å¥–åŠ±çš„å½±å“
        
        Args:
            reward_components: å¥–åŠ±ç»„æˆå­—å…¸
            
        Returns:
            dict: å½’ä¸€åŒ–å½±å“åˆ†æ
        """
        impact = {}
        
        # è®¡ç®—å½’ä¸€åŒ–å‰åçš„å·®å¼‚
        for component in reward_components.get('normalization_applied', []):
            raw_key = f"{component}_raw"
            normalized_key = f"{component}_normalized"
            
            if raw_key in reward_components and normalized_key in reward_components:
                raw_value = reward_components[raw_key]
                normalized_value = reward_components[normalized_key]
                impact[component] = {
                    'raw': raw_value,
                    'normalized': normalized_value,
                    'difference': raw_value - normalized_value,
                    'reduction_ratio': (raw_value - normalized_value) / raw_value if raw_value != 0 else 0
                }
        
        return impact
    
    def _log_reward_components(self, reward_components):
        """
        è®°å½•è¯¦ç»†çš„å¥–åŠ±ç»„æˆä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        
        Args:
            reward_components: å¥–åŠ±ç»„æˆå­—å…¸
        """
        print(f"[å¥–åŠ±è¯¦æƒ…] Step {self.step_count}")
        print(f"  æœ‰æ•ˆUAVæ•°é‡: {reward_components['n_active_uavs']}")
        print(f"  æœ€ç»ˆå¥–åŠ±: {reward_components['final_reward']:.3f}")
        print(f"  æ­£å‘å¥–åŠ±æ€»è®¡: {reward_components['total_positive']:.3f}")
        print(f"  æˆæœ¬æ€»è®¡: {reward_components['total_costs']:.3f}")
        
        # è¾“å‡ºå½’ä¸€åŒ–ä¿¡æ¯
        if reward_components['normalization_applied']:
            print(f"  å½’ä¸€åŒ–ç»„ä»¶: {reward_components['normalization_applied']}")
            for component, impact in reward_components['per_agent_normalization']['normalization_impact'].items():
                print(f"    {component}: {impact['raw']:.3f} -> {impact['normalized']:.3f} (å‡å°‘ {impact['reduction_ratio']:.1%})")

    def _calculate_collaboration_reward(self, target, uav):
        """
        è®¡ç®—åä½œå¡‘å½¢å¥–åŠ±
        
        é¼“åŠ±åˆç†çš„åä½œï¼Œé¿å…è¿‡åº¦é›†ä¸­æˆ–è¿‡åº¦åˆ†æ•£
        """
        collaboration_reward = 0.0
        
        # è·å–å½“å‰åˆ†é…åˆ°è¯¥ç›®æ ‡çš„UAVæ•°é‡
        current_uav_count = len(target.allocated_uavs)
        
        if current_uav_count > 0:
            # è®¡ç®—ç›®æ ‡çš„èµ„æºéœ€æ±‚é‡
            target_demand = np.sum(target.resources)
            
            # ä¼°ç®—ç†æƒ³çš„UAVæ•°é‡ (åŸºäºèµ„æºéœ€æ±‚)
            avg_uav_capacity = 50.0  # å‡è®¾å¹³å‡UAVå®¹é‡
            ideal_uav_count = max(1, min(4, int(np.ceil(target_demand / avg_uav_capacity))))
            
            # åä½œæ•ˆç‡å¥–åŠ±
            if current_uav_count <= ideal_uav_count:
                # ç†æƒ³åä½œèŒƒå›´å†…
                efficiency_factor = 1.0 - abs(current_uav_count - ideal_uav_count) / ideal_uav_count
                collaboration_reward = 1.0 * efficiency_factor
            else:
                # è¿‡åº¦åä½œï¼Œé€’å‡å¥–åŠ±
                over_collaboration_penalty = (current_uav_count - ideal_uav_count) * 0.2
                collaboration_reward = max(0.2, 1.0 - over_collaboration_penalty)
            
            # å¤šæ ·æ€§å¥–åŠ±: å¦‚æœUAVæ¥è‡ªä¸åŒèµ·å§‹ä½ç½®
            if current_uav_count > 1:
                diversity_bonus = 0.3  # åŸºç¡€å¤šæ ·æ€§å¥–åŠ±
                collaboration_reward += diversity_bonus
        
        return collaboration_reward
    
    def _calculate_global_progress_reward(self):
        """
        è®¡ç®—å…¨å±€è¿›åº¦å¡‘å½¢å¥–åŠ±
        
        åŸºäºæ•´ä½“ä»»åŠ¡å®Œæˆè¿›åº¦ç»™äºˆå¥–åŠ±ï¼Œé¼“åŠ±ç³»ç»Ÿæ€§è¿›å±•
        """
        if not self.targets:
            return 0.0
        
        # è®¡ç®—å…¨å±€å®Œæˆç‡
        total_demand = sum(np.sum(target.resources) for target in self.targets)
        total_remaining = sum(np.sum(target.remaining_resources) for target in self.targets)
        
        if total_demand <= 0:
            return 0.0
        
        completion_rate = (total_demand - total_remaining) / total_demand
        
        # åŸºäºå®Œæˆç‡çš„è¿›åº¦å¥–åŠ±
        progress_reward = 0.0
        
        # é‡Œç¨‹ç¢‘å¥–åŠ±
        milestones = [0.25, 0.5, 0.75, 0.9]
        milestone_rewards = [0.5, 1.0, 1.5, 2.0]
        
        for milestone, reward in zip(milestones, milestone_rewards):
            if completion_rate >= milestone:
                # æ£€æŸ¥æ˜¯å¦åˆšè¾¾åˆ°è¿™ä¸ªé‡Œç¨‹ç¢‘
                if not hasattr(self, '_milestone_reached'):
                    self._milestone_reached = set()
                
                if milestone not in self._milestone_reached:
                    self._milestone_reached.add(milestone)
                    progress_reward += reward
        
        # è¿ç»­è¿›åº¦å¥–åŠ± (å¹³æ»‘çš„è¿›åº¦æ¿€åŠ±)
        smooth_progress = 0.2 * completion_rate
        progress_reward += smooth_progress
        
        return progress_reward
    
    def _calculate_active_uav_count(self) -> int:
        """
        è®¡ç®—å½“å‰æœ‰æ•ˆæ— äººæœºæ•°é‡ï¼Œç”¨äºPer-Agentå¥–åŠ±å½’ä¸€åŒ–
        
        æœ‰æ•ˆæ— äººæœºå®šä¹‰ï¼š
        - æ‹¥æœ‰å‰©ä½™èµ„æº (resources > 0)
        - é€šä¿¡/æ„ŸçŸ¥ç³»ç»Ÿæ­£å¸¸ (is_alive = 1.0)
        
        Returns:
            int: å½“å‰æœ‰æ•ˆæ— äººæœºæ•°é‡ N_active
        """
        active_count = 0
        
        for i, uav in enumerate(self.uavs):
            # æ£€æŸ¥æ˜¯å¦æœ‰å‰©ä½™èµ„æº
            has_resources = np.any(uav.resources > 0)
            
            # æ£€æŸ¥é€šä¿¡/æ„ŸçŸ¥çŠ¶æ€
            is_alive = self._calculate_uav_alive_status(uav, i)
            
            # åªæœ‰åŒæ—¶æ»¡è¶³èµ„æºå’Œé€šä¿¡æ¡ä»¶çš„UAVæ‰ç®—æœ‰æ•ˆ
            if has_resources and is_alive >= 0.5:  # is_alive >= 0.5 è¡¨ç¤ºè‡³å°‘éƒ¨åˆ†åŠŸèƒ½æ­£å¸¸
                active_count += 1
        
        # ç¡®ä¿è‡³å°‘æœ‰1ä¸ªæœ‰æ•ˆUAVï¼Œé¿å…é™¤é›¶é”™è¯¯
        return max(active_count, 1)
    
    def _calculate_congestion_penalty(self, target, uav, n_active_uavs: int) -> float:
        """
        è®¡ç®—æ‹¥å µæƒ©ç½š - ä¸æ— äººæœºæ•°é‡ç›¸å…³çš„æƒ©ç½šé¡¹
        
        æ‹¥å µæƒ©ç½šçš„æ ¸å¿ƒæ€æƒ³ï¼š
        1. å½“å¤šä¸ªUAVåŒæ—¶åˆ†é…åˆ°åŒä¸€ç›®æ ‡æ—¶ï¼Œäº§ç”Ÿæ‹¥å µ
        2. æ‹¥å µç¨‹åº¦ä¸åˆ†é…åˆ°è¯¥ç›®æ ‡çš„UAVæ•°é‡æˆæ­£æ¯”
        3. è¯¥æƒ©ç½šé¡¹ä¼šéšç€æ€»UAVæ•°é‡å¢åŠ è€Œå¢é•¿ï¼Œå› æ­¤éœ€è¦å½’ä¸€åŒ–
        
        Args:
            target: ç›®æ ‡å¯¹è±¡
            uav: å½“å‰UAVå¯¹è±¡
            n_active_uavs: å½“å‰æœ‰æ•ˆæ— äººæœºæ•°é‡
            
        Returns:
            float: æ‹¥å µæƒ©ç½šå€¼ (åŸå§‹å€¼ï¼Œè°ƒç”¨æ–¹è´Ÿè´£å½’ä¸€åŒ–)
        """
        congestion_penalty = 0.0
        
        # 1. ç›®æ ‡æ‹¥å µæƒ©ç½šï¼šåˆ†é…åˆ°åŒä¸€ç›®æ ‡çš„UAVè¿‡å¤š
        allocated_uav_count = len(target.allocated_uavs)
        if allocated_uav_count > 1:
            # è®¡ç®—ç†æƒ³åˆ†é…æ•°é‡
            target_demand = np.sum(target.resources)
            avg_uav_capacity = 50.0  # å‡è®¾å¹³å‡UAVå®¹é‡
            ideal_allocation = max(1, min(3, int(np.ceil(target_demand / avg_uav_capacity))))
            
            if allocated_uav_count > ideal_allocation:
                # è¿‡åº¦åˆ†é…æƒ©ç½šï¼ŒéšUAVæ•°é‡çº¿æ€§å¢é•¿
                over_allocation = allocated_uav_count - ideal_allocation
                congestion_penalty += over_allocation * 2.0  # æ¯ä¸ªå¤šä½™UAVæƒ©ç½š2åˆ†
        
        # 2. å…¨å±€æ‹¥å µæƒ©ç½šï¼šç³»ç»Ÿæ•´ä½“UAVå¯†åº¦è¿‡é«˜
        if n_active_uavs > len(self.targets) * 2:  # å¦‚æœUAVæ•°é‡è¶…è¿‡ç›®æ ‡æ•°é‡çš„2å€
            density_factor = n_active_uavs / (len(self.targets) * 2)
            global_congestion = (density_factor - 1.0) * 1.5  # å¯†åº¦è¶…æ ‡æƒ©ç½š
            congestion_penalty += global_congestion
        
        # 3. å±€éƒ¨æ‹¥å µæƒ©ç½šï¼šè®¡ç®—å½“å‰UAVå‘¨å›´çš„æ‹¥å µæƒ…å†µ
        local_congestion = self._calculate_local_congestion(uav, target)
        congestion_penalty += local_congestion
        
        return max(congestion_penalty, 0.0)  # ç¡®ä¿æƒ©ç½šå€¼éè´Ÿ
    
    def _calculate_local_congestion(self, uav, target) -> float:
        """
        è®¡ç®—å±€éƒ¨æ‹¥å µæƒ…å†µ
        
        Args:
            uav: å½“å‰UAVå¯¹è±¡
            target: ç›®æ ‡å¯¹è±¡
            
        Returns:
            float: å±€éƒ¨æ‹¥å µæƒ©ç½šå€¼
        """
        local_congestion = 0.0
        congestion_radius = 200.0  # æ‹¥å µæ£€æµ‹åŠå¾„
        
        # ç»Ÿè®¡åœ¨æ‹¥å µåŠå¾„å†…çš„å…¶ä»–UAVæ•°é‡
        nearby_uavs = 0
        for other_uav in self.uavs:
            if other_uav.id != uav.id:
                distance = np.linalg.norm(
                    np.array(other_uav.current_position) - np.array(uav.current_position)
                )
                if distance < congestion_radius:
                    nearby_uavs += 1
        
        # å¦‚æœé™„è¿‘UAVè¿‡å¤šï¼Œäº§ç”Ÿæ‹¥å µæƒ©ç½š
        if nearby_uavs > 2:  # è¶…è¿‡2ä¸ªé‚»è¿‘UAVå°±ç®—æ‹¥å µ
            local_congestion = (nearby_uavs - 2) * 0.5  # æ¯ä¸ªå¤šä½™é‚»è¿‘UAVæƒ©ç½š0.5åˆ†
        
        return local_congestion
    
    def _calculate_normalization_impact(self, reward_components: dict) -> dict:
        """
        è®¡ç®—å½’ä¸€åŒ–å¯¹å¥–åŠ±çš„å½±å“ç¨‹åº¦
        
        Args:
            reward_components: å¥–åŠ±ç»„æˆå­—å…¸
            
        Returns:
            dict: å½’ä¸€åŒ–å½±å“åˆ†æ
        """
        impact = {
            'total_raw_normalized_rewards': 0.0,
            'total_normalized_rewards': 0.0,
            'normalization_savings': 0.0,
            'components_impact': {}
        }
        
        # è®¡ç®—åä½œå¥–åŠ±çš„å½’ä¸€åŒ–å½±å“
        if 'collaboration_raw' in reward_components and 'collaboration_normalized' in reward_components:
            raw_collab = reward_components['collaboration_raw']
            norm_collab = reward_components['collaboration_normalized']
            impact['total_raw_normalized_rewards'] += raw_collab
            impact['total_normalized_rewards'] += norm_collab
            impact['components_impact']['collaboration'] = {
                'raw': raw_collab,
                'normalized': norm_collab,
                'reduction': raw_collab - norm_collab
            }
        
        # è®¡ç®—æ‹¥å µæƒ©ç½šçš„å½’ä¸€åŒ–å½±å“
        if 'congestion_penalty_raw' in reward_components and 'congestion_penalty_normalized' in reward_components:
            raw_congestion = abs(reward_components['congestion_penalty_raw'])
            norm_congestion = abs(reward_components['congestion_penalty_normalized'])
            impact['total_raw_normalized_rewards'] += raw_congestion
            impact['total_normalized_rewards'] += norm_congestion
            impact['components_impact']['congestion_penalty'] = {
                'raw': raw_congestion,
                'normalized': norm_congestion,
                'reduction': raw_congestion - norm_congestion
            }
        
        # è®¡ç®—æ€»çš„å½’ä¸€åŒ–èŠ‚çœ
        impact['normalization_savings'] = impact['total_raw_normalized_rewards'] - impact['total_normalized_rewards']
        
        return impact
    
    def _log_reward_components(self, reward_components: dict):
        """
        è®°å½•å¥–åŠ±ç»„æˆçš„è¯¦ç»†æ—¥å¿—ï¼Œç”¨äºè°ƒè¯•å’Œç›‘æ§
        
        Args:
            reward_components: å¥–åŠ±ç»„æˆå­—å…¸
        """
        normalization_info = reward_components['per_agent_normalization']
        
        print(f"[Step {self.step_count}] Per-Agentå¥–åŠ±å½’ä¸€åŒ–è¯¦æƒ…:")
        print(f"  æœ‰æ•ˆUAVæ•°é‡: {normalization_info['n_active_uavs']}/{normalization_info['total_uavs']}")
        print(f"  å½’ä¸€åŒ–å› å­: {normalization_info['normalization_factor']:.4f}")
        print(f"  åº”ç”¨å½’ä¸€åŒ–çš„ç»„ä»¶: {normalization_info['components_normalized']}")
        
        impact = normalization_info['normalization_impact']
        if impact['normalization_savings'] > 0:
            print(f"  å½’ä¸€åŒ–èŠ‚çœ: {impact['normalization_savings']:.4f}")
            for component, details in impact['components_impact'].items():
                print(f"    {component}: {details['raw']:.4f} -> {details['normalized']:.4f} "
                      f"(å‡å°‘ {details['reduction']:.4f})")
        
        print(f"  æœ€ç»ˆå¥–åŠ±: {reward_components['final_reward']:.4f}")
        print()
    
    def _calculate_uav_alive_status(self, uav, uav_idx: int) -> float:
        """
        è®¡ç®—UAVçš„å­˜æ´»çŠ¶æ€ï¼Œè€ƒè™‘é€šä¿¡/æ„ŸçŸ¥å¤±æ•ˆæƒ…å†µ
        
        é²æ£’æ€§æ©ç æœºåˆ¶çš„æ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºæ ‡è¯†æ— äººæœºçš„é€šä¿¡/æ„ŸçŸ¥çŠ¶æ€ã€‚
        åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œè¿™å¯ä»¥åŸºäºï¼š
        - é€šä¿¡é“¾è·¯è´¨é‡
        - ä¼ æ„Ÿå™¨çŠ¶æ€
        - ç”µæ± ç”µé‡
        - ç³»ç»Ÿå¥åº·çŠ¶æ€
        
        Args:
            uav: UAVå®ä½“å¯¹è±¡
            uav_idx: UAVç´¢å¼•
            
        Returns:
            float: å­˜æ´»çŠ¶æ€ (0.0=å¤±æ•ˆ, 1.0=æ­£å¸¸)
        """
        # åŸºç¡€å­˜æ´»æ£€æŸ¥ï¼šæ˜¯å¦æœ‰å‰©ä½™èµ„æº
        has_resources = np.any(uav.resources > 0)
        if not has_resources:
            return 0.0
        
        # æ¨¡æ‹Ÿé€šä¿¡å¤±æ•ˆåœºæ™¯ï¼ˆå¯é…ç½®çš„å¤±æ•ˆæ¦‚ç‡ï¼‰
        communication_failure_rate = getattr(self.config, 'UAV_COMM_FAILURE_RATE', 0.0)
        if communication_failure_rate > 0:
            # ä½¿ç”¨ç¡®å®šæ€§çš„ä¼ªéšæœºæ•°ï¼ŒåŸºäºstep_countå’Œuav_idxç¡®ä¿å¯å¤ç°æ€§
            failure_seed = (self.step_count * 31 + uav_idx * 17) % 1000
            failure_prob = failure_seed / 1000.0
            if failure_prob < communication_failure_rate:
                return 0.0
        
        # æ¨¡æ‹Ÿæ„ŸçŸ¥ç³»ç»Ÿå¤±æ•ˆï¼ˆåŸºäºè·ç¦»å’Œç¯å¢ƒå¤æ‚åº¦ï¼‰
        sensing_failure_rate = getattr(self.config, 'UAV_SENSING_FAILURE_RATE', 0.0)
        if sensing_failure_rate > 0:
            # è®¡ç®—ç¯å¢ƒå¤æ‚åº¦å› å­ï¼ˆéšœç¢ç‰©å¯†åº¦ã€ç›®æ ‡å¯†åº¦ç­‰ï¼‰
            complexity_factor = self._calculate_environment_complexity(uav)
            adjusted_failure_rate = sensing_failure_rate * complexity_factor
            
            sensing_seed = (self.step_count * 23 + uav_idx * 19) % 1000
            sensing_prob = sensing_seed / 1000.0
            if sensing_prob < adjusted_failure_rate:
                return 0.0
        
        # æ¨¡æ‹Ÿç”µæ± ç”µé‡å½±å“çš„é€šä¿¡èƒ½åŠ›
        battery_threshold = getattr(self.config, 'UAV_LOW_BATTERY_THRESHOLD', 0.1)
        if hasattr(uav, 'battery_level'):
            if uav.battery_level < battery_threshold:
                # ä½ç”µé‡æ—¶é€šä¿¡èƒ½åŠ›ä¸‹é™ï¼Œä½†ä¸å®Œå…¨å¤±æ•ˆ
                return 0.3
        
        # æ¨¡æ‹Ÿç³»ç»Ÿè¿‡è½½å¯¼è‡´çš„å“åº”å»¶è¿Ÿ
        system_load = len([u for u in self.uavs if np.any(u.resources > 0)])
        max_concurrent_uavs = getattr(self.config, 'MAX_CONCURRENT_UAVS', 20)
        if system_load > max_concurrent_uavs:
            # ç³»ç»Ÿè¿‡è½½æ—¶ï¼Œéƒ¨åˆ†UAVå¯èƒ½å“åº”å»¶è¿Ÿ
            overload_factor = (system_load - max_concurrent_uavs) / max_concurrent_uavs
            if (uav_idx + self.step_count) % system_load < overload_factor * system_load:
                return 0.5  # éƒ¨åˆ†åŠŸèƒ½å—é™
        
        return 1.0  # æ­£å¸¸çŠ¶æ€
    
    def _calculate_target_visibility_status(self, target, target_idx: int) -> float:
        """
        è®¡ç®—ç›®æ ‡çš„å¯è§æ€§çŠ¶æ€ï¼Œè€ƒè™‘æ„ŸçŸ¥èŒƒå›´å’Œç¯å¢ƒé®æŒ¡
        
        é²æ£’æ€§æ©ç æœºåˆ¶çš„æ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºæ ‡è¯†ç›®æ ‡çš„å¯è§æ€§çŠ¶æ€ã€‚
        åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œè¿™å¯ä»¥åŸºäºï¼š
        - ä¼ æ„Ÿå™¨æ„ŸçŸ¥èŒƒå›´
        - ç¯å¢ƒé®æŒ¡ï¼ˆå»ºç­‘ç‰©ã€åœ°å½¢ï¼‰
        - å¤©æ°”æ¡ä»¶
        - ç›®æ ‡ç‰¹æ€§ï¼ˆå¤§å°ã€åå°„ç‡ç­‰ï¼‰
        
        Args:
            target: ç›®æ ‡å®ä½“å¯¹è±¡
            target_idx: ç›®æ ‡ç´¢å¼•
            
        Returns:
            float: å¯è§æ€§çŠ¶æ€ (0.0=ä¸å¯è§, 1.0=å®Œå…¨å¯è§)
        """
        # åŸºç¡€å¯è§æ€§æ£€æŸ¥ï¼šç›®æ ‡æ˜¯å¦è¿˜æœ‰å‰©ä½™èµ„æº
        has_remaining_resources = np.any(target.remaining_resources > 0)
        if not has_remaining_resources:
            return 0.0
        
        # è®¡ç®—æœ€è¿‘UAVåˆ°ç›®æ ‡çš„è·ç¦»
        min_distance = float('inf')
        closest_uav_alive = False
        
        for i, uav in enumerate(self.uavs):
            if np.any(uav.resources > 0):  # UAVä»ç„¶æ´»è·ƒ
                dist = np.linalg.norm(
                    np.array(target.position) - np.array(uav.current_position)
                )
                if dist < min_distance:
                    min_distance = dist
                    # æ£€æŸ¥æœ€è¿‘çš„UAVæ˜¯å¦å¤„äºæ­£å¸¸é€šä¿¡çŠ¶æ€
                    closest_uav_alive = self._calculate_uav_alive_status(uav, i) > 0.5
        
        # å¦‚æœæ²¡æœ‰æ´»è·ƒçš„UAVï¼Œç›®æ ‡ä¸å¯è§
        if min_distance == float('inf') or not closest_uav_alive:
            return 0.0
        
        # åŸºäºè·ç¦»çš„å¯è§æ€§è¡°å‡
        max_sensing_range = getattr(self.config, 'MAX_SENSING_RANGE', 1000.0)
        if min_distance > max_sensing_range:
            return 0.0
        
        # è·ç¦»è¡°å‡å‡½æ•°ï¼šè¿‘è·ç¦»å®Œå…¨å¯è§ï¼Œè¿œè·ç¦»é€æ¸è¡°å‡
        distance_visibility = max(0.0, 1.0 - (min_distance / max_sensing_range) ** 2)
        
        # æ¨¡æ‹Ÿç¯å¢ƒé®æŒ¡å½±å“
        occlusion_rate = getattr(self.config, 'TARGET_OCCLUSION_RATE', 0.0)
        if occlusion_rate > 0:
            # åŸºäºç›®æ ‡ä½ç½®å’Œç¯å¢ƒå¤æ‚åº¦è®¡ç®—é®æŒ¡æ¦‚ç‡
            occlusion_seed = (self.step_count * 37 + target_idx * 41) % 1000
            occlusion_prob = occlusion_seed / 1000.0
            
            # ç¯å¢ƒå¤æ‚åº¦å½±å“é®æŒ¡æ¦‚ç‡
            env_complexity = self._calculate_target_environment_complexity(target)
            adjusted_occlusion_rate = occlusion_rate * env_complexity
            
            if occlusion_prob < adjusted_occlusion_rate:
                distance_visibility *= 0.2  # é®æŒ¡æ—¶å¯è§æ€§å¤§å¹…ä¸‹é™
        
        # æ¨¡æ‹Ÿå¤©æ°”æ¡ä»¶å½±å“
        weather_visibility = getattr(self.config, 'WEATHER_VISIBILITY_FACTOR', 1.0)
        distance_visibility *= weather_visibility
        
        # æ¨¡æ‹Ÿç›®æ ‡ç‰¹æ€§å½±å“ï¼ˆå¤§å°ã€åå°„ç‡ç­‰ï¼‰
        target_detectability = getattr(target, 'detectability_factor', 1.0)
        distance_visibility *= target_detectability
        
        # ç¡®ä¿è¿”å›å€¼åœ¨[0, 1]èŒƒå›´å†…
        return float(np.clip(distance_visibility, 0.0, 1.0))
    
    def _calculate_environment_complexity(self, uav) -> float:
        """
        è®¡ç®—UAVå‘¨å›´ç¯å¢ƒçš„å¤æ‚åº¦å› å­
        
        ç”¨äºè°ƒæ•´é€šä¿¡/æ„ŸçŸ¥å¤±æ•ˆæ¦‚ç‡ã€‚ç¯å¢ƒè¶Šå¤æ‚ï¼Œå¤±æ•ˆæ¦‚ç‡è¶Šé«˜ã€‚
        
        Args:
            uav: UAVå®ä½“å¯¹è±¡
            
        Returns:
            float: ç¯å¢ƒå¤æ‚åº¦å› å­ [0.5, 2.0]
        """
        complexity = 1.0
        
        # éšœç¢ç‰©å¯†åº¦å½±å“
        if hasattr(self, 'obstacles') and self.obstacles:
            nearby_obstacles = 0
            search_radius = 200.0  # æœç´¢åŠå¾„
            
            for obstacle in self.obstacles:
                if hasattr(obstacle, 'position'):
                    dist = np.linalg.norm(
                        np.array(uav.current_position) - np.array(obstacle.position)
                    )
                    if dist < search_radius:
                        nearby_obstacles += 1
            
            # éšœç¢ç‰©å¯†åº¦å› å­
            obstacle_density = nearby_obstacles / max(1, len(self.obstacles))
            complexity += obstacle_density * 0.5
        
        # UAVå¯†åº¦å½±å“ï¼ˆé€šä¿¡å¹²æ‰°ï¼‰
        nearby_uavs = 0
        interference_radius = 150.0
        
        for other_uav in self.uavs:
            if other_uav.id != uav.id and np.any(other_uav.resources > 0):
                dist = np.linalg.norm(
                    np.array(uav.current_position) - np.array(other_uav.current_position)
                )
                if dist < interference_radius:
                    nearby_uavs += 1
        
        # UAVå¯†åº¦å› å­
        uav_density = nearby_uavs / max(1, len(self.uavs) - 1)
        complexity += uav_density * 0.3
        
        # ç›®æ ‡å¯†åº¦å½±å“ï¼ˆæ„ŸçŸ¥è´Ÿè½½ï¼‰
        nearby_targets = 0
        sensing_radius = 300.0
        
        for target in self.targets:
            if np.any(target.remaining_resources > 0):
                dist = np.linalg.norm(
                    np.array(uav.current_position) - np.array(target.position)
                )
                if dist < sensing_radius:
                    nearby_targets += 1
        
        # ç›®æ ‡å¯†åº¦å› å­
        target_density = nearby_targets / max(1, len(self.targets))
        complexity += target_density * 0.2
        
        # é™åˆ¶å¤æ‚åº¦å› å­èŒƒå›´
        return float(np.clip(complexity, 0.5, 2.0))
    
    def _calculate_target_environment_complexity(self, target) -> float:
        """
        è®¡ç®—ç›®æ ‡å‘¨å›´ç¯å¢ƒçš„å¤æ‚åº¦å› å­
        
        ç”¨äºè°ƒæ•´ç›®æ ‡é®æŒ¡æ¦‚ç‡ã€‚ç¯å¢ƒè¶Šå¤æ‚ï¼Œé®æŒ¡æ¦‚ç‡è¶Šé«˜ã€‚
        
        Args:
            target: ç›®æ ‡å®ä½“å¯¹è±¡
            
        Returns:
            float: ç¯å¢ƒå¤æ‚åº¦å› å­ [0.5, 2.0]
        """
        complexity = 1.0
        
        # éšœç¢ç‰©é®æŒ¡å½±å“
        if hasattr(self, 'obstacles') and self.obstacles:
            nearby_obstacles = 0
            occlusion_radius = 100.0  # é®æŒ¡å½±å“åŠå¾„
            
            for obstacle in self.obstacles:
                if hasattr(obstacle, 'position'):
                    dist = np.linalg.norm(
                        np.array(target.position) - np.array(obstacle.position)
                    )
                    if dist < occlusion_radius:
                        nearby_obstacles += 1
            
            # éšœç¢ç‰©é®æŒ¡å› å­
            occlusion_density = nearby_obstacles / max(1, len(self.obstacles))
            complexity += occlusion_density * 0.8
        
        # å…¶ä»–ç›®æ ‡çš„å¹²æ‰°å½±å“
        nearby_targets = 0
        interference_radius = 80.0
        
        for other_target in self.targets:
            if (other_target.id != target.id and 
                np.any(other_target.remaining_resources > 0)):
                dist = np.linalg.norm(
                    np.array(target.position) - np.array(other_target.position)
                )
                if dist < interference_radius:
                    nearby_targets += 1
        
        # ç›®æ ‡å¹²æ‰°å› å­
        target_interference = nearby_targets / max(1, len(self.targets) - 1)
        complexity += target_interference * 0.3
        
        # é™åˆ¶å¤æ‚åº¦å› å­èŒƒå›´
        return float(np.clip(complexity, 0.5, 2.0))
    
    def _calculate_robust_masks(self) -> Dict[str, np.ndarray]:
        """
        è®¡ç®—å¢å¼ºçš„é²æ£’æ€§æ©ç ï¼Œç»“åˆis_aliveå’Œis_visibleä½
        
        æ©ç æœºåˆ¶çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
        1. åŸºç¡€æœ‰æ•ˆæ€§æ©ç ï¼šåŸºäºèµ„æºçŠ¶æ€
        2. é€šä¿¡/æ„ŸçŸ¥æ©ç ï¼šåŸºäºis_aliveå’Œis_visibleä½
        3. ç»„åˆæ©ç ï¼šä¸ºTransformerGNNæä¾›å¤±æ•ˆèŠ‚ç‚¹å±è”½èƒ½åŠ›
        4. ä½¿ç”¨å›ºå®šç»´åº¦ç¡®ä¿æ‰¹å¤„ç†å…¼å®¹æ€§
        
        Returns:
            Dict[str, np.ndarray]: åŒ…å«å¤šå±‚æ©ç çš„å­—å…¸
        """
        # ä½¿ç”¨å›ºå®šçš„æœ€å¤§æ•°é‡ï¼Œç¡®ä¿ç»´åº¦ä¸€è‡´æ€§
        max_uavs = getattr(self.config, 'MAX_UAVS', 10)
        max_targets = getattr(self.config, 'MAX_TARGETS', 15)
        
        n_uavs = len(self.uavs)
        n_targets = len(self.targets)
        
        # === åŸºç¡€æœ‰æ•ˆæ€§æ©ç  ===
        # UAVåŸºç¡€æ©ç ï¼šåŸºäºèµ„æºçŠ¶æ€ï¼Œä½¿ç”¨å›ºå®šç»´åº¦
        uav_resource_mask = np.zeros(max_uavs, dtype=np.int32)
        for i, uav in enumerate(self.uavs):
            uav_resource_mask[i] = 1 if np.any(uav.resources > 0) else 0
        
        # ç›®æ ‡åŸºç¡€æ©ç ï¼šåŸºäºå‰©ä½™èµ„æºçŠ¶æ€ï¼Œä½¿ç”¨å›ºå®šç»´åº¦
        target_resource_mask = np.zeros(max_targets, dtype=np.int32)
        for i, target in enumerate(self.targets):
            target_resource_mask[i] = 1 if np.any(target.remaining_resources > 0) else 0
        
        # === é€šä¿¡/æ„ŸçŸ¥æ©ç  ===
        # UAVé€šä¿¡æ©ç ï¼šåŸºäºis_aliveä½ï¼Œä½¿ç”¨å›ºå®šç»´åº¦
        uav_communication_mask = np.zeros(max_uavs, dtype=np.int32)
        for i, uav in enumerate(self.uavs):
            uav_communication_mask[i] = 1 if self._calculate_uav_alive_status(uav, i) > 0.5 else 0
        
        # ç›®æ ‡å¯è§æ€§æ©ç ï¼šåŸºäºis_visibleä½ï¼Œä½¿ç”¨å›ºå®šç»´åº¦
        target_visibility_mask = np.zeros(max_targets, dtype=np.int32)
        for i, target in enumerate(self.targets):
            target_visibility_mask[i] = 1 if self._calculate_target_visibility_status(target, i) > 0.5 else 0
        
        # === ç»„åˆæ©ç ï¼ˆç”¨äºTransformerGNNï¼‰ ===
        # UAVæœ‰æ•ˆæ©ç ï¼šåŒæ—¶æ»¡è¶³èµ„æºå’Œé€šä¿¡æ¡ä»¶
        uav_effective_mask = uav_resource_mask & uav_communication_mask
        
        # ç›®æ ‡æœ‰æ•ˆæ©ç ï¼šåŒæ—¶æ»¡è¶³èµ„æºå’Œå¯è§æ€§æ¡ä»¶
        target_effective_mask = target_resource_mask & target_visibility_mask
        
        # === äº¤äº’æ©ç  ===
        # UAV-ç›®æ ‡äº¤äº’æ©ç  [max_uavs, max_targets]ï¼šæ ‡è¯†å“ªäº›UAV-ç›®æ ‡å¯¹å¯ä»¥è¿›è¡Œæœ‰æ•ˆäº¤äº’
        interaction_mask = np.zeros((max_uavs, max_targets), dtype=np.int32)
        
        for i in range(n_uavs):
            for j in range(n_targets):
                # åªæœ‰å½“UAVæœ‰æ•ˆä¸”ç›®æ ‡æœ‰æ•ˆæ—¶ï¼Œæ‰èƒ½è¿›è¡Œäº¤äº’
                if uav_effective_mask[i] == 1 and target_effective_mask[j] == 1:
                    # é¢å¤–æ£€æŸ¥è·ç¦»çº¦æŸ
                    uav = self.uavs[i]
                    target = self.targets[j]
                    dist = np.linalg.norm(
                        np.array(target.position) - np.array(uav.current_position)
                    )
                    max_interaction_range = getattr(self.config, 'MAX_INTERACTION_RANGE', 2000.0)
                    
                    if dist <= max_interaction_range:
                        interaction_mask[i, j] = 1
        
        # æ„å»ºå®Œæ•´çš„æ©ç å­—å…¸
        masks = {
            # åŸºç¡€æ©ç ï¼ˆå‘åå…¼å®¹ï¼‰
            "uav_mask": uav_effective_mask,
            "target_mask": target_effective_mask,
            
            # è¯¦ç»†æ©ç ï¼ˆç”¨äºè°ƒè¯•å’Œåˆ†æï¼‰
            "uav_resource_mask": uav_resource_mask,
            "uav_communication_mask": uav_communication_mask,
            "target_resource_mask": target_resource_mask,
            "target_visibility_mask": target_visibility_mask,
            
            # äº¤äº’æ©ç ï¼ˆç”¨äºTransformerGNNçš„æ³¨æ„åŠ›è®¡ç®—ï¼‰
            "interaction_mask": interaction_mask,
            
            # ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºç›‘æ§å’Œè°ƒè¯•ï¼‰
            "active_uav_count": np.sum(uav_effective_mask),
            "visible_target_count": np.sum(target_effective_mask),
            "total_interactions": np.sum(interaction_mask)
        }
        
        return masks