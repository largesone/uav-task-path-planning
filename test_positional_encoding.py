# -*- coding: utf-8 -*-
# æ–‡ä»¶å: test_positional_encoding.py
# æè¿°: ç›¸å¯¹ä½ç½®ç¼–ç æœºåˆ¶çš„å•å…ƒæµ‹è¯•

import torch
import torch.nn as nn
import numpy as np
from transformer_gnn import PositionalEncoder, TransformerGNN
from gymnasium.spaces import Box, Dict
import unittest


class TestPositionalEncoder(unittest.TestCase):
    """ä½ç½®ç¼–ç å™¨æµ‹è¯•ç±»"""
    
    def setUp(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        self.position_dim = 2
        self.embed_dim = 64
        self.hidden_dim = 32
        self.encoder = PositionalEncoder(
            position_dim=self.position_dim,
            embed_dim=self.embed_dim,
            hidden_dim=self.hidden_dim
        )
    
    def test_positional_encoder_initialization(self):
        """æµ‹è¯•ä½ç½®ç¼–ç å™¨åˆå§‹åŒ–"""
        self.assertEqual(self.encoder.position_dim, self.position_dim)
        self.assertEqual(self.encoder.embed_dim, self.embed_dim)
        
        # æ£€æŸ¥MLPç»“æ„
        self.assertIsInstance(self.encoder.position_mlp, nn.Sequential)
        
        # æ£€æŸ¥æƒé‡åˆå§‹åŒ–
        for module in self.encoder.modules():
            if isinstance(module, nn.Linear):
                # æƒé‡åº”è¯¥è¢«åˆå§‹åŒ–
                self.assertFalse(torch.allclose(module.weight, torch.zeros_like(module.weight)))
    
    def test_positional_encoder_forward(self):
        """æµ‹è¯•ä½ç½®ç¼–ç å™¨å‰å‘ä¼ æ’­"""
        batch_size = 4
        num_pairs = 6
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        relative_positions = torch.randn(batch_size, num_pairs, self.position_dim)
        
        # å‰å‘ä¼ æ’­
        position_embeddings = self.encoder(relative_positions)
        
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
        expected_shape = (batch_size, num_pairs, self.embed_dim)
        self.assertEqual(position_embeddings.shape, expected_shape)
        
        # æ£€æŸ¥è¾“å‡ºä¸æ˜¯é›¶å‘é‡
        self.assertFalse(torch.allclose(position_embeddings, torch.zeros_like(position_embeddings)))
    
    def test_positional_encoder_deterministic(self):
        """æµ‹è¯•ä½ç½®ç¼–ç å™¨çš„ç¡®å®šæ€§"""
        batch_size = 2
        num_pairs = 4
        
        # åˆ›å»ºç›¸åŒçš„è¾“å…¥
        relative_positions = torch.randn(batch_size, num_pairs, self.position_dim)
        
        # ä¸¤æ¬¡å‰å‘ä¼ æ’­
        self.encoder.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        output1 = self.encoder(relative_positions)
        output2 = self.encoder(relative_positions)
        
        # è¾“å‡ºåº”è¯¥ç›¸åŒ
        self.assertTrue(torch.allclose(output1, output2, atol=1e-6))
    
    def test_positional_encoder_different_inputs(self):
        """æµ‹è¯•ä¸åŒè¾“å…¥äº§ç”Ÿä¸åŒè¾“å‡º"""
        batch_size = 2
        num_pairs = 4
        
        # åˆ›å»ºä¸åŒçš„è¾“å…¥
        positions1 = torch.randn(batch_size, num_pairs, self.position_dim)
        positions2 = torch.randn(batch_size, num_pairs, self.position_dim)
        
        self.encoder.eval()
        output1 = self.encoder(positions1)
        output2 = self.encoder(positions2)
        
        # ä¸åŒè¾“å…¥åº”è¯¥äº§ç”Ÿä¸åŒè¾“å‡º
        self.assertFalse(torch.allclose(output1, output2, atol=1e-3))
    
    def test_positional_encoder_gradient_flow(self):
        """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
        batch_size = 2
        num_pairs = 3
        
        relative_positions = torch.randn(batch_size, num_pairs, self.position_dim, requires_grad=True)
        
        # å‰å‘ä¼ æ’­
        position_embeddings = self.encoder(relative_positions)
        loss = position_embeddings.sum()
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦å­˜åœ¨
        self.assertIsNotNone(relative_positions.grad)
        self.assertFalse(torch.allclose(relative_positions.grad, torch.zeros_like(relative_positions.grad)))


class TestTransformerGNN(unittest.TestCase):
    """TransformerGNNæµ‹è¯•ç±»"""
    
    def setUp(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        # åˆ›å»ºè§‚æµ‹ç©ºé—´
        self.obs_space_flat = Box(low=-np.inf, high=np.inf, shape=(128,))
        self.obs_space_dict = Dict({
            'uav_features': Box(low=-np.inf, high=np.inf, shape=(3, 64)),
            'target_features': Box(low=-np.inf, high=np.inf, shape=(5, 64)),
            'relative_positions': Box(low=-np.inf, high=np.inf, shape=(15, 2))  # 3*5 pairs
        })
        
        # åˆ›å»ºåŠ¨ä½œç©ºé—´
        self.action_space = Box(low=-1, high=1, shape=(4,))
        self.num_outputs = 4
        
        # æ¨¡å‹é…ç½®
        self.model_config = {
            "embed_dim": 64,
            "num_heads": 4,
            "num_layers": 2,
            "dropout": 0.1,
            "use_position_encoding": True
        }
    
    def test_transformer_gnn_initialization_flat_obs(self):
        """æµ‹è¯•TransformerGNNåœ¨æ‰å¹³è§‚æµ‹ä¸‹çš„åˆå§‹åŒ–"""
        model = TransformerGNN(
            self.obs_space_flat,
            self.action_space,
            self.num_outputs,
            self.model_config,
            "test_model"
        )
        
        self.assertFalse(model.is_dict_obs)
        self.assertEqual(model.input_dim, 128)
        self.assertTrue(model.use_position_encoding)
        self.assertIsInstance(model.position_encoder, PositionalEncoder)
    
    def test_transformer_gnn_initialization_dict_obs(self):
        """æµ‹è¯•TransformerGNNåœ¨å­—å…¸è§‚æµ‹ä¸‹çš„åˆå§‹åŒ–"""
        model = TransformerGNN(
            self.obs_space_dict,
            self.action_space,
            self.num_outputs,
            self.model_config,
            "test_model"
        )
        
        self.assertTrue(model.is_dict_obs)
        self.assertTrue(model.use_position_encoding)
        self.assertIsInstance(model.position_encoder, PositionalEncoder)
    
    def test_transformer_gnn_forward_flat_obs(self):
        """æµ‹è¯•TransformerGNNåœ¨æ‰å¹³è§‚æµ‹ä¸‹çš„å‰å‘ä¼ æ’­"""
        model = TransformerGNN(
            self.obs_space_flat,
            self.action_space,
            self.num_outputs,
            self.model_config,
            "test_model"
        )
        
        batch_size = 4
        obs = torch.randn(batch_size, 128)
        input_dict = {"obs": obs}
        
        # å‰å‘ä¼ æ’­
        logits, state = model.forward(input_dict, [], None)
        
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
        self.assertEqual(logits.shape, (batch_size, self.num_outputs))
        
        # æ£€æŸ¥å€¼å‡½æ•°
        values = model.value_function()
        self.assertEqual(values.shape, (batch_size,))
    
    def test_transformer_gnn_forward_dict_obs(self):
        """æµ‹è¯•TransformerGNNåœ¨å­—å…¸è§‚æµ‹ä¸‹çš„å‰å‘ä¼ æ’­"""
        model = TransformerGNN(
            self.obs_space_dict,
            self.action_space,
            self.num_outputs,
            self.model_config,
            "test_model"
        )
        
        batch_size = 4
        obs_dict = {
            'uav_features': torch.randn(batch_size, 3, 64),
            'target_features': torch.randn(batch_size, 5, 64),
            'relative_positions': torch.randn(batch_size, 15, 2)
        }
        input_dict = {"obs": obs_dict}
        
        # å‰å‘ä¼ æ’­
        logits, state = model.forward(input_dict, [], None)
        
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
        self.assertEqual(logits.shape, (batch_size, self.num_outputs))
        
        # æ£€æŸ¥å€¼å‡½æ•°
        values = model.value_function()
        self.assertEqual(values.shape, (batch_size,))
    
    def test_position_encoding_effect(self):
        """æµ‹è¯•ä½ç½®ç¼–ç çš„æ•ˆæœ"""
        # åˆ›å»ºä¸¤ä¸ªæ¨¡å‹ï¼šä¸€ä¸ªä½¿ç”¨ä½ç½®ç¼–ç ï¼Œä¸€ä¸ªä¸ä½¿ç”¨
        config_with_pos = self.model_config.copy()
        config_with_pos["use_position_encoding"] = True
        
        config_without_pos = self.model_config.copy()
        config_without_pos["use_position_encoding"] = False
        
        model_with_pos = TransformerGNN(
            self.obs_space_dict,
            self.action_space,
            self.num_outputs,
            config_with_pos,
            "with_pos"
        )
        
        model_without_pos = TransformerGNN(
            self.obs_space_dict,
            self.action_space,
            self.num_outputs,
            config_without_pos,
            "without_pos"
        )
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        obs_dict = {
            'uav_features': torch.randn(batch_size, 3, 64),
            'target_features': torch.randn(batch_size, 5, 64),
            'relative_positions': torch.randn(batch_size, 15, 2)
        }
        input_dict = {"obs": obs_dict}
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model_with_pos.eval()
        model_without_pos.eval()
        
        # å‰å‘ä¼ æ’­
        logits_with_pos, _ = model_with_pos.forward(input_dict, [], None)
        logits_without_pos, _ = model_without_pos.forward(input_dict, [], None)
        
        # ä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºåº”è¯¥ä¸åŒï¼ˆå› ä¸ºä½ç½®ç¼–ç çš„å½±å“ï¼‰
        self.assertFalse(torch.allclose(logits_with_pos, logits_without_pos, atol=1e-3))
    
    def test_permutation_invariance_breaking(self):
        """æµ‹è¯•ä½ç½®ç¼–ç èƒ½å¤Ÿç ´åæ’åˆ—ä¸å˜æ€§"""
        model = TransformerGNN(
            self.obs_space_dict,
            self.action_space,
            self.num_outputs,
            self.model_config,
            "test_model"
        )
        model.eval()
        
        batch_size = 1
        
        # åˆ›å»ºå…·æœ‰æ˜æ˜¾ä¸åŒä½ç½®çš„UAVå’Œç›®æ ‡
        uav_features = torch.randn(batch_size, 3, 64)
        target_features = torch.randn(batch_size, 5, 64)
        
        # åˆ›å»ºå…·æœ‰æ˜æ˜¾å·®å¼‚çš„ç›¸å¯¹ä½ç½®çŸ©é˜µ
        # UAV 0 ä¸æ‰€æœ‰ç›®æ ‡çš„ç›¸å¯¹ä½ç½®
        uav0_positions = torch.tensor([[[1.0, 0.0], [2.0, 0.0], [3.0, 0.0], [4.0, 0.0], [5.0, 0.0]]], dtype=torch.float32)
        # UAV 1 ä¸æ‰€æœ‰ç›®æ ‡çš„ç›¸å¯¹ä½ç½®  
        uav1_positions = torch.tensor([[[0.0, 1.0], [0.0, 2.0], [0.0, 3.0], [0.0, 4.0], [0.0, 5.0]]], dtype=torch.float32)
        # UAV 2 ä¸æ‰€æœ‰ç›®æ ‡çš„ç›¸å¯¹ä½ç½®
        uav2_positions = torch.tensor([[[-1.0, -1.0], [-2.0, -2.0], [-3.0, -3.0], [-4.0, -4.0], [-5.0, -5.0]]], dtype=torch.float32)
        
        # åˆå¹¶ä¸ºå®Œæ•´çš„ç›¸å¯¹ä½ç½®çŸ©é˜µ [batch_size, num_uavs, num_targets, 2]
        relative_positions = torch.cat([uav0_positions, uav1_positions, uav2_positions], dim=1)
        
        obs_dict1 = {
            'uav_features': uav_features,
            'target_features': target_features,
            'relative_positions': relative_positions
        }
        
        # åˆ›å»ºæ’åˆ—åçš„è§‚æµ‹ï¼ˆäº¤æ¢å‰ä¸¤ä¸ªUAVçš„ç‰¹å¾å’Œå¯¹åº”çš„ç›¸å¯¹ä½ç½®ï¼‰
        uav_features_permuted = uav_features.clone()
        uav_features_permuted[:, [0, 1]] = uav_features_permuted[:, [1, 0]]
        
        # ç›¸åº”åœ°äº¤æ¢ç›¸å¯¹ä½ç½®ï¼ˆäº¤æ¢UAV 0å’ŒUAV 1çš„ç›¸å¯¹ä½ç½®ï¼‰
        relative_positions_permuted = relative_positions.clone()
        relative_positions_permuted[:, [0, 1]] = relative_positions_permuted[:, [1, 0]]
        
        obs_dict2 = {
            'uav_features': uav_features_permuted,
            'target_features': target_features,
            'relative_positions': relative_positions_permuted
        }
        
        # å‰å‘ä¼ æ’­
        logits1, _ = model.forward({"obs": obs_dict1}, [], None)
        logits2, _ = model.forward({"obs": obs_dict2}, [], None)
        
        # ç”±äºä½ç½®ç¼–ç çš„å­˜åœ¨ï¼Œæ’åˆ—åçš„è¾“å‡ºåº”è¯¥ä¸åŒ
        # ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼ï¼Œå› ä¸ºä½ç½®ç¼–ç çš„å½±å“å¯èƒ½è¾ƒå°
        self.assertFalse(torch.allclose(logits1, logits2, atol=1e-4))
        
        # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿å·®å¼‚è¶³å¤Ÿå¤§
        diff = torch.abs(logits1 - logits2).max().item()
        self.assertGreater(diff, 1e-5, f"ä½ç½®ç¼–ç çš„å½±å“å¤ªå°ï¼Œæœ€å¤§å·®å¼‚: {diff}")
    
    def test_gradient_flow_with_position_encoding(self):
        """æµ‹è¯•å¸¦ä½ç½®ç¼–ç çš„æ¢¯åº¦æµåŠ¨"""
        model = TransformerGNN(
            self.obs_space_dict,
            self.action_space,
            self.num_outputs,
            self.model_config,
            "test_model"
        )
        
        batch_size = 2
        obs_dict = {
            'uav_features': torch.randn(batch_size, 3, 64, requires_grad=True),
            'target_features': torch.randn(batch_size, 5, 64, requires_grad=True),
            'relative_positions': torch.randn(batch_size, 15, 2, requires_grad=True)
        }
        input_dict = {"obs": obs_dict}
        
        # å‰å‘ä¼ æ’­
        logits, _ = model.forward(input_dict, [], None)
        loss = logits.sum()
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦å­˜åœ¨
        self.assertIsNotNone(obs_dict['uav_features'].grad)
        self.assertIsNotNone(obs_dict['target_features'].grad)
        self.assertIsNotNone(obs_dict['relative_positions'].grad)
        
        # æ£€æŸ¥ä½ç½®ç¼–ç å™¨çš„æ¢¯åº¦
        pos_encoder_has_grad = False
        for param in model.position_encoder.parameters():
            if param.grad is not None and not torch.allclose(param.grad, torch.zeros_like(param.grad)):
                pos_encoder_has_grad = True
                break
        
        self.assertTrue(pos_encoder_has_grad, "ä½ç½®ç¼–ç å™¨åº”è¯¥æœ‰æ¢¯åº¦æµåŠ¨")


def run_position_encoding_tests():
    """è¿è¡Œä½ç½®ç¼–ç æµ‹è¯•"""
    print("å¼€å§‹è¿è¡Œä½ç½®ç¼–ç æœºåˆ¶æµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ PositionalEncoderæµ‹è¯•
    test_suite.addTest(unittest.makeSuite(TestPositionalEncoder))
    
    # æ·»åŠ TransformerGNNæµ‹è¯•
    test_suite.addTest(unittest.makeSuite(TestTransformerGNN))
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    if result.wasSuccessful():
        print("\nâœ… æ‰€æœ‰ä½ç½®ç¼–ç æµ‹è¯•é€šè¿‡ï¼")
        print(f"è¿è¡Œäº† {result.testsRun} ä¸ªæµ‹è¯•")
    else:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼")
        print(f"è¿è¡Œäº† {result.testsRun} ä¸ªæµ‹è¯•")
        print(f"å¤±è´¥: {len(result.failures)}")
        print(f"é”™è¯¯: {len(result.errors)}")
        
        # æ‰“å°å¤±è´¥è¯¦æƒ…
        for test, traceback in result.failures:
            print(f"\nå¤±è´¥æµ‹è¯•: {test}")
            print(f"é”™è¯¯ä¿¡æ¯: {traceback}")
        
        for test, traceback in result.errors:
            print(f"\né”™è¯¯æµ‹è¯•: {test}")
            print(f"é”™è¯¯ä¿¡æ¯: {traceback}")
    
    return result.wasSuccessful()


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    success = run_position_encoding_tests()
    
    if success:
        print("\nğŸ‰ ä½ç½®ç¼–ç æœºåˆ¶å®ç°å®Œæˆå¹¶é€šè¿‡æ‰€æœ‰æµ‹è¯•ï¼")
        print("\nä¸»è¦åŠŸèƒ½:")
        print("1. âœ… PositionalEncoderæ¨¡å— - é€šè¿‡å°å‹MLPç”Ÿæˆä½ç½®åµŒå…¥")
        print("2. âœ… TransformerGNNé›†æˆ - å°†ä½ç½®åµŒå…¥åŠ åˆ°å®ä½“ç‰¹å¾åµŒå…¥ä¸Š")
        print("3. âœ… æ’åˆ—ä¸å˜æ€§ç ´å - ä½ç½®ç¼–ç è§£å†³äº†æ’åˆ—ä¸å˜æ€§é—®é¢˜")
        print("4. âœ… æ¢¯åº¦æµåŠ¨æ­£å¸¸ - æ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒ")
        print("5. âœ… å¤šç§è§‚æµ‹æ ¼å¼æ”¯æŒ - æ”¯æŒæ‰å¹³å’Œå­—å…¸è§‚æµ‹")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ï¼")